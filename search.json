[
  {
    "objectID": "posts/projects/cdrc-semantic-search/index.html",
    "href": "posts/projects/cdrc-semantic-search/index.html",
    "title": "CDRC Semantic Search System",
    "section": "",
    "text": "The CDRC Semantic Search System is a project designed to enhance the search capabilities of the Centre for Consumer Data Research (CDRC) data catalogue. The goal is to implement a semantic search approach that goes beyond traditional keyword-based searches, providing users with more accurate and relevant results."
  },
  {
    "objectID": "posts/projects/cdrc-semantic-search/index.html#overview",
    "href": "posts/projects/cdrc-semantic-search/index.html#overview",
    "title": "CDRC Semantic Search System",
    "section": "Overview",
    "text": "Overview\nThe CDRC Semantic Search System is a project designed to enhance the search capabilities of the Centre for Consumer Data Research (CDRC) data catalogue. The goal is to implement a semantic search approach that goes beyond traditional keyword-based searches, providing users with more accurate and relevant results."
  },
  {
    "objectID": "posts/projects/cdrc-semantic-search/index.html#features",
    "href": "posts/projects/cdrc-semantic-search/index.html#features",
    "title": "CDRC Semantic Search System",
    "section": "Features",
    "text": "Features\n\nSemantic Search: Utilizes advanced natural language processing techniques to understand the meaning behind user queries, enabling a more intuitive and precise search experience.\n\n\n\n\nStreamlit demo (19/12/23)"
  },
  {
    "objectID": "posts/projects/cdrc-semantic-search/index.html#system-architecture",
    "href": "posts/projects/cdrc-semantic-search/index.html#system-architecture",
    "title": "CDRC Semantic Search System",
    "section": "System Architecture",
    "text": "System Architecture\nThe CDRC Semantic Search System follows a standard Retrieval Augmented Generation (RAG) architecture:\n Credit to Heiko Hotz (https://towardsdatascience.com/rag-vs-finetuning-which-is-the-best-tool-to-boost-your-llm-application-94654b1eaba7)"
  },
  {
    "objectID": "posts/projects/cdrc-semantic-search/index.html#installation",
    "href": "posts/projects/cdrc-semantic-search/index.html#installation",
    "title": "CDRC Semantic Search System",
    "section": "Installation",
    "text": "Installation\nTo get started with the CDRC Semantic Search System, follow these steps:\n\nClone the repository:\ngit clone https://github.com/cjber/cdrc-semantic-search.git\nInstall dependencies:\n\nWith pip:\ncd cdrc-semantic-search\npip install -r requirements.txt\nWith pdm:\ncd cdrc-semantic-search\npdm install\n\nConfigure the system:\nEdit the config/config.toml file to customize settings such as API keys, or model settings.\nRun the system using a DVC pipeline.\ndvc repro\n\n\nNote: The CDRC Semantic Search System is an ongoing project, and we appreciate your feedback and support in making it a valuable tool for researchers at the Centre for Consumer Data Research."
  },
  {
    "objectID": "posts/manuscripts/overture/index.html",
    "href": "posts/manuscripts/overture/index.html",
    "title": "Overture POI data for the United Kingdom: a comprehensive, queryable open data product",
    "section": "",
    "text": "Point of Interest (POI) data is an invaluable source of information, acting as a key input to much of the research that has, and continues to be generated in urban analytics and city science. These data provide key locational attributes about a broad variety of social, environmental and economic phenomena, including historical landmarks, parks, hospitals and retailers, and have been vital sources of data for different applications, including health (Green et al. 2018; Hobbs et al. 2019), urban mobility (Graells-Garrido et al. 2021; Jay et al. 2022), retail and location analysis (Ballantyne et al. 2022), transportation (Owen, Arribas-Bel, and Rowe 2023; Credit 2018), and many others. However, a major challenge when working with POI data relates to the coverage and comprehensiveness of these datasets (Ballantyne et al. 2022; Zhang and Pfoser 2019). By this we mean how much the chosen source(s) of POI data restricts the analyses to specific cities or regions (i.e., coverage), and the attributes and characteristics that are provided for each POI (i.e., comprehensiveness).\nMany POI datasets offer a high level of global coverage and availability, such as OpenStreetMap. However there are problems when considering the coverage and comprehensiveness of OpenStreetMap data at finer spatial resolutions and in areas with less contributors (Haklay 2010), as well as in less developed countries (Mahabir et al. 2017). Similarly, datasets like OpenStreetMap often contain inconsistent attributes for economic activities like retail stores and leisure (Zhang and Pfoser 2019; Ballantyne et al. 2022). Some POI datasets exist which fill this gap, such as the Ordnance Survey ‘Points of Interest’ data product, which provides a more comprehensive database of economic activities (Haklay 2010 ), but is not openly-available. Other data providers have democratised access to comprehensive POI datasets such as SafeGraph and the Local Data Company, however these datasets exhibit poor global coverage of non-branded POIs (SafeGraph), and a lack of comprehensive coverage in the UK (Dolega et al. 2021). As a result, there is a clear gap for data that can address some of these limitations, by providing an openly-available, comprehensive and accurate source of POIs for the UK. In this article, we introduce readers to a processed version of the Overture Maps places (POI) dataset (Overture Maps Foundation 2023), which arguably provides a strong solution to many of these problems, and can facilitate groundbreaking urban analytics research in a number of different application areas."
  },
  {
    "objectID": "posts/manuscripts/overture/index.html#footnotes",
    "href": "posts/manuscripts/overture/index.html#footnotes",
    "title": "Overture POI data for the United Kingdom: a comprehensive, queryable open data product",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://figshare.com/s/144265a705159c03c08f?file=42761512↩︎\nhttps://figshare.com/s/144265a705159c03c08f?file=42809656↩︎\nhttps://figshare.com/s/144265a705159c03c08f?file=42809500↩︎\nhttps://figshare.com/s/144265a705159c03c08f?file=42809452↩︎"
  },
  {
    "objectID": "posts/manuscripts/sdss/index.html",
    "href": "posts/manuscripts/sdss/index.html",
    "title": "Geoparsing comments from Reddit to extract mental place connectivity within the United Kingdom",
    "section": "",
    "text": "Connectivity between places may be explored through the physical movement of individuals, using population movement data like transport records (Yang, Li, and Li 2019; Allard and Moura 2016; Gong et al. 2021; Farber and Li 2013), or GPS information through mobile phone data (Lin, Wu, and Li 2019; SafeGraph 2022). Due to the advent of ‘Volunteered Geographic Information’ (VGI) (Goodchild 2007), these connections may also be explored through geotagged social media posts (Arthur and Williams 2019; Ostermann et al. 2015; Li et al. 2021), with results that mirror true population movements [li2021;Kuchler, Russel, and Stroebel (2020)].\nLiterature discussing the role of human cognition in constructing mental images of cities (Lynch 1964), and how they can be represented through mental maps (Gould and White 1986), shows that the way humans conceive spatial structures and place relationships are substantially entrenched in individuals’ experiences and geographic knowledge, which only partially derive from movements. In particular, while movements are constrained by time and euclidean distance in geographic space (Miller 2018; Patterson and Farber 2015), representational spaces expressed in mental maps do not necessarily correlate with these spatio-temporal boundaries.\nNew forms of data, especially social media and text data, offer novel opportunities to explore place connections, emerging from peoples naïve and experiential geographic knowledge. Recent studies have investigated how digital social networks, i.e. Facebook friendships (Bailey et al. 2018), may be explored in this regard. While, geo-semantic relatedness outlines the ability to quantify relationships between geographic terms in text (Ballatore, Bertolotto, and Wilson 2014), with work applying this to co-occurring city names found in news articles, social media, and general web pages (Hu, Ye, and Shaw 2017; Ye, Gong, and Li 2021; Liu et al. 2014; Meijers and Peris 2019). Our paper considers the use of a novel corpus of text data from comments on the online social media website Reddit as a source of VGI. A task-specific geoparsing pipeline is first used to identify place names4 related to the United Kingdom and resolve them to geographic coordinates (Purves et al. 2018), at a geographic resolution higher than is typically explored through geoparsing. We derive connectivity between each location in our corpus based on the number of times two distinct locations co-occur, normalised by the total number of users that mention each location. The context for co-occurrences is derived from the total collection of comments submitted by each unique user, meaning every location mentioned by a single user is treated as co-occurring, and exhibiting some implicit connectivity. A full interactive map showing place connections is available through Unfolded.ai.\nWith the connectivity between places established, we consider the ability to derive explainable characteristics from the text, to determine why different levels of place connectivity occur. While traditional connections between places may be influenced by factors like transport availability (Allard and Moura 2016), the alternative mental place connections derived through our paper may be more heavily influenced by a subconscious bias. Connectivity is therefore examined against sentiment, expected to highlight these biases, alongside a standard measure of relative material deprivation (limited to England), through the Indices of Multiple Deprivation (IMD), an alternative measure that would be expected to affect traditional place connectivity."
  },
  {
    "objectID": "posts/manuscripts/sdss/index.html#data-sources",
    "href": "posts/manuscripts/sdss/index.html#data-sources",
    "title": "Geoparsing comments from Reddit to extract mental place connectivity within the United Kingdom",
    "section": "Data sources",
    "text": "Data sources\nReddit is a public discussion, news aggregation social network, among the top 20 most visited websites in the United Kingdom. As of 2020, Reddit had around 430 million active monthly users, comparable to the number of Twitter users (Murphy 2019; Statista 2022). Reddit is divided into separate independent subreddits each with specific topics of discussion, where users may submit posts which each have dedicated nested conversation threads that users can add comments to. In total there are 213 subreddits that relate to ‘places’ within the United Kingdom1. For each subreddit, every single historic comment was retrieved using the Pushshift Reddit archive (Baumgartner et al. 2020). In total 8,295,591 comments were extracted, submitted by 492,123 unique users, between 2011-01-01 and 2022-04-17.\nTo train a model to identify place names from comments, the WNUT-17 corpus was used (Derczynski et al. 2017), keeping only ‘location’ labels. In total this corpus covers 5,690 individual documents from Reddit, Twitter, YouTube, and StackExchange. Two gazetteers were selected to geocode place names, chosen to be UK centric, and at a high resolution. We were specifically interested in a gazetteer that did not include country names external to the UK, but included fine-grained named locations like street names. For our gazetteer we combined OS Open Names, and non-settlements from the Gazetteer of British Place Names."
  },
  {
    "objectID": "posts/manuscripts/sdss/index.html#geoparsing",
    "href": "posts/manuscripts/sdss/index.html#geoparsing",
    "title": "Geoparsing comments from Reddit to extract mental place connectivity within the United Kingdom",
    "section": "Geoparsing",
    "text": "Geoparsing\nA custom named entity recognition (NER) model was built using a RoBERTa based transformer language model, pre-trained using Twitter data2, as this architecture has given good results on the WNUT-17 corpus (Barbieri et al. 2020).\nWith place names identified, we developed a method for attributing each name to a single set of geographic coordinates. Place names typically appear multiple times in gazetteers, especially when grounding fine-grained locations like street names, meaning a disambiguation method is required. We disambiguate place names by finding their minimum distance to a collection of contextual locations. Contextual locations in this case refer to all gazetteer entries matching place names that appear in sentences with this target place name, in the same subreddit. This works under the assumption that each unique place name in a single subreddit is likely to refer to the same location, and that locations mentioned in surrounding text are likely close together (Kamalloo and Rafiei 2018).\nSentiment was attributed to each sentence in our corpus containing a place name, using an existing fine-tuned sentiment classification transformer model3. Each place name identified by our NER model was assigned sentiment based on its context sentence."
  },
  {
    "objectID": "posts/manuscripts/sdss/index.html#measuring-place-connectivity",
    "href": "posts/manuscripts/sdss/index.html#measuring-place-connectivity",
    "title": "Geoparsing comments from Reddit to extract mental place connectivity within the United Kingdom",
    "section": "Measuring place connectivity",
    "text": "Measuring place connectivity\nOur place connectivity methodology considers the co-occurrence between each place mentioned by every unique user in our corpus. The following equation described by (Li et al. 2021) outlines this concept:\n\\[\nPCI_{ij} = \\frac{\\mathbf{S}_{ij}}{\\sqrt{\\mathbf{S}_i\\mathbf{S}_j}}, i, j \\in [1, N]\n\\]\n\\(PCI_{ij}\\) is the place connectivity index between locations \\(i\\) and \\(j\\), \\(\\mathbf{S}_{ij}\\) is the total number of users that mention both locations (i.e. the intersect in set theory). This is normalised, given locations with higher populations are expected to be mentioned by a larger number of users, using \\(\\sqrt{\\mathbf{S}_{i}\\mathbf{S}_{j}}\\), the total number of users mentioning \\(\\mathbf{S}_{i}\\) multiplied by the total number of users mentioning \\(\\mathbf{S}_{j}\\), taking the square root. \\(n\\) is the total number of unique locations found in all comments."
  },
  {
    "objectID": "posts/manuscripts/sdss/index.html#place-connectivity",
    "href": "posts/manuscripts/sdss/index.html#place-connectivity",
    "title": "Geoparsing comments from Reddit to extract mental place connectivity within the United Kingdom",
    "section": "Place connectivity",
    "text": "Place connectivity\n\n\n\n\n\n\nFigure 1: Place connections aggregated to Local Authority District, edge size weighted by PCI values, full interactive map available through Unfolded\n\n\n\nFigure 1 demonstrates mental place connections when aggregated to Local Authority Districts (LAD). This aggregation is performed by treating a place equivalent to the LAD it is contained in. Aggregation allows for a higher level view of place connections to be observed, for example combining several points of interest within a major city produces a single strong connection, rather than several weaker connections. Figure 1 (a) shows connections within England and Wales. Clusters emerge where isolated urban areas and their surrounding LADs exhibit strong connectivity, for example around London, Liverpool and Manchester, and Bristol. Notably Wales appears reasonably isolated from England, but inter-connectivity between LADs is strong, including in more rural areas. This isolation between Wales and the rest of the UK has also been observed through semantic analysis of Twitter (Ostermann et al. 2015).\nFigure 1 (b) shows connections within Scotland, these are much stronger generally than the rest of the UK, and highly inter-connected. These connections are also less isolated compared with Wales, with links between Glasgow, Edinburgh and London, as well as strong links with Durham and Newcastle. Links in Scotland are not restricted to urban areas, with strong connections between the rural Highland LAD and all major urban centres. These connections may however be influenced by the varying levels of ambiguity between English place names compared with names in remote Scotland. For example in the ‘Highland’ LAD, 27% of place names are ambiguous, compared with 40% in Manchester, meaning toponym disambiguation is likely more accurate in the Highlands.\nFigure 1 (c) shows a highly urbanised area of England, with two major cities; Liverpool and Manchester. While Manchester links directly with Liverpool, this figure does not appear to reflect the contiguous urban area that links these two cities (Dembski 2015), given intermediate LADs are not connected. The link between these two cities appears direct from a mental perspective, meaning intermediate urban zones are perceived as less connected through our index.\n\n\n\n\n\n\nFigure 2: Correlation between PCI and target (a) IMD, (b) Sentiment, and (c) Dis- tance for unaggregated connections between locations. Connections with fewer than 100 shared users have been excluded. Values aggregated into 200 bins for readability.\n\n\n\nWhile there are a multitude of factors that affect this mental place connectivity, typical connectivity models explore the effects of distance decay (Yang, Li, and Li 2019; Gong et al. 2021; Li et al. 2021; Bailey et al. 2018; Hu, Ye, and Shaw 2017). As past work has found, with both social and physical connections between places, our mental PCI does experience distance decay, with a medium strength negative correlation between PCI and the log of distance (R = −0.55; p = 0.0; Figure 2 (c)). Additionally we explore how sentiment and deprivation may influence PCI values. Figure 2 (a) shows a significant weak correlation between PCI and sentiment (R = 0.18; p = 0.0), while Figure 2 (b) shows a very weak negative correlation between PCI and the IMD Score (R = −0.04; p = 0.0).\nLower connectivity for places with more negative sentiment may be influenced by factors like ‘fear of crime’ (Solymosi et al. 2021), and given deprivation appears less influential, these perceptions likely capture information that is not directly quantifiable through traditional data sources. Certain areas in the United Kingdom have been shown to have ‘reputations’ (Kearns, Kearns, and Lawson 2013), meaning they are known to be viewed negatively both inside and outside their occupants, particularly between the North and South of England (Gould and White 1986)."
  },
  {
    "objectID": "posts/manuscripts/sdss/index.html#footnotes",
    "href": "posts/manuscripts/sdss/index.html#footnotes",
    "title": "Geoparsing comments from Reddit to extract mental place connectivity within the United Kingdom",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://reddit.com/r/unitedkingdom/wiki/british_subreddits↩︎\nhttps://huggingface.co/cardiffnlp/twitter-roberta-base↩︎\nhttps://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest↩︎"
  },
  {
    "objectID": "posts/manuscripts/gisruk/index.html",
    "href": "posts/manuscripts/gisruk/index.html",
    "title": "Comparing rule-based methods and pre-trained language models to classify flood related Tweets",
    "section": "",
    "text": "Twitter presents large continuous feed of information regarding emergency events, contributed through individual users, as these events occur. Many emergency events have been studied in relation to Twitter, including hurricanes and floods in the US (Hughes et al. 2014; Kim and Hastak 2018), Paris terror attacks in 2015 (Reilly and Vicari 2021), and UK flooding events (Saravanou et al. 2015; Brouwer et al. 2017).\nExtreme weather events have become increasingly common (Kron, Löw, and Kundzewicz 2019), a trend that is expected to continue (Forzieri et al. 2017), meaning there is an increasing demand to predict and understand how natural disasters develop. Tweets have proved useful in complementing and supporting emergency response in many cases, and often the first reports about emergencies on social media often precede those of mainstream media (Perng et al. 2013; Martínez-Rojas, Pardo-Ferreira, and Rubio-Romero 2018; Kim and Hastak 2018; Laylavi, Rajabifard, and Kalantari 2016). It is therefore important to be able to extract flood related Tweets, removing the noise that often comes with social media streams (Ashktorab et al. 2014).\nMuch of the past work that has used Twitter to study past emergency events has used keywords to identify relevant Tweets (Kryvasheyeu et al. 2016; Brouwer et al. 2017; Morstatter et al. 2013). This however has several issues, keywords are human selected, meaning they require a pre-existing knowledge of the semantics used to describe targeted events. Certain keywords also do not always relate to these emergency events (Sakaki, Okazaki, and Matsuo 2010; Spielhofer et al. 2016), for example a person may be in ‘floods of tears’. Finally, Tweets relevant to emergency events also do not necessarily contain an obvious keyword (‘Cars are floating down the street!’), and therefore are unable to be detected. More recent work has considered the ability to use machine learning to classify Tweets into those relevant to emergency events, and those that are irrelevant (Imran et al. 2020; Arthur et al. 2018; Sakaki, Okazaki, and Matsuo 2010; Li et al. 2018). These studies have utilised a variety of methods, building from classical approaches like Naïve Bayes classification (Imran et al. 2013; Li et al. 2018) and Support Vector Machines (SVMs) (Caragea et al. 2011; Sakaki, Okazaki, and Matsuo 2010), while more recent work has considered the emerging prevalence of neural networks in text-based classification (Caragea, Silvescu, and Tapia 2016; de Bruijn et al. 2020; Nguyen et al. 2017). Traditional machine learning methods however rely on the use of feature engineering to determine model input, are unable to preserve word order, and have limited capability to use context, often over-fitting based on features selected (Caragea, Silvescu, and Tapia 2016). Work with neural networks has shown that given pre-trained word embeddings, they have the capability to outperform these methods (Ghafarian and Yazdi 2020; Caragea, Silvescu, and Tapia 2016; Algiriyage and Prasanna 2021).\nThis work considers the retrospective classification of a selection of Tweets from past flooding events in the United Kingdom, evaluating the effectiveness of a neural classification model called a transformer against a keyword based approach. This work aims to demonstrate the benefits and costs of the use of new sophisticated methods in natural language processing for this task. Further work is expected to build on this, allowing for information extraction from the relevant Tweets to inform first responders, providing more fine-grained information based on the first-hand experience of individuals like specific property damage, or missing persons, allowing social media to complement existing methods used during flood events [muller2015]."
  },
  {
    "objectID": "posts/manuscripts/gisruk/index.html#data-collection",
    "href": "posts/manuscripts/gisruk/index.html#data-collection",
    "title": "Comparing rule-based methods and pre-trained language models to classify flood related Tweets",
    "section": "Data Collection",
    "text": "Data Collection\n\nFlood Data\nA historical dataset containing all Severe Flood Warnings, Flood Warnings, and Flood Alerts issued by the UK flood warning system is available through the UK Government under the Open Government Licence. This data was linked with flood zones from the Environment Agency Real Time Flood-Monitoring API. To reduce the volume of flood events being considered, only Severe Flood Warnings occurring after 2010 were selected, leaving a total of 314 individual Severe Flood Warning events.\n\n\nTweets\nThe Twitter API v2 was used to extract Tweets from the full historic Tweet archive. For each flood warning the query was constructed using several requirements:\n\nTime-frame: 7 days before to 7 days after flood warning\nBounds: Bounding box of the relevant flood area\nParameters: has geography, exclude retweets, exclude replies, exclude quotes\n\nGeographic information associated with every Tweet was required due to the decision to use bounding boxes to pre-emptively filter Tweets in areas not subject to flooding. The new Twitter API now uses a combination of factors to associate geographic coordinates with Tweets which overcomes the issues with limited availability of geotags found with many previous studies (Middleton, Middleton, and Modafferi 2014; Carley et al. 2016; Morstatter et al. 2013). Geography associated with a Tweet may now include either geotags, user profile location or locations mentioned in Tweet. The total number of Tweets extracted was 89,864, with an average of 286 Tweets per flood warning. From this corpus, only a random subset of ~2,500 Tweets were considered for training and evaluation, selecting a subset that balances time constraints and mirroring the corpus size of past work (Caragea, Silvescu, and Tapia 2016; Ghafarian and Yazdi 2020)."
  },
  {
    "objectID": "posts/manuscripts/gisruk/index.html#classification",
    "href": "posts/manuscripts/gisruk/index.html#classification",
    "title": "Comparing rule-based methods and pre-trained language models to classify flood related Tweets",
    "section": "Classification",
    "text": "Classification\n\n\n\n\n\n\nFigure 1: Overview of the model processing pipeline.\n\n\n\nFigure 1 gives an overview of the classification pipeline used, each Tweet was first pre-processed to normalise usernames and web addresses, and hashtags were parsed to extract words (Pota et al. 2020) (Stage 1). The selected ~2,500 Tweets were manually annotated to train the classification model using Doccano (Nakayama et al. 2018), with 20% used for model validation (Stage 2). The validation subset was then used to evaluate model performance in relation to the simple rule-based approach (Stage 3).\nThe model builds on the established NLP task of sequence classification, taking token sequences (\\(\\mathbf{x} = \\{x_{0}, x_{1}\\dots x_{n}\\}\\)), and predicting a single label (\\(\\mathit{y}\\)). A pre-trained transformer language model based on the RoBERTa architecture was used as a base, pre-trained using a corpus of 58 million Tweets (Barbieri et al. 2020)1.\nTo construct a rule-based approach for evaluation against this model, Tweets from the validation subset that included a selection of 456 keywords provided by (Saravanou et al. 2015) were labelled as being flood related (FLOOD), while all Tweets that did not contain this selection of keywords were labelled as NOT_FLOOD.\nFor comparative evaluation, the F1 metric was used, which takes the harmonic mean of the precision and recall, meaning class imbalance is accounted for. To qualitatively assess the performance of the transformer model, attributions2 for each word in a few selected Tweets were visualised to identify the ability of the model to capture information relevant to flood events, without having to explicitly be fed in keywords (Sundararajan, Taly, and Yan 2017).\nOverall the classification model out-performed the rule-based method on the validation subset, achieving an F1 score of 0.938, compared with 0.814 for the rule-base approach. There is both a lower recall for the rule-based model (0.905 compared with 0.952), and a lower precision (0.952 compared with 0.988).\nFigure 2 explores the decisions made by the transformer model, using four example Tweets to demonstrate the attribution given to each token when assigning a label. Figure \\(\\ref{fig:transformerviz}\\) (A) first gives an example Tweet that is correctly identified as being flood related by the transformer, but does not contain any selected flood related keywords. In this example three keywords are highlighted as important by the model for its correct classification gravel, river and wier. This suggests that the model is able to infer from context that these words relate to flooding, rather than having to be explicitly told through feature engineering or keywords.\n\n\n\n\n\n\nFigure 2: Attribution levels for selected Tweets classified by the transformer model. Attribution label indicates the human annotated label, predicted label shows assigned label with confidence values. Positive attributions dictate the importance of a feature in the given label prediction.\n\n\n\nOn Figure 2 (B), an example is chosen where the model was able to correctly identify the Tweet as being unrelated to flooding, but contains the keyword lightning meaning the rule-based method incorrectly identified it as flood related. Several keywords again appear important for this correct classification, finally which is unlikely to appear in Tweets relevant to floods, in addition to apples and ipad pro, both of which likely appear relatively frequently on Twitter, but rarely in flood related contexts.\nThe final two sub-figures give examples where the model gives incorrect classifications, but the rule-based method does not. Figure 2 (C) shows that while the model realises that raining is a word positively associated with flooding, the rest of the sentence implies that the overall Tweet is likely not in reference to a flooding event. This example reflects a potential issue with selecting a broad annotation scheme, which considered mentions of weather that may relate to flooding events to be a positive match. A Tweet like this is relatively borderline, even for human annotation, meaning it is unsurprising that the model struggles to make a correct decision. This issue is also reflected in Figure 2 (D), the words tide, mark and kent are all identified as flood related words, which is likely true and the label reflects an issue with human annotation."
  },
  {
    "objectID": "posts/manuscripts/gisruk/index.html#footnotes",
    "href": "posts/manuscripts/gisruk/index.html#footnotes",
    "title": "Comparing rule-based methods and pre-trained language models to classify flood related Tweets",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAvailable on the Huggingface Model Hub (Wolf et al. 2020)↩︎\nhttps://github.com/cdpierse/transformers-interpret↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "NLP Engineer at the Consumer Data Research Centre (CDRC). Building a RAG powered search system for the CDRC Data Catalogue, using the Llama Index framework. Aiming to improve data search and discovery.\nExperienced with PyTorch and Hugging Face Transformers, MLOps with DagsHub, DVC, GH Actions. Machine Learning with Scikit-Learn, Pandas, Polars, and Numpy. Keen to learn more about AWS and other cloud services.\n\nOther Links\n Cillian Berragan\n 0000-0003-2198-2245"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cillian Berragan",
    "section": "",
    "text": "project\n\n\ncode\n\n\n\n\n\n\n\n\n\nFeb 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nproject\n\n\ncode\n\n\n\n\n\n\n\n\n\nSep 7, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "Cillian Berragan",
    "section": "",
    "text": "project\n\n\ncode\n\n\n\n\n\n\n\n\n\nFeb 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nproject\n\n\ncode\n\n\n\n\n\n\n\n\n\nSep 7, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#papers",
    "href": "index.html#papers",
    "title": "Cillian Berragan",
    "section": "Papers",
    "text": "Papers\n\n\n\n\n\n\n\n\n\n\nMapping Cognitive Place Associations within the United Kingdom through Online Discussion on Reddit\n\n\n  \n\n\n\nopen-access\n\n\nfigshare\n\n\ndoi\n\n\ngithub\n\n\ncode\n\n\n\nThis published research article builds a custom transformer-based geoparsing pipeline using the Hugging Face transformers Python library, to extract all place names from 8.3 million Reddit comments. DVC pipelines are used to ensure full reproducibility of the model workflow.\n\n\n\n\n\nJan 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nOverture POI data for the United Kingdom: a comprehensive, queryable open data product\n\n\n   \n\n\n\nopen-access\n\n\npreprint\n\n\nfigshare\n\n\ndoi\n\n\ngithub\n\n\ncode\n\n\narxiv\n\n\n\n\n\n\n\n\n\nOct 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluating the Similarity of Location-based Corpora Identified in Reddit Comments\n\n\n \n\n\n\nopen-access\n\n\nceur\n\n\n\n\n\n\n\n\n\nApr 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTransformer based named entity recognition for place name extraction from unstructured text\n\n\n  \n\n\n\nopen-access\n\n\nfigshare\n\n\ndoi\n\n\ngithub\n\n\ncode\n\n\n\nThis published research article builds a custom transformer-based named entity recognition model (NER), evaluating the performance of this model against existing solutions for geoparsing geographic place names from text. This project uses the AllenNLP python library, containerised using Docker for reproducibility.\n\n\n\n\n\nOct 2, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nGeoparsing comments from Reddit to extract mental place connectivity within the United Kingdom\n\n\n \n\n\n\nopen-access\n\n\ndoi\n\n\n\n\n\n\n\n\n\nSep 9, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nComparing rule-based methods and pre-trained language models to classify flood related Tweets\n\n\n  \n\n\n\nopen-access\n\n\nzenodo\n\n\ndoi\n\n\n\n\n\n\n\n\n\nMar 29, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/manuscripts/geoext/index.html",
    "href": "posts/manuscripts/geoext/index.html",
    "title": "Evaluating the Similarity of Location-based Corpora Identified in Reddit Comments",
    "section": "",
    "text": "Social interaction is typically studied in the context of mobility, using data sources like Census or transport records, where physical movement is restricted by distance and ease of connectivity between two locations (Rae 2009; Titheridge et al. 2009). In contrast to this, social interaction has also been studied using phone call data (Sobolevsky et al. 2013), and social media networks (Lengyel et al. 2015), where the spatial and temporal bounds of connectivity between two locations does not restrict interactions. Despite this however, many studies have found that geographic identities within communities still persist in these networks, with interaction strength influenced by the geographic distance between them (Arthur and Williams 2019; Ratti et al. 2010).\nSocial media also presents rich semantic information regarding locations through text associated with geotagged social media posts. Comparative analysis of corpora associated with geotagged locations similarly exhibit regionality; for example, tweets from the North East of England are statistically different compared with the South (Arthur and Williams 2019).\nOur paper explores the similarity of corpora with respect to locational mentions from data taken directly from text, without relying on geotagged metadata. This approach offers an alternative perspective for the analysis of social interaction, built directly from the semantic information associated with locations, rather than the location associated with social media users themselves. Collective semantic information from social media embeds the regional identity of locations across a continuous spectrum, allowing for the direct comparison between these identities and their relationships."
  },
  {
    "objectID": "posts/manuscripts/geoext/index.html#similarity-of-place-corpora",
    "href": "posts/manuscripts/geoext/index.html#similarity-of-place-corpora",
    "title": "Evaluating the Similarity of Location-based Corpora Identified in Reddit Comments",
    "section": "Similarity of Place Corpora",
    "text": "Similarity of Place Corpora\nComparing the similarity between two or more distinct texts first relies on an appropriate method for processing the text into a numerical format. For each location we obtained a corpus of comments, consisting of sentences where each location is mentioned. These were then processed into a single vector, reflecting the semantic information attributed with locations. Typically, a TF-IDF approach is used to generate document embeddings (Daniel and James H 2007), however we found comparative analysis between embeddings did not always provide insightful information. Each vector shared similar properties, giving cosine similarities which did not result in any distinct variation between locations. This is likely a problem with the language between locations sharing similar properties, meaning the more nuanced semantic information is not captured through a TF-IDF method.\nWe therefore extracted embeddings from a deep neural network called a transformer. Unlike TF-IDF or simpler neural network models, transformers are able to use contextual information to generate word embeddings, meaning the same word in two different contexts will not share the exact same vector, capturing different embedded semantic information (Vaswani et al. 2017). Additionally, transformers are pre-trained on a large corpus of text, meaning general information regarding the English language is already embedded within the model, allowing for improved understanding of semantic information. These core features mean that embeddings generated from transformers are likely to capture information that allows for more the accurate comparative analysis. We generated embeddings using the all-mpnet-base-v2 model from the sentence-transformers library in Python (Reimers and Gurevych 2019). Unlike a standard ‘BERT’-like transformer, this library implements modifications to base models that more appropriately captures semantic information in their output embeddings.\nBefore calculating embeddings we first masked every mention of a location with a generic token ‘PLACE’, this ensured that when analysing embeddings, no explicit geographic information was captured accidentally. For example, Manchester and Liverpool may mention matching locations frequently in each of their comments because they are geographically close. To both remove noise and reduce the computational requirements for this work, only locations with over 10,000 unique mentions were kept, from these a random sample of 1,000 comments were selected for each. Once embeddings were generated for every comment in each city corpus, the mean for each corpus was generated, giving a vector 768 decimal values for each city.\nWith a single vector for each selected location, we first calculated K-Means clusters to determine whether distinct groupings of locations could be identified across the UK. To visualise these clusters we used a PCA decomposition to reduce the dimensionality from 768 down to 2 dimensions. Finally, we calculated the cosine similarity between each and every location vector."
  },
  {
    "objectID": "posts/manuscripts/geoext/index.html#footnotes",
    "href": "posts/manuscripts/geoext/index.html#footnotes",
    "title": "Evaluating the Similarity of Location-based Corpora Identified in Reddit Comments",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.reddit.com/r/unitedkingdom/wiki/british_subreddits↩︎"
  },
  {
    "objectID": "posts/manuscripts/associations/index.html",
    "href": "posts/manuscripts/associations/index.html",
    "title": "Mapping Cognitive Place Associations within the United Kingdom through Online Discussion on Reddit",
    "section": "",
    "text": "The importance of relational thinking to understand geographical phenomena has been widely acknowledged in human and computational geography (Glückler and Panitz 2021; Bergmann and O’Sullivan 2018; Lukermann 1961). Spatial networks have been explored from a variety of perspectives, to uncover the dynamics underpinning the spatial behaviours of individuals (González, Hidalgo, and Barabási 2008; Noulas et al. 2011), or to challenge conceptualisations of regions as bounded by administrative definitions (Calafiore et al. 2021; Alessandretti, Aslak, and Lehmann 2020).\nWithin computational geography, most research has explored direct connections between places by investigating physical movements of individuals, using population movement data from both traditional data sources such as Census or surveys (Rae 2009; Titheridge et al. 2009), or through alternative forms of data like transport records (Yang, Li, and Li 2019; Allard and Moura 2016; Gong et al. 2021; Farber and Li 2013), mobile phone data (Lin, Wu, and Li 2019; SafeGraph 2022; Rowe et al. 2022), and geotagged social media (Steiger et al. 2015; Arthur and Williams 2019; Ostermann et al. 2015; Z. Li et al. 2021). However, focussing only on connections built through population movement conceals associations that persist through individuals or community subconscious, regardless of any physical movement.\nLiterature discussing the role of human cognition in constructing mental images of cities (Lynch 1964), and how they can be represented through mental maps (Gould and White 1986), reveals that the way humans conceive spatial structures and associations between places are substantially entrenched in individuals’ experiences and geographic knowledge, which only partially derive from movements. Places represent a complex network of socio-spatial relationships that emerge from linked individual experiences (Pierce, Martin, and Murphy 2011), enabling the definition of collectively recognised place associations. While movements in geographic space are limited by time and distance (Miller 2018; Patterson and Farber 2015), representational spaces expressed through mental maps are not necessarily bounded by spatio-temporal constraints (Merrifield 1993). Modern developments in transport and communication access warp the perceptions of distance between places (Massey 2008), and in turn their perceived level of connectivity (Fabrikant et al. 2002).\nAlternatively, online sources of data offer novel opportunities to explore place associations, built directly from the passive contributions of individual users. Recent work has demonstrated how digital social friendships (Bailey et al. 2018), or embedded links in Wikipedia articles (Salvini and Fabrikant 2016), may be used to provide insight into social place connections. Other works have instead considered that text itself can be used to quantify relationships between geographic terms, described as ‘geo-semantic relatedness’ (Ballatore, Bertolotto, and Wilson 2014). Work building on this concept has applied it to city and region names identified in news articles, social media, and general web pages (Hu, Ye, and Shaw 2017; Ye, Gong, and Li 2021; Liu et al. 2014; Meijers and Peris 2019).\nDistance is a key influence on observed levels of connectivity in spatial interaction literature (Haynes and Fotheringham 1985), and Tobler’s first law of geography, where locations that are further apart are typically less well-connected (Tobler 1970), has generated the term ‘distance decay’ (Taylor 1983), which has various forms of mathematical representation. Given a legacy of empirical evidence, distance decay in its various forms can be sensibly assumed in place connections when both temporal and spatial constraints are considered in our physical environment. However, when considering the links between locations from the perspective of cognitive associations built through mental maps, such constraints are no longer as restrictive (Fabrikant et al. 2002). Quantifying the effect of distance on cognitive place associations may therefore result in unexpected patterns in the effect of distance on associations, that reveal the cognitive biases used to construct mental maps.\nThe objective of this paper is to quantify cognitive place associations across the UK1 to build mental maps, while evaluating the effect of distance on the strength of these associations, measuring the level of distance decay through a gravity model. To generate association measures from a cognitive rather than geographic perspective, we infer associations through co-occurring locations extracted from a large corpus of informal, unstructured and discursive text from the social media website Reddit. Locations when mentioned in informal comments are drawn from a cognitive process associated with mental maps of these locations, subconsciously illustrating associations between places from memory and based on experience.\nSection 2 outlines existing literature relating to cognitive place associations, detailing methods that can be used for the automated extraction and grounding of place names2 from a large corpus of unstructured text. Section 3 provides details on our data sources, our methodology for geoparsing place names, and the computation of a gravity model to examine the effects of distance decay on the strength of associations. In Section 4 we present the results of our gravity model and demonstrate variations in distance decay with respect to six locations. In Section 5 we conclude our findings and outline the scope for future work."
  },
  {
    "objectID": "posts/manuscripts/associations/index.html#quantifying-associations",
    "href": "posts/manuscripts/associations/index.html#quantifying-associations",
    "title": "Mapping Cognitive Place Associations within the United Kingdom through Online Discussion on Reddit",
    "section": "Quantifying associations",
    "text": "Quantifying associations\nWithin mobility research, connections between locations are broadly quantified through mapping the flow of populations, goods, services or other entities between origin and destination locations (Shaw and Hesse 2010). This relates to the concept of Spatial Interaction, which describes a mathematical or statistical representation of physical movements over space, typically observed in the context of commuting, migration or information and commodity flows (Haynes and Fotheringham 1985; Shaw and Hesse 2010; Dennett and Wilson 2013; Rowe, Lovelace, and Dennett 2022; Singleton, Wilson, and O’Brien 2012; Rowe et al. 2022). In spatial interaction literature, the strength of connectivity between locations is generally quantified through gravity models, which incorporate the effect of relative distance on the strength of interaction between origin and destination locations (Erlander 1980; Haynes and Fotheringham 1985). Typically, an increase in distance leads to a decrease in spatial interaction, known as ‘distance decay’ (Taylor and Openshaw 1975). While conceptualising geographic connections in this manner builds a picture that is constrained by spatio-temporal movement, this is not necessarily a requirement for a more generalised understanding of associations as a geographic concept (Merriman 2012).\nUnlike physical connections, which are described by movements across Euclidean space, cognitive associations are decoupled from the restriction of physical movements; both the distance between locations, and the time taken to travel between them, do not directly influence the strength of cognitive associations. Instead, they capture the persistent perceived associations that reflect the experiential geographic knowledge used by individuals to generate ‘mental maps’ (Gould and White 1986). Such associations capture non-specific subconscious links between locations, influenced by personal experiences, incorporating cultural similarities (Greenberg Raanan and Shoval 2014), distortion through commuting methods and navigation technologies (Peake and Moore 2004), online communication (Zook 2006), and other influences on cognitive bias. For example, transport access and telecommunication warp a general sense of perceived geographic distance between certain locations (Massey 2008), and the cognitive understanding of ‘nearness’ does not necessarily correlate with Euclidean distance (Fabrikant et al. 2002; Worboys 2001; Montello 1993). These implicit associations between places are generated through a complex network of socio-spatial relationships, built through linked individual experiences, and allow for shared experiences of places to captured (Pierce, Martin, and Murphy 2011).\nTraditional approaches to the exploration of differences between cognitive and real-world associations would have relied on the use of large-scale studies and active individual participation to derive these associations, for example through volunteered geographic information (VGI) (Goodchild 2007), or participatory mapping (Chambers 2006; Pánek 2016). Additionally, while mental maps may be generated through hand-drawn sketches, such methods do not scale well to derive a population level understanding.\nBy contrast, alternative forms of data online present an opportunity to infer associations between places, by capturing persistent links that are not temporally or spatially bounded. Facebook for example has been used to generate social connections between locations, geographically grounded based on the home location of two friends (Bailey et al. 2018). Geographic networks may also be generated from crowdsourced databases like Wikipedia (Salvini and Fabrikant 2016), which demonstrate connections between cities based on hyperlinks embedded in articles that contain general knowledge. Unlike these structured data sources, unstructured online text also provides embedded geographic information as place names, which may be extracted through computational techniques, such as natural language processing (Purves et al. 2018; Berragan et al. 2022).\nRelationships between locational mentions in text are typically examined through co-occurrences, where locations that are frequently mentioned in a shared context are assumed to have a real-world relationship (Hu, Ye, and Shaw 2017; Ye, Gong, and Li 2021; Liu et al. 2014; Meijers and Peris 2019; Ballatore, Bertolotto, and Wilson 2014). Current research has however concentrated primarily on examining the relationships between city names on news articles (Hu, Ye, and Shaw 2017), or general web pages (Liu et al. 2014), where locational mentions do not necessarily capture a collective and generalised view generated from the mental perceptions of geography that exist within populations.\nAlternative sources include online social media, which contribute a large volume of natural language text submitted by many unique users, discussing a range of informal topics, typically with shared user interactions. Place names discussed on social media more frequently include fine-grained locations (Han et al. 2018; C. Li and Sun 2014), and given interactions are often more informal, the information captured likely exhibit user cognitive biases related to their own mental maps (Jang and Kim 2019). While past work has built co-occurring place names from single news articles or documents, they may instead be built from a user facing perspective, building co-occurrences from comments associated with each user in a large corpus of social media data. We argue that this approach more appropriately captures the cognitive information each user uses to associate two locations, which can then be generalised by combining associations across each user in the corpus.\nThere are however concerns with the use of passively contributed, user-generated data for place-focussed geographic research; primarily with the representativeness of populations, and the bias in contributions (Gardner et al. 2020; Graham, Straumann, and Hogan 2015). For example, despite having over 300 million users, Twitter users typically post from high-density urban areas, rather than where they live (Ballatore and De Sabbata 2020), demographic groups have variable propensity to contribute (Hecht and Stephens 2014; Ballatore and De Sabbata 2018; Gardner et al. 2020), and contributions to gazetteers or digital maps are increased in more densely populated, urban locations (Graham, Straumann, and Hogan 2015; Laurier, Brown, and McGregor 2016; Smith et al. 2020). Another concern with user-generated data comes from the tendency for few users to contribute the greatest proportion of activity (Haklay 2016), meaning that despite a large volume of unique users, there may be bias towards the contributions of certain individuals. In other research methods like participatory mapping, biases often reflect the social and cultural background of the communities contributing their understanding of geographies (Corbett and Rambaldi 2009; Pánek 2016), which in our work equates to the experiential knowledge used to construct those mental maps that inform our cognitive place associations."
  },
  {
    "objectID": "posts/manuscripts/associations/index.html#extracting-locations-from-text",
    "href": "posts/manuscripts/associations/index.html#extracting-locations-from-text",
    "title": "Mapping Cognitive Place Associations within the United Kingdom through Online Discussion on Reddit",
    "section": "Extracting Locations from Text",
    "text": "Extracting Locations from Text\nPast works that considered links between locational mentions in text have identified locations either by querying articles for city names (Hu, Ye, and Shaw 2017), or simply using a word list of city names to parse articles for their occurrences (Meijers and Peris 2019). Such approaches suffer with performance, Meijers and Peris (2019) for example identified that 2.8% of their target place names could refer to multiple locations, while 1% of names were words that appeared in the English vocabulary. In total, they identify that around 15% of their place names displayed some level of ambiguity, and quantitative assessment of the effect of this demonstrated that it negatively impacted the quality of the associations identified. To avoid such issues, instead of a simple rule-based approach for the extraction of place names from our text, we construct a structured process using machine learning. This implements geoparsing, which is the process of extracting place names from unstructured text and matching them to the correct associated geographic coordinates (Purves et al. 2018). This task can be divided into two stages; identifying place names in text, followed by the association of these place names with a unique identifier in a knowledge base (typically a gazetteer) in a process called geocoding or toponym disambiguation.\nModern geoparsing processes use Named Entity Recognition (NER) to identify place names from natural language text (Purves et al. 2018; Karimzadeh et al. 2019; Halterman 2017). Unlike simpler methods which use knowledge or rule-based methods (Leidner and Lieberman 2011), NER uses more complex supervised machine learning to identify place names. The use of machine learning allows for the identification of place names that do not already appear within formal gazetteers, which is particularly useful in research considering colloquial names (Hollenstein 2008). Word context may also be used to improve accuracy, as words may appear in a gazetteer but not be used in a geographic context (Reading could be considered a place in the UK or a noun) (Purves et al. 2018). This is particularly important when considering informal text, where capitalisation may not always indicate the use of proper nouns, misspellings may be frequent, and names that do not often appear in gazetteers are common.\nRecent work however has noted that current geoparsing systems using existing NER models do not necessarily perform well for the task of place name extraction (Berragan et al. 2022). Such pre-built models do not always consider geographically specific issues like the use of metonyms (Gritta, Pilehvar, and Collier 2020), and are typically trained on news articles, which limits their performance on other forms of text, like social media (Won, Murrieta-Flores, and Martins 2018; Berragan et al. 2022). For toponym disambiguation, the global GeoNames3 database is typically used as a gazetteer in these geoparsing systems, which has limited data for fine-grained locations in the United Kingdom (Stock et al. 2013; Moncla et al. 2014), while increasing potential noise with the inclusion of place names outside the UK. As such, existing geoparsers were considered unsuitable for our task; geoparsing UK place names within Reddit comments, with the inclusion of fine-grained locations."
  },
  {
    "objectID": "posts/manuscripts/associations/index.html#geoparsing-reddit-comments",
    "href": "posts/manuscripts/associations/index.html#geoparsing-reddit-comments",
    "title": "Mapping Cognitive Place Associations within the United Kingdom through Online Discussion on Reddit",
    "section": "Geoparsing Reddit Comments",
    "text": "Geoparsing Reddit Comments\n\n\nReddit4 is a public discussion, news aggregation social network, and among the top 20 most visited websites in the United Kingdom. As of 2020, Reddit had around 430 million active monthly users, comparable to the number of Twitter users (Murphy 2019; Statista 2022). Reddit is divided into separate independent subreddits each covering specific topics of discussion, where users may submit posts that have dedicated nested conversational threads enabling users to add and respond to comments. Subreddits cover a wide range of topics, and in the interest of geography, they also act as forums for the discussion of local places. The United Kingdom subreddit5 acts as a general hub for related topics, notably including a list of smaller and more geographically specific related subreddits. This list provides a ‘Places’ section, a collection of local British subreddits, ranging in scale from country level (/r/England), regional (/r/thenorth, /r/Teeside), to cities (/r/Manchester) and small towns (/r/Alnwick). In total there are 213 subreddits that relate to ‘places’ within the United Kingdom6. For each subreddit, every single historic comment was retrieved using the Pushshift7 Reddit archive (Baumgartner et al. 2020). In total 8,070,827 comments were extracted, submitted by 490,534 unique users, between 2011-01-01 and 2022-04-17, this represents a very large corpus of text comprising 262 million words.\n\n\nWe then implemented our own geoparsing methodology to extract and geolocate any place name mention within each comment text. We first identified all place name mentions using a custom-built NER model8. This model was built using a large language model called BERT (Devlin et al. 2019), which is pre-trained on a large corpus of general human text, meaning for tasks like NER it performs better compared with simpler models. Our NER model was then trained to identify all place names within this corpus. Coordinate information was attributed with all identified place names, using OS Open Names9, and ‘natural’ locations from the Gazetteer of British Place Names10. Given place names typically appear multiple times in gazetteers, a disambiguation method was required. We therefore disambiguated place names by finding their minimum distance to a collection of contextual locations. Contextual locations in this case referred to all gazetteer entries matching place names that appear in sentences with this target place name, within the same subreddit. This worked under the assumption that each unique place name in a single subreddit is likely to refer to the same location, and that locations mentioned in surrounding text are likely geographically close together (Kamalloo and Rafiei 2018). When associating locations with coordinate information, we excluded any location that was larger than a city, for example countries or regions.\n\n\nOur final dataset therefore consisted of a collection of place names with their geographic coordinates, corpus location, and an anonymised user ID for the user of the comment the place name was taken from. In total, 213,764 unique users mentioned at least one place name in our corpus, 39,050 mentioned more than 10 place names, and 3,158 over 100. 1% of these contributed 32% of all place names, representing the top 2,137 users. As is common in user-generated content, our data are skewed in that proportionally few users mention a large proportion of our total place names. The large volume of unique users that contribute low volumes of comments do however mean that we likely still achieve a broad representation, particularly compared with past work that generated mental maps for a limited number of individuals (Goodchild and Li 2012). As our comments spanned a period of over 10 years, we also examined the temporality of contributions made by users. The mean time between a user’s first and final comment is 318.1 days, with a maximum of 4112 days. As such, the contributor distribution is highly skewed, as the majority of users (55%) only have commented a maximum of 1 day apart."
  },
  {
    "objectID": "posts/manuscripts/associations/index.html#place-associations-through-co-occurrence",
    "href": "posts/manuscripts/associations/index.html#place-associations-through-co-occurrence",
    "title": "Mapping Cognitive Place Associations within the United Kingdom through Online Discussion on Reddit",
    "section": "Place associations through Co-occurrence",
    "text": "Place associations through Co-occurrence\n‘Cognitive association strength’ is defined in our paper as the normalised proportion of co-occurrences between two locations in our corpus, where co-occurrences represent the total collection of locations mentioned by a single user. The following section first outlines the construction of distance decay measures using a gravity model that incorporates cognitive association strength alongside distance, then details how we generate a scaled measure of this cognitive association strength. The first measure enables us to quantify how distance impacts our association strength, determining whether there is an observable distance decay effect when considering locational co-occurrences in user comments. The second enables the direct strength of association between locations to be examined, without the incorporation of distance in the calculation.\nTo measure the effect of distance decay we employ the same gravity model used by both Liu et al. (2014) and Hu, Ye, and Shaw (2017), shown on Equation 1:\n\\[\n\\mathbf{S}_{i j} \\propto \\frac{\\mathbf{S}_{i} \\mathbf{S}_{j}}{d_{i j}^{\\beta}},\n\\tag{1}\\]\nwhere \\(\\mathbf{S}_{ij}\\) is the total number of users that mention both places \\(i\\) and \\(j\\), and \\(\\mathbf{S}_{i}\\mathbf{S}_{j}\\) is the total number of users that mention place \\(i\\), multiplied by the total number of users that mention place \\(j\\). \\(d_{ij}\\) is the distance between the two locations \\(i\\) and \\(j\\), and \\(\\beta\\) is the friction factor. Larger values for \\(\\beta\\) indicate a stronger distance decay effect. Estimating the value of \\(\\beta\\) generates a quantifiable measure of the distance decay effect (Hu, Ye, and Shaw 2017).\nWe can decompose Equation 1 into the following multiple linear regression model (Taaffe 1996):\n\\[\n\\log(\\mathbf{S}_{ij}) = b_{0} + b_{1}\\log(\\mathbf{S}_{i}\\mathbf{S}_{j}) + b_{2}\\log(d_{ij}),\n\\tag{2}\\]\nwhere \\(b_2 = -(\\beta * b_1)\\), meaning we can calculate our \\(\\beta\\) coefficient using \\(\\beta = -(b_2 / b_1)\\) (Hu, Ye, and Shaw 2017; C. Li and Sun 2014).11\nWhile this approach enables the calculation of a global \\(\\beta\\) to measure distance decay, a spatial regression model would enable us to calculate local values of \\(\\beta\\), quantifying the distance decay effect on individual locations (Rey, Arribas-Bel, and Wolf 2023). We therefore additionally implement a spatial regression model which incorporates a fixed spatial effect for the H3 polygon name, allowing for \\(\\beta\\) coefficients to be calculated for each location in our study, to explore spatial heterogeneity.\nFinally, we generate a normalised cognitive association measure to assess the strength between two locations. Unlike the previous gravity models, co-occurrences are not incorporated alongside distance. This mirrors similar work that considered the strength of social connections between Facebook (Bailey et al. 2018), and Twitter users (Z. Li et al. 2021), enabling the direct strength of associations to be generated:\n\\[\n\\frac{\\mathbf{S}_{i j}}{\\sqrt{\\mathbf{S}_{i} \\mathbf{S}_{j}}}\n\\tag{3}\\]\nIn this equation, dividing by \\(\\sqrt{\\mathbf{S}_{i} \\mathbf{S}_{j}}\\) normalises our values, given locations with higher populations are expected to be mentioned by a larger number of users. Values therefore range from 0 indicating no association, to 1, showing a complete overlap in user mentions.\nTo present the results of our analysis we aggregate our user location mentions into H3 hexagons12, a hierarchical spatial indexing system which partitions all locations across earth into a uniform hexagonal grid, available for different levels of aggregation. We select an H3 resolution of 5 which equates to an average hexagon area of 252 km2 and an edge length of 9.8 km. All associations between each location contained within a shared H3 hexagon are then combined, forming association measures between hexagons, rather that unique point locations. To name hexagons we select the most frequently occurring location.\nThe use of fixed unit size hexagons for aggregating data in our analysis is beneficial for several reasons. Firstly, hexagons are geometries that enable us to obtain results that are statistically more robust especially when analysing distance decay between locations, because of the constant number of neighbours, with an equal distance separating them (Birch, Oom, and Beecham 2007). Secondly, hexagon grids help to minimise misrepresentation in spatial visualisation (Langton and Solymosi 2021), and allow us to capture inter-region heterogeneity. Finally, aggregation is essential given the data representations of locations within gazetteers; despite many locations having large footprints, they are all represented as a single coordinate pair (Goodchild and Hill 2008). This problem means that despite users mentioning locations like parks within cities, without aggregation they are treated as two distinct points, with a geographic distance separating them. Alternatively OpenStreetMap can be used to provide more accurate place footprints, but at the cost of a very large data volume when considering the entirety of the UK (Haklay and Weber 2008).\nAdditionally, we classify our H3 hexagons into both rural and urban using the England and Wales Rural Urban Classification13 and Scotland Rural Urban Classification14. For Scotland classes 1 and 2 were considered Urban."
  },
  {
    "objectID": "posts/manuscripts/associations/index.html#extracting-names-and-locations-assessing-geoparsing-performance",
    "href": "posts/manuscripts/associations/index.html#extracting-names-and-locations-assessing-geoparsing-performance",
    "title": "Mapping Cognitive Place Associations within the United Kingdom through Online Discussion on Reddit",
    "section": "Extracting Names and Locations: Assessing Geoparsing performance",
    "text": "Extracting Names and Locations: Assessing Geoparsing performance\n\n\nIn total, 26.8% of all comments within the Reddit corpus contained at least one place name: 5,001,261 place names were identified, with 2,848,310 (57.0%) being attributable to a set of coordinates15. From these locations, 42,333 were found to be unique, of which 21,014 were only mentioned a single time, while London was the most frequently encountered location, at 283,521 mentions. The most ambiguous place name was found to be ‘High Street’, with 47 total unique coordinate locations. As expected, many of the most ambiguous place names were street names, including ‘Church Street’ (36 locations), ‘Bridge Street’ (34 locations), and ‘London Road’ (34 locations).\n\n\n\n\n\n\n\n\n\n\nFigure 1: Locations of three place names that appear in the UK gazetteer that are difficult to correctly disambiguate. Size of the green points indicate frequency in mentions, black points are user locations determined through mean locational mentions. Values indicate the proportional contributions of each disambiguated location to their respective polygon (Top four percentages shown).\n\n\n\n\n\nIn Figure 1 we consider three examples where place names may have been incorrectly geoparsed. Figure 1 (a) shows the geographic distribution of all 47 ‘High Street’ locations. The percentage values indicate the proportion of ‘High Street’ mentions within a particular H3 polygon, compared to all other locations in this polygon. Aggregation here appears to mitigate the risk of noise in most cases, given most ‘High Street’ locations contribute lower than 1% towards polygon associations. A similar case is shown in Figure 1 (b), where ‘City Centre’ mentions only account for 3.2% of the Manchester hexagon. Figure 1 (c) instead demonstrates a location that is impossible to correctly geoparse in our model, and despite there being 13 unique locations in the UK called ‘California’, this issue only appears prominent in one hexagon. This hexagon named ‘California’ does potentially generate noise in our analysis, given the high contribution of 85.1%. However, as users that mention ‘California’ are spread across the country, it is less likely to largely impact our associations.\n\n\nNotably, despite both ‘High Street’ and ‘City Centre’ being shared with non-specific geographic concepts, the model is still able to distinguish between them depending on context. For example, ‘city centre’ appears 23,961 times in our corpus, but is only tagged by our NER model 3,008 times. While ‘high street’ appears 7,773 times and is only tagged 768 times. These results suggest that the model is often able to correctly understand that identical phrases may or may not refer to place names, depending on their semantic context."
  },
  {
    "objectID": "posts/manuscripts/associations/index.html#measuring-distance-decay-of-cognitive-association-strength",
    "href": "posts/manuscripts/associations/index.html#measuring-distance-decay-of-cognitive-association-strength",
    "title": "Mapping Cognitive Place Associations within the United Kingdom through Online Discussion on Reddit",
    "section": "Measuring Distance Decay of Cognitive Association Strength",
    "text": "Measuring Distance Decay of Cognitive Association Strength\nIn the following section we present the levels of distance decay observed when evaluating place association strength through the gravity model specified in Equation 1, quantifying the level of distance decay using a \\(\\beta\\) coefficient. As calculated, higher \\(\\beta\\) coefficient values indicate a stronger distance decay effect, meaning that co-occurrences between locations that are geographically more distant tend to be less frequent. A \\(\\beta\\) value of zero would indicate that distance has no effect on the frequency of co-occurrences between locations.\n\n\nOur gravity model gives a \\(\\beta\\) coefficient of 1.00 (Pearson’s R2: 0.772), reflecting a distance decay from co-occurrences in Reddit comments that is stronger than decay observed in other studies that explored news articles (0.23) (Hu, Ye, and Shaw 2017), or general web queries (0.2) (Liu et al. 2014). Confirming the existence of a general distance decay effect for Reddit derived places demonstrates that distance typically contributes to lower co-occurrences in locations that are further apart, a similarity that is shared with past work that examines decay from the perspective of true population movements (Gong et al. 2021; Yang, Li, and Li 2019), and the social relationships of regions examined through social media (Bailey et al. 2018; Z. Li et al. 2021). Our place associations generated from users on Reddit therefore appear to more appropriately incorporate a geographic component, compared with city mentions in news articles or general web pages. However, while this gravity model gives us an indication of the global level of distance decay in our corpus, it is likely that the level of distance decay varies by location. In the following analysis we therefore consider locations where the gravity model does not achieve a good approximation.\n\n\n\n\n\n\n\n\n\n\nFigure 2: H3 polygons showing (a) Top 20 associations by residual values in green (&gt;0), and (b) bottom 20 associations by residual values (&lt;0) in red. (c) Residuals taken from Equation 2 against co-occurrence strength (10,000 samples). (*) Indicates a location that is incorrectly geoparsed.\n\n\n\n\n\n\n\nFigure 2 (a) and (b) plots the top (most positive) and bottom (most negative) 20 residuals from our gravity model, demonstrating associations that are stronger or weaker than expected, when accounting for the distance between two regions. Many of the top residuals concern associations shared with London and other major cities in the UK, with some associations between urban areas in Scotland. The most positive residual is the association between London and Edinburgh (3.89), Glasgow and London in second (3.84), and Glasgow and Edinburgh in third (3.81). As expected, these residuals reflect a strong association between regions over larger distances (mean 293 km), highlighting associations where distance decay is less effective. Notably, there is an incorrect association here between a natural feature named ‘London Bridge’ and London, which has appeared due to the lack of urban landmarks in our gazetteer. The bottom residuals are more sporadic, typically showing associations over shorter distances (mean 156 km), between lesser known locations that are unusually weak. For example, highlighted on this figure is the association between Swansea Bay and Southampton (-2.55). Figure 2 (c) plots the model residuals against cognitive association strength, showing that for locations with a greater proportion of co-occurrences, the model is likely to be under-estimating in prediction, leading to an over-prediction in distance decay, with the inverse true for locations with a lower proportion of co-occurrences."
  },
  {
    "objectID": "posts/manuscripts/associations/index.html#regional-difference-in-distance-decay",
    "href": "posts/manuscripts/associations/index.html#regional-difference-in-distance-decay",
    "title": "Mapping Cognitive Place Associations within the United Kingdom through Online Discussion on Reddit",
    "section": "Regional Difference in Distance Decay",
    "text": "Regional Difference in Distance Decay\nFigure 2 demonstrates that there are clear regional variations in the observed level of distance decay, which do not conform with the general decay effect calculated through our proposed gravity model approximation. To examine regional effects on distance decay, we implement a spatial regression model (a mixed linear model with spatial fixed effects), allowing \\(\\beta\\) values to change depending on the location of each polygon.\n\n\n\n\n\n\n\n\nFigure 3: H3 polygons showing (a) Distribution of geoparsed locations. (b) Urban rural classification index for England, Wales, and Scotland, reclassified into binary ‘Urban’ or ‘Rural’. (c) Calculated \\(\\beta\\) coefficients for the spatial regression model; higher \\(\\beta\\) values indicate a greater distance decay strength.\n\n\n\n\n\n\n\nIncorporating this spatial information gives a more effective approximation of our gravity model, and achieves an improved Pearson’s R2 of 0.946, suggesting that distance decay is not uniform across all regions in our study. Figure 3 (a) shows the distribution of locations mentioned in our study, which broadly conform with the binary urban rural classification shown on Figure 3 (b). Figure 3 (c) maps the spatial \\(\\beta\\) coefficients obtained through our spatial regression model, with high distance decay present across Scotland, Wales, and areas in the South West and North East of England. Users that mention locations in these regions typically do not mention other locations that are geographically distant, highlighting areas that are either more isolated from the rest of the UK, or have stronger associations with nearby locations.\nIn the North East, this perceived isolation from the rest of the UK mirrors lexical research, where Tweets in the North East have been shown to be unlike other regions (Arthur and Williams 2019). This region in particular has been known to suffer economically following the historic decline of local industries (Middleton and Freestone 2008), where lack of job opportunities has resulted in poor inward migration, with among the lowest population growth in the country (Office for National Statistics 2022). Alternatively, this observation may be also attributable to a general sense of identity that is associated with these regions. Both the South West and North East of England are known to exhibit a strong sense of localised identity (Deacon 2007; Middleton and Freestone 2008), which is similarly translatable to the national identity that generates strong associations within Scotland and Wales, that are not shared with England (Haesly 2005).\nTo explicitly quantify the difference in distance decay between urban and rural areas we calculate separate \\(\\beta\\) coefficient values based on the binary split of areas into urban or rural. Urban areas have a \\(\\beta\\) coefficient of 0.68, while rural areas had a \\(\\beta\\) coefficient of 1.14, indicating that urban areas do appear to have a lower overall level of distance decay compared with rural regions. This correlates with the results of traditional mobility studies where more populated areas tend to exhibit a lower distance decay (Thomas 1981), largely dictated by the improved accessibility to external locations through public transport, the road network or job opportunities (Moseley 2023; Findlay, Short, and Stockdale 2000), and the general cultural significance that is more frequently associated with urban locations (Lynch 1964; Borer 2006).\n\n\nWe have demonstrated that not only does distance decay vary between rural and urban locations, but within these classes there is also apparent heterogeneity. In the following section, we therefore consider the ability to directly map the strength of cognitive associations with respect to a selection of both rural and urban regions in our study, to understand the variation in distance decay patterns."
  },
  {
    "objectID": "posts/manuscripts/associations/index.html#mapping-cognitive-place-associations",
    "href": "posts/manuscripts/associations/index.html#mapping-cognitive-place-associations",
    "title": "Mapping Cognitive Place Associations within the United Kingdom through Online Discussion on Reddit",
    "section": "Mapping Cognitive Place Associations",
    "text": "Mapping Cognitive Place Associations\n\n\n\n\n\n\n\n\nFigure 4: Data subsets with respect to eight selected locations showing cognitive association strength associated with each H3 polygon containing each named location (highlighted in green), \\(\\beta\\) values generated from data subsets. Distance decay plots below maps show association strength against distance for each selected location. Lines show rolling mean for 250 samples in black and lower samples in grey.\n\n\n\n\n\nIn Figure 4 we map the cognitive association strength of each H3 polygon in our study, with respect to four major cities, and four rural locations in the UK, also indicating the associated \\(\\beta\\) coefficients. Mapped cognitive association values are given by Equation 3, and indicate the proportion of users that have comments that mention locations both within the target polygon (e.g. London), and locations in other polygons. Distance decay curves for each polygon are shown below these maps, indicating patterns in decay associated with each location. London has the lowest \\(\\beta\\) coefficient of all cities, indicating that locations at increasing distance from London decay in their association at a slower rate compared with other cities. This is reflected by the shallow overall decay curve for London, increasing at points associated with main urban conurbations in England and Scotland, observable on the map for Figure 4 (a). Such trends are perhaps unsurprising given London’s prominence as the capital city. Manchester on Figure 4 (b) reveals a different decay pattern, showing a sharp drop in associations initially, that reduces and reverses when cities like London or Edinburgh are included in the distribution. Unlike Manchester, Newcastle (d) has an overall greater \\(\\beta\\) coefficient, where association strength drops more quickly and is less persistent across England, only increasing with urban locations in Scotland, and slightly with London. While both are major cities in England, Manchester is both physically more well connected to the rest of the country through existing rail routes (Miyoshi and Givoni 2013), and is a greater economic centre compared with Newcastle. These factors likely contribute to the perceived strength of associations with these cities, which is captured in our analysis.\nEdinburgh (c) is distinct compared with other cities, with a steeper initial decay curve compared with London, largely dictated by stronger initial associations with Scottish locations. Again, as is common for many cities in the UK, this city also shares a strong association with London, regardless of the distance. This increased strength of association with locations within Scotland gives Edinburgh the highest \\(\\beta\\) coefficient, an effect that captures the strong sense of identity between areas in Scotland (Haesly 2005).\nFigure 4 (e-g) give examples of variable distance decay curves for rural locations across the UK. Both ‘(e) Milford Haven’ in Wales, and ‘(f) Cowel’ in Scotland share general associations across each respective country, which appears to drop past the border into England. This similarly captures the sense of national identity associated with both Wales and Scotland, and conforms with results from the analysis of both physical and networks, where strong ‘boundary effects’ often see intra-connectivity within regions, that becomes weaker when moving across borders (Z. Li et al. 2021; Bailey et al. 2018; Arthur and Williams 2019; Yin et al. 2017). Given a national identity is less prominent in England, the town ‘(g) St Austell’ gives a steep distance decay curve, with low association strength between any location more than 50 km away, a noticeably different curve compared with the rural locations analysed in Scotland and Wales.\nWe also examine the incorrectly disambiguated ‘(h) California’ polygon, and confirm that the distance decay curve does not appear to show a geographically cohesive pattern, with no noticeable gradient. The positive \\(\\beta\\) coefficient appears to relate with an unexplained increase in association with Scotland, however values remain low."
  },
  {
    "objectID": "posts/manuscripts/associations/index.html#footnotes",
    "href": "posts/manuscripts/associations/index.html#footnotes",
    "title": "Mapping Cognitive Place Associations within the United Kingdom through Online Discussion on Reddit",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOrdnance Survey UK does not include data for Northern Ireland.↩︎\nIn our work we consider ‘place names’ to be ambiguous noun phrases found in text, without associated coordinates, while ‘locations’ are place names that have attributed coordinates. We use ‘place associations’ to capture the vagueness involved in cognitive associations.↩︎\nhttps://www.geonames.org↩︎\nhttps://reddit.com↩︎\nhttps://reddit.com/r/unitedkingdom↩︎\nhttps://www.reddit.com/r/unitedkingdom/wiki/british_subreddits↩︎\nhttps://pushshift.io/↩︎\nURL provided following submission↩︎\nhttps://www.ordnancesurvey.co.uk/business-government/products/open-map-names↩︎\nhttps://gazetteer.org.uk↩︎\nFor a more detailed mathematical explanation see Hu, Ye, and Shaw (2017)↩︎\nhttps://www.uber.com/en-GB/blog/h3/↩︎\nhttps://www.gov.uk/government/collections/rural-urban-classification↩︎\nhttps://www.gov.scot/publications/scottish-government-urban-rural-classification-2020↩︎\nNote that many names absent from our gazetteer include locations outside the UK↩︎"
  },
  {
    "objectID": "posts/manuscripts/transformers/index.html",
    "href": "posts/manuscripts/transformers/index.html",
    "title": "Transformer based named entity recognition for place name extraction from unstructured text",
    "section": "",
    "text": "Place names are frequently encountered in natural language and provide an additional geographic dimension to much of the textual information present online, when associated with spatial coordinates and geographic locations. Despite this, research in place name extraction primarily concentrates on entities as described by annotation schemes that do not explicitly consider geographic place names (Karimzadeh et al. 2019; Halterman 2017; Hu, Mao, and McKenzie 2019). Pre-built named entity recognition (NER) models based on these schemes are also not task specific; trained on data unrelated to the task they are used for, despite language involving place names varying significantly depending on the context (Purves et al. 2018). When identifying place names in text, research typically only considers known administrative names and their associated strict boundaries, despite natural language often containing place names that either do not exist formally, are hyper-localised e.g. street names, or are alternative names that may be absent from administrative databases, which often only consider a single formal name.\nThe training corpora used by pre-built NER models typically identifies a number of entities that have no relevance to geographic place names, e.g. persons, and those that have some relevance in specific contexts; locations, geopolitical entities or facilities (Weischedel et al. 2013; Tjong Kim Sang and De Meulder 2003). Notably, they do not specifically target a ‘place name’ entity, meaning, while often these three related entity types may often refer to a place name, this is not always the case. Additionally, these corpora consist of text that often differs in structure, compared with the text being processed by models trained using them; for example, social media text is typically more informal compared with the news articles used to build the popular dataset, CoNLL03 (Tjong Kim Sang and De Meulder 2003).\nNew forms of geographic information online present an opportunity to train and evaluate models on texts that contain a large volume of place names (Goodchild and Li 2011), building models from the ground up, and using annotation schemes that are explicitly designed for the extraction of place names from text. Results from these models are expected to outperform existing pre-built models which use unrelated training data, and do not include a ‘place name’ entity type.\nOur paper presents five NER models, trained on manually labelled Wikipedia data and used to identify and extract any span of text considered to be a place name, from articles relating to geographic locations in the United Kingdom. Our model is evaluated against pre-built solutions that are commonly used for this task, demonstrating the importance of model training with task specific data, and the consideration that named entity recognition as a task is not appropriate for place name extraction, due to the exclusion of a ‘place name’ entity type, and the inclusion of a number of unrelated entities. New developments in natural language processing (NLP) are utilised, outlining the benefit of selecting modern architectures that are not yet implemented by off the shelf models. Our paper considers the ability to extract place names from Wikipedia articles for the United Kingdom that do not appear in the GeoNames Gazetteer, with the goal of identifying the additional geographic information that may be effectively extracted from unstructured sources of online text.\nSection 2 outlines the research and concepts associated with geography in NLP, considering its relation to the new forms of geographic data present online, the techniques in natural language processing that explicitly deal with geography, and the developments in NLP that have enabled higher accuracy with limited labelled data. Section 3 presents the workflow undertaken for the models constructed in this paper, as well as the data collection and analysis of the entities extracted. The performance of each NER model is then presented in Section 4 and evaluated against pre-built solutions using a corpus of labelled test data. Place names are extracted using the model for the entire Wikipedia corpus, and compared against GeoNames, identifying names that are not present, discussing the reasons they may be found within Wikipedia articles, but not in an explicitly geographic gazetteer."
  },
  {
    "objectID": "posts/manuscripts/transformers/index.html#introduction",
    "href": "posts/manuscripts/transformers/index.html#introduction",
    "title": "Transformer based named entity recognition for place name extraction from unstructured text",
    "section": "",
    "text": "Place names are frequently encountered in natural language and provide an additional geographic dimension to much of the textual information present online, when associated with spatial coordinates and geographic locations. Despite this, research in place name extraction primarily concentrates on entities as described by annotation schemes that do not explicitly consider geographic place names (Karimzadeh et al. 2019; Halterman 2017; Hu, Mao, and McKenzie 2019). Pre-built named entity recognition (NER) models based on these schemes are also not task specific; trained on data unrelated to the task they are used for, despite language involving place names varying significantly depending on the context (Purves et al. 2018). When identifying place names in text, research typically only considers known administrative names and their associated strict boundaries, despite natural language often containing place names that either do not exist formally, are hyper-localised e.g. street names, or are alternative names that may be absent from administrative databases, which often only consider a single formal name.\nThe training corpora used by pre-built NER models typically identifies a number of entities that have no relevance to geographic place names, e.g. persons, and those that have some relevance in specific contexts; locations, geopolitical entities or facilities (Weischedel et al. 2013; Tjong Kim Sang and De Meulder 2003). Notably, they do not specifically target a ‘place name’ entity, meaning, while often these three related entity types may often refer to a place name, this is not always the case. Additionally, these corpora consist of text that often differs in structure, compared with the text being processed by models trained using them; for example, social media text is typically more informal compared with the news articles used to build the popular dataset, CoNLL03 (Tjong Kim Sang and De Meulder 2003).\nNew forms of geographic information online present an opportunity to train and evaluate models on texts that contain a large volume of place names (Goodchild and Li 2011), building models from the ground up, and using annotation schemes that are explicitly designed for the extraction of place names from text. Results from these models are expected to outperform existing pre-built models which use unrelated training data, and do not include a ‘place name’ entity type.\nOur paper presents five NER models, trained on manually labelled Wikipedia data and used to identify and extract any span of text considered to be a place name, from articles relating to geographic locations in the United Kingdom. Our model is evaluated against pre-built solutions that are commonly used for this task, demonstrating the importance of model training with task specific data, and the consideration that named entity recognition as a task is not appropriate for place name extraction, due to the exclusion of a ‘place name’ entity type, and the inclusion of a number of unrelated entities. New developments in natural language processing (NLP) are utilised, outlining the benefit of selecting modern architectures that are not yet implemented by off the shelf models. Our paper considers the ability to extract place names from Wikipedia articles for the United Kingdom that do not appear in the GeoNames Gazetteer, with the goal of identifying the additional geographic information that may be effectively extracted from unstructured sources of online text.\nSection 2 outlines the research and concepts associated with geography in NLP, considering its relation to the new forms of geographic data present online, the techniques in natural language processing that explicitly deal with geography, and the developments in NLP that have enabled higher accuracy with limited labelled data. Section 3 presents the workflow undertaken for the models constructed in this paper, as well as the data collection and analysis of the entities extracted. The performance of each NER model is then presented in Section 4 and evaluated against pre-built solutions using a corpus of labelled test data. Place names are extracted using the model for the entire Wikipedia corpus, and compared against GeoNames, identifying names that are not present, discussing the reasons they may be found within Wikipedia articles, but not in an explicitly geographic gazetteer."
  },
  {
    "objectID": "posts/manuscripts/transformers/index.html#sec-trans-review",
    "href": "posts/manuscripts/transformers/index.html#sec-trans-review",
    "title": "Transformer based named entity recognition for place name extraction from unstructured text",
    "section": "Literature review",
    "text": "Literature review\nNatural language often describes places using imprecise referents, non-administrative names, and an understanding of place footprints that does not conform with the formal administrative boundaries given to them (Gao, Janowicz, and Couclelis 2017; Goodchild and Li 2011). Despite this, regions and place names in computational geography are usually formally defined by administrative datasets, meaning any informal place names are unable to be identified, or associated with a position in space. This distinction has given rise to a focus on place based GIS, rather than space based, which considers the ability to capture place references that may not appear in administrative datasets (Gao et al. 2013).\nSince the advent of Web 2.0, increased access to mobile devices which include passive GPS and open-access mapping information, several scientific disciplines have developed to take advantage of the data being produced, including crowdsourcing, and user-generated content (See et al. 2016). With geographically referenced content through social media, mapping platforms and Wikipedia there is now a wealth of information that Goodchild (2007) terms ‘Volunteered Geographic Information’ (VGI). These data sources present a large collection of continually updated references to places, often providing informal and unstructured geographic information.\nMuch of the past work using VGI has concentrated either on explicitly geographic crowd-sourced mapping platforms like Open Street Map (Antoniou, Morley, and Haklay 2010), or ‘geotagged’ content which enables, often passively contributed, user-generated data through sites like Twitter or Flickr, used to extract geographic information. Gao, Janowicz, and Couclelis (2017) for example present an approach for the construction of cognitive regions from various VGI sources, querying place names found in tags with associated geotags to create vague boundaries. A similar approach is taken by Hollenstein and Purves (2010) who identified tags containing vague spatial concepts like ‘downtown’ and ‘citycentre’, deriving regions from geotags. These methods demonstrate the ability to derive informal geographic information from VGI, while giving similar results to that of manually collected questionnaire data (Twaroch et al. 2019; Gao, Janowicz, and Couclelis 2017).\nWhile this work concentrates solely on the use of geotags and short single phrase tags associated with social media documents to analyse ‘place’ focussed geographies, another source of online information that is less frequently considered to have geographic properties is unstructured text, which has the potential to provide an even larger source of geographically focussed information. Good results have been reported using basic semantic rules to identify places names found in unstructured text (Moncla et al. 2014), however, these methods have relied on this text almost solely containing place names as entities. Alternatively to rule-based approaches, Hu, Mao, and McKenzie (2019) demonstrate the use of four pre-trained NER models to extract local, informal place names from housing advertisements descriptions with associated coordinates, to enrich existing gazetteers with place names not normally present, alongside derived boundaries. The results of this paper show the promising ability for NER models to extract informal place names directly from text, also demonstrating a bottom-up approach to gazetteer construction, enabling informal place definitions to be captured from VGI, that may be absent from administrative datasets. Model evaluation however showed low precision and recall when evaluating against a labelled dataset, reflecting issues with the use of pre-built NER models for this task. Similar evaluation results are observed by Karimzadeh et al. (2019) when considering various pre-built NER models for use in the GeoTxt geoparsing system, which uses either SpaCy or Stanza pre-built models (Qi et al. 2018; Honnibal and Montani 2017). While the precision of these pre-built NER models can be relatively high for more sophisticated models, they all suffer from low recall. Karimzadeh et al. (2019) note particularly that while improved results would be expected by training a model from the ground up, the amount of labelled training data required to create a suitable model would be very large. To improve the accuracy of systems that rely on place name extraction, NER models should be constructed with more suitable training data, and with annotations tailored for this specific task.\nWhile large, open-access, text-based sources of semantic geographic information are scarce, Wikipedia provides a large collection of articles about almost any subject, many of which relate to geographic locations. This presents an alternative data source for use in geographically focussed NLP applications, with place names, their semantic context, and article geotags providing geographic information. Various studies have used Wikipedia as a data source for the extraction of place names, DeLozier, Baldridge, and London (2015) for example, identify place names in Wikipedia articles and use a clustering technique using document contexts to disambiguate their geographic locations. Speriosu and Baldridge (2013) use geotagged Wikipedia articles to provide contextual information regarding a range of place names for disambiguation. Both these works first use a pre-built Named Entity Recognition (NER) model to identify place names found in text, before further analysis. Improvements made to these NER models for place name extraction present a stronger foundation, leading to both better recall, and precision of place names being identified, before they are resolved to coordinates (Leidner 2008; Purves et al. 2018). Our paper selects Wikipedia articles to demonstrate the geographic information that may be extracted from unstructured text, presenting a first-stage baseline approach for tasks that rely on accurate place name extraction.\n\nNamed entity recognition in the geographic domain\nNatural language processing techniques involving geography typically focus around geoparsing; the automated extraction of place names from text, followed by the resolution of the identified place names to geographic coordinates (Gritta, Pilehvar, and Collier 2020; Leidner 2008; Buscaldi 2011). Modern place name extraction techniques primarily rely on named entity recognition (NER) to identify place names as entities within text (Kumar and Singh 2019; Purves et al. 2018). While most pre-built NER systems are able to identify ‘geopolitical entities’ and ‘locations’ as defined by popular annotation schemes1, these only act as a proxy for place names in text. The majority of entities recognised by these systems are unrelated to place names, and as such simply contribute to lower overall recall when other entities are preferred by models over geographic place names. For example, a model may consider a named organisational headquarters as an ‘organisation’ entity, rather than a ‘location’, even when used as a locational reference.\nThe concept of a place name as an entity defined by the labelled corpora NER models were trained on hinders place name extraction, identifying only (and any) administrative place names in text (Gritta et al. 2017). The geoparser Mordecai2 for example, uses an NER tagger provided through the SpaCy Python library, which provides a variety of entities including those unrelated to place names (e.g. : persons), and three entities that may be considered related, (Geopolitical Entity), (Location), and (Facility). While these categories often do relate to place names, they do not consider whether the entity could be contextually considered a place name that could be geo-located. For example, geopolitical entities are often used in a metonymic sense; a figure of speech where a concept is substituted by a related concept. In the phrase ‘Madrid plays Kiev today’ for example, sports teams are replaced by their associated place name (Gritta, Pilehvar, and Collier 2020). As place name based metonyms do not explicitly relate to geographic locations, and instead a related entity, we are uninterested in their extraction. Due to the reliance on large labelled corpora for NER training, and limited source of geography specific data (Karimzadeh et al. 2019), little work has considered explicitly targeting place names through new data, as it is often time-consuming to produce.\nWhile at present pre-built NER models identify entities as defined by widely used annotated corpora, some work has considered the need to identify spatial entities. SpatialML is a natural language annotation scheme that presents the PLACE tag for any mention of a location (Mani et al. 2010). Tasks identified by the Semantic Evaluation Workshop built on this annotation scheme and defined several entities relating to spatial language (SemEval-2015 Task 8: SpaceEval, Pustejovsky et al. 2015), described by the ISO-Space annotation specification (Pustejovsky 2017). In order to more appropriately consider geography when parsing unstructured text for place related entities, models should be built from the ground up, taking into account an alternative annotation scheme that identifies place names, excluding unrelated entities.\nRecent progress in NLP and the use of GPU accelerated training has brought with it the ability to process large quantities of unlabelled text. This development has recently led to the creation of general purpose ‘language models’ that implement the ‘transformer’ architecture, using semi-supervised learning to train using very large corpora (Vaswani et al. 2017). For example, Google’s pioneering BERT model was trained using the entirety of English Wikipedia, and over 11,000 books (Devlin et al. 2019). This development has led to models which perform well for many given tasks, even with relatively limited additional labelled training data.\nOur paper proposes fine-tuning transformer-based language models for place name extraction using named entity recognition, to extract all place names from UK ‘place’ classed articles on Wikipedia. 200 of these articles are annotated, labelling place names to train and evaluate model performance. We train and compare the performance of three popular transformer-based NER models; BERT - a large, popular transformer model, RoBERTa - similar to BERT, using a different pre-training procedure, which has had better results on some tasks, and DistilBERT - a much smaller and less complex transformer model based on RoBERTa. In addition to these transformer models, two simpler Bidirectional LSTM (BiLSTM) models are compared, one using pre-trained GloVe embeddings, representing an equivalent complexity model used by Stanza or SpaCy pre-built NER solutions, and another showing a baseline model without any pre-trained word embeddings. These models are then evaluated against three pre-built NER systems that are popular for place name extraction, and used in existing geoparsing systems including GeoTxt and Mordecai."
  },
  {
    "objectID": "posts/manuscripts/transformers/index.html#sec-trans-methodology",
    "href": "posts/manuscripts/transformers/index.html#sec-trans-methodology",
    "title": "Transformer based named entity recognition for place name extraction from unstructured text",
    "section": "Methodology",
    "text": "Methodology\nFigure 1 gives an overview of the model and data processing pipeline used in our paper. This section first outlines the computational infrastructure used. The data collection and data processing is then described, obtaining a corpus of Wikipedia articles for locations in Great Britain with place names labelled.\nThis dataset was then used to train custom NER models of various architectures, which were evaluated using separate test data against each other and popular pre-built NER models. We then selected our DistilBERT transformer model to extract all place names from the full corpus of Wikipedia articles, as this model performed well as indicated by its test F1 score, despite its smaller size.\n\n\n\n\n\n\nFigure 1: Overview of the model processing pipeline\n\n\n\n\nSoftware & hardware infrastructure\nModels used in our paper were written in Python using the AllenNLP library for deep learning in natural language processing (Gardner et al. 2018). AllenNLP is built on top of PyTorch (Paszke et al. 2019), providing abstractions to commonly used operations for working with state-of-the-art deep neural networks in natural language processing.\nModel training was GPU accelerated using a single NVIDIA GeForce RTX 2070 SUPER with 8192MB memory paired with a Ryzen 3700x CPU with 8 physical and 16 logical cores. Python version 3.8.5 was used with AllenNLP version 1.5.0.\n\n\nAnnotation & data collection\n\nWikipedia data collection\nWikipedia presents a large collection of well-formatted text contributed by a variety of users, with frequent instances of place names, a consistent written style and without misspellings. Existing NER models are trained on either CoNLL-03 or OntoNotes 5, both of which are well-formatted text datasets, consisting primarily of news articles. As such, it was considered appropriate to select Wikipedia for a comparison between these models and ours, compared with other sources of VGI that are of lower overall quality.\nThe Wikipedia text data used in our paper was accessed through DBpedia (Auer et al. 2007), a community gathered database of information from Wikipedia, presented as an open knowledge graph, with ontologies that link and define information in articles. A query was built to obtain English Wikipedia abstracts for each DBpedia article with the Place class in Great Britain, using the DBpedia SPARQL endpoint. Querying just for Place articles within Great Britain ensured that articles extracted contained a large number of place names and language indicative of place names, without additional, unnecessary information.\nThese abstracts are the text provided at the top of each article, before any headings, sometimes called the summary. As an example, the Wikipedia abstract for Rowlatts Hill, a suburb of Leicester, UK is as follows, with hyperlinks indicated in bold:\n\nRowlatts Hill (also known as Rowlatts Hill Estate, or R.H.E.) is an eastern, residential suburb of the English city of Leicester. It contains mostly council-owned housing.\n\n\nThe suburb is roughly bordered by Spencefield Lane to the east and Whitehall Road to the south, which separates it from neighbouring Evington. A second boundary within the estate consists of Coleman Road to Ambassador Road through to Green Lane Road; Rowlatts Hill borders Crown Hills to the west. To the north, at the bottom of Rowlatts Hill is Humberstone Park which is located within Green Lane Road, Ambassador Road and also leads on to Uppingham Road (the A47), which is also Rowlatts Hill.\n\nUsing DBpedia enabled a fast executing query which, when combined with the Place class from the DBpedia ontology, returned a complete dataset of Wikipedia pages for many geographic locations in Great Britain. A total 42,222 article abstracts were extracted.\n\n\nInput format\nFor use in the models, a random subset of 200 articles were annotated using the CoNLL-03 NER format, which uses line delimitation to separate tokens, with entities associated with each token sharing the same line, separated by a space. Articles were first cleaned using regular expressions to remove quotation marks, text inside parentheses, and non-ascii characters. The SpaCy large web-based pre-trained model pipeline (en_core_web_lg) was used for further processing, using a non-monotonic arc-eager transition-system for sentence segmentation (Honnibal and Johnson 2015), and tokenisation using a rule-based algorithm. Each sentence-length sequence of tokens was treated as a separate instance to be fed as batches into models for training. Each token in every sequence was annotated as being a place name or not, assisted through the open source annotation tool Doccano (Nakayama et al. 2018).\nFor place names that span multiple tokens, the BIOUL tagging scheme was used, which stands for the ‘Beginning, Inside and Last tokens of multi-token chunks’; for place names that span more than one token (e.g. B-Place: New, L-Place: York). ‘Unit-length chunks and Outside’, place names of only a single token, and outside for any token that isn’t a place name. This scheme was used over the simpler BIO scheme which is more difficult for models to learn (Ratinov and Roth 2009). During annotation it became clear that the length of certain multi-token place names could be considered ambiguous. For example, it may not be clear when a cardinal direction is part of a place name, ‘northern Ireland’ may refer to a northern region in Ireland, while ‘Northern Ireland’ refers to the constituent country in the United Kingdom. To unify labelling decisions we chose to consider capitalisation as an indication of multi-token noun phrases that constituted a single place name. The following sentence shows a sequence of tokens with their corresponding tags, demonstrating the annotation scheme with BIOUL information prepending each tag:\nFrom these 200 labelled Wikipedia abstracts, 10% were kept for both validation and testing, leading to a training set of 21,080 labelled tokens, a validation dataset of 2,907 labelled tokens, and a testing dataset of 3,347 labelled tokens.\n\n\n\nBuilding the entity recognition models\nNamed entity recognition is a subset of token classification where a sequence of tokens \\(\\mathbf{x} = \\{x_{0}, x_{1}\\dots x_{n}\\}\\) are taken as input, and the most likely sequence tags \\(\\mathbf{y} = \\{y_0, y_1, \\dots y_n\\}\\) are predicted. The models constructed in our paper may be divided into three main components, outlined on Figure 1:\n\nEmbedding Layer: Each token in a sequence represented as high dimension numerical space, they may be either:\n\nRandomly initialised\nPre-trained: GloVe, transformer\n\nIntermediate Layers: A deep neural network that input embeddings propagate through, either:\n\nBidirectional LSTM\nTransformer\n\nClassification layer: The final layer of the model that takes a high dimensional output from the previous layers, and projects them to the classification dimension. The argmax from this layer corresponds to the label selected for each token. Each model uses a Conditional Random Field (CRF) to classify tokens which are popular in NER tasks, as they consider tagging decisions between all input tokens (Lample et al. 2016). This is necessary given the inside tag for a place (I-PLACE), cannot directly follow a unit tag (U-PLACE) for example.\n\n\n\n\nTable 1: Overview of the models trained through our paper, detailing the architecture used. Integers in brackets indicate the vector dimensions\n\n\n\n\n\n\nTable 1 gives an overview of the model architectures built through our paper. First a simplistic model was constructed as a baseline, using untrained randomly initialised 50 dimension token embeddings, fed into a two-layer Bidirectional LSTM (BiLSTM) with 200 hidden dimensions. The output from the BiLSTM was input into a conditional random field classifier. A second BiLSTM model was also created based on the architecture described in Peters et al. (2018), adding pre-trained GloVe token embeddings (Pennington, Socher, and Manning 2014) with 50 dimensions and 16 dimension character embeddings. Both models used the Adam optimiser which makes use of stochastic gradient descent for weight optimisation (Kingma and Ba 2017).\nThree BERT-based transformer models were also created, using BERT (Devlin et al. 2019), RoBERTa which attempts to optimise the training process of BERT (Liu et al. 2019), and DistilBERT, which distils the data used in pre-training to create a smaller, faster model (Sanh et al. 2020). The primary architecture of transformers is ‘attention’ which enables them to consider and weight each word in a sequence against each other word simultaneously. This allows them to be highly parallel, providing significant improvements to computational speed with GPUs which can handle highly parallel tasks, and benefits over traditional architectures like Long Short-Term Memory (LSTM) which are only able to consider sequences sequentially (Vaswani et al. 2017). These models were pre-trained on very large general text corpora, enabling ‘transfer learning’, where a pre-trained model like BERT is used as a base and fine-tuned to be task specific. Conceptually, these pre-trained models learn deep embedded weights for words based on comprehensive contextual information extracted from the large general text corpora, these then only require smaller adjustments in fine-tuning to achieve good task-specific results. Fine-tuning these pre-trained models in NLP has produced results that often outperform models using traditional architectures that include manually trained word embeddings (Word2Vec, Mikolov et al. 2013), which are limited by the volume of data provided to them and pre-trained embeddings like GloVe (Pennington, Socher, and Manning 2014).\nPre-trained transformer models replace both the BiLSTM layers of the previous models and token embeddings, taking encoded sequences, associating each token with a 768 dimension vector representation from a vocabulary, feeding them into sequential transformer layers and outputting into a CRF classifier. Each model was initialised with pre-trained weights provided by the transformers Python library (Wolf et al. 2020), these weights are initialised in both the embedding layers and intermediate layers. For weight optimisation, these models used the weight decay Adam algorithm (AdamW, Loshchilov and Hutter 2019). Every layer of the transformer models was updated during training, which enabled the pre-trained weights to adjust and learn for the specific task. Hyper-parameters selected for each model were largely based on the values as suggested for token classification by their respective implementation papers.\nFor every model, weights were adjusted each epoch to minimise the training loss. Following the final intermediate layer of a model, a token representation \\(C\\in\\mathbb{R}^H\\) feeds into the classification layer weights \\(W\\in\\mathbb{R}^{K\\times H}\\), where \\(K\\) is the number of unique labels. Classification loss is then calculated using \\(log(softmax(CW^T))\\).\nEarly stopping was used in each model, stopping training early if no improvement was made to the validation F1 score in eight subsequent epochs. Automatic Mixed Precision (AMP) was used throughout training to use half-precision (16 bit) floating point numbers in some operations which reduced the memory overhead and increased computation speed. For transformers, the learning rate was optimised towards the end of training, using a reduce on plateau learning rate scheduler, reducing the learning rate by 1/10th once the overall F1 validation metric had stopped improving after two epochs, this only increased training time on the BiLSTM models with no improvement, so was excluded. Following training, the weights from the best performing epoch were automatically chosen for the final model.\n\n\nEvaluation against pre-built models\nFollowing the training of each model, their accuracy, precision, recall and F1 score was evaluated using a corpus of test data, against three popular modern pre-built NER models provided through the SpaCy and Stanza Python packages. A SpaCy model is used in the Mordecai geoparser and optionally in the GeoTxt geoparser, while the Stanza model is a more recent implementation of the Stanford NLP model used by the GeoTxt geoparser.\nAs these pre-built models were not trained to recognise ‘place names’, their tags were adjusted so that anything labelled as ‘GPE’ (Geopolitical Entity), ‘LOC’ (Location), or ‘FAC’ (facility) was considered to be a ‘place name’, mirroring the process used to discard unrelated entities by geoparsing systems that use these models. The default Stanza NER model, and two SpaCy models (en_core_web_sm, en_core_web_lg) were evaluated on the labelled test data. Table 2 gives an overview of these pre-built models.\nEach model was evaluated on 3 separate subsets of the annotated test dataset, giving a range of scores for each model. Significance testing was then performed using paired t-tests to test the null hypothesis:\n\n\\(\\mathbf{H_0}\\): There will be no statistically significant difference between the mean F1 score of each custom built model against the best performing pre-built model (Stanza).\n\nSignificant results that reject this null hypothesis were indicated by \\(p&lt;0.05\\) and are shown on Table 3.\nThe best performing model trained on the annotated Wikipedia data was also evaluated using paired t-tests against each other model trained on the same data, to test the null hypothesis:\n\n\\(\\mathbf{H_0}\\): There will be no statistically significant difference between the mean F1 score of the best performing custom built model trained on annotated Wikipedia data and each other model trained on this data.\n\nSignificant results that reject this null hypothesis were also indicated by \\(p&lt;0.05\\).\nIt should be noted that significance testing is not common in deep learning research (Dror and Reichart 2018), but papers that do report the significance of mean scores between models tend to use paired t-tests, despite potentially violating the parametric assumptions made. Dror and Reichart (2018) suggest that while normality may be assumed due to the Central Limit Theorem, it is likely that future progress in this field will present more appropriate statistical significance testing.\n\n\n\nTable 2: Pre-built NER models\n\n\n\n\n\n\n\n\nOutput processing\nA predictor was created from the DistilBERT model to run inference over the total corpus of Wikipedia articles. Place names extracted from the Wikipedia articles by this model were saved to a CSV file with the context sentence, the associated article, and coordinate information for the article that contained the place.\nPlace names were compared against a full corpus of British place names from the GeoNames gazetteer, to examine which names are excluded from the gazetteer, but identified within Wikipedia articles."
  },
  {
    "objectID": "posts/manuscripts/transformers/index.html#sec-trans-results-discussion",
    "href": "posts/manuscripts/transformers/index.html#sec-trans-results-discussion",
    "title": "Transformer based named entity recognition for place name extraction from unstructured text",
    "section": "Results & discussion",
    "text": "Results & discussion\nThis section first evaluates the results of the models presented against each other, and in relation to existing pre-built NER solutions. The place names extracted by our best performing model are compared with pre-built models, showing how our method improves on those used in existing place name extraction methods. Following this, examples from the corpus of place names extracted from Wikipedia articles are noted, demonstrating use-cases for the method presented that wouldn’t be possible or as effective, through pre-built NER solutions.\n\nModel performance\nTable 3 shows three popular pre-built NER models, evaluated on the labelled Wikipedia test data, compared with the models produced through our paper. The BiLSTM-CRF (basic) model gives a baseline reference for a typical NER model with a simple architecture. Out of the pre-built models, Stanza performs the best, achieving precision and accuracy just below the trained baseline model, with an F1 score which isn’t significantly worse (paired t-test \\(p&gt;0.05\\)), both SpaCy models however show notably worse results compared with Stanza. The primary issue with the pre-built models is recall, which is far below any of the custom-built models, reflecting a high number of false negatives.\n\n\n\nTable 3: Geographic entity recognition mean (±SD) performance metrics over 3 runs of annotated Wikipedia test data subsets. Pre-built NER models are shown in italics. Bold values indicate statistically significant F1 scores of fine-tuned models in relation to Stanza (Paired t-tests \\(p&lt;0.05\\)).\n\n\n\n\n\n\nIt is worth noting that due to class imbalances, i.e. many more ‘other’ (O) entities relative to the small number of PLACE entities, accuracy should be considered a poor metric, and is only included for completeness. This class imbalance means that as only approximately 15% of tokens are labelled as entities, it is possible to achieve 85% accuracy and high precision by labelling all tokens as not entities. F1 score is often used to compensate for these issues in multiple classification tasks, but it should be known that it is not itself a perfect metric. With respect to the best performing pre-built model Stanza, all transformer models fine-tuned on the Wikipedia annotated data, have significantly higher F1 scores (paired t-test \\(p&lt;0.05\\)).\nThe DistilBERT transformer model is less complex than both the BERT and RoBERTa model, with a total of 260 MB in model weights, compared with 433 MB and 498 MB respectively. Despite this, the DistilBERT model achieves similar results to RoBERTa on test data (Table 3). While all transformer models perform significantly better than the best performing pre-built model, Stanza, both CRF models do not give significantly better F1 scores (paired t-test \\(p&gt;0.05\\)). BERT performs best overall, with an F1 score of 0.939 on the test data, a result that is only significantly better than the two CRF models (paired t-test \\(p&lt;0.05)\\).\nFigure 2 shows the output of the chosen fine-tuned NER model DistilBERT alongside SpaCy (large) and Stanza, applied to a simple Wikipedia article summary. Figure 2 (A) gives promising results for DistilBERT, with the summary for the Wikipedia page ‘Rowlatts Hill’, correctly identifying all place names.\nWhile evaluation metrics indicate that Stanza performs reasonably well, it primarily suffers from the annotation scheme used, some place names are misidentified as ‘Person’, or ‘Organisation’, meaning a standard geoparsing system would miss several place names here, given they are not otherwise identifiable (Figure 2).\n\n\n\n\n\n\nFigure 2: Comparison of outputs between the best performing fine-tuned transformer model and the two best performing pre-built NER models.\n\n\n\n\n\n\n\n\n\nFigure 3: Ability for trained model to distinguish between metonymic usage of place names.\n\n\n\nFigure 3 demonstrates the ability for our DistilBERT transformer model to accurately ignore entities that do not relate to place names. This example paragraph only refers to a single geographic location in text, the location of the 1952 Summer Games, in Helsinki, Finland. While Stanza identifies a large number of GPE tags, they either relate to China used in a metynomic sense, meaning the Chinese Olympic team (‘China competed’), or as a related geopolitical noun (‘delegation of ROC’), which is not considered to be a place name referring to a geographic location in this context. Our model correctly infers the single mention of a geographic place name based on the contextual information, meaning a large amount of unrelated information is excluded. Particularly, recognising and ignoring these nouns related to place names is something that is noted as an issue in current geoparsing systems (Gritta, Pilehvar, and Collier 2020). This figure also demonstrates the importance of using a pre-trained model base for this task, as the BiLSTM CRF performs poorly. It is likely that this issue stems from the limited training data used, as the model is unable to learn more complex cases where place names are less obvious (Figure 3 (B)). Using a pre-trained transformer enables the model to correctly identify instances where proper nouns do not relate to place names, taking information learned through its pre-training procedure.\n\n\nIdentified place names from Wikipedia\n\n\n\nTable 4: Top and bottom named places by frequency, excluding any present in the GeoNames gazetteer or mentioned less than 100 times.\n\n\n\n\n\n\nTable 4 gives an overview of the most common place names identified by the DistilBERT model and the SpaCy model. Notably, the SpaCy model appears to struggle with correctly aligning entities, including ‘the’ with ‘United Kingdom’, and partially missing place names containing ‘Tyne’ (e.g. ‘Tyne and Wear’ or ‘River Tyne’). The DistilBERT model also extracts around 6 times the number of place names compared with SpaCy, reflected by the low recall noted above. One example where the DistilBERT model appears confused is by giving the place name ‘Church of England’, this problem relates to the language used in Wikipedia articles, when churches are described as a ‘Church of England church’, a nominal mention of a place rather than specific.\nThe total number of place names extracted from the Wikipedia summaries by the DistilBERT model was 614,672, with 99,697 unique place names. In total 62,178 unique place names were extracted that are not found within the GeoNames gazetteer. These entities primarily exist as granular names mentioned in single instances (e.g. road names: Shady Lane, Chapeltown Road), organisational names used in a place related context (e.g. describing locations along the Great Western Railway route), and alternative names that are not captured by GeoNames. For example, ‘M1’ appears in GeoNames as ‘M1 Motorway’3. While the ‘M1 motorway’ is used in Wikipedia articles, it is often also referred to as just the ‘M1’."
  },
  {
    "objectID": "posts/manuscripts/transformers/index.html#conclusion",
    "href": "posts/manuscripts/transformers/index.html#conclusion",
    "title": "Transformer based named entity recognition for place name extraction from unstructured text",
    "section": "Conclusion",
    "text": "Conclusion\nOur paper demonstrates a new approach towards the extraction of place names from text by building an NER model using data annotated with geographic place names. This work aims to direct geographic NLP research towards the use of models which move away from the generalisable annotation schemes of pre-built NER solutions, to include task-specific, relevant training data. Notably this differs from the perceived generalisability of pre-built models used for general geoparsing. We believe this is an important approach for geographic place name extraction given geographic language differs greatly based on context (Purves et al. 2018), with contexts varying greatly based on the corpora used for inference. This is demonstrated by the poor results observed in previous work when applying pre-built NER solutions, which use training data unrelated to the task-specific data they are being applied to (Hu, Mao, and McKenzie 2019; Karimzadeh et al. 2019). Wallgrün et al. (2018) recognise this problem, developing GeoCorpora, a task-specific training dataset for micro-blog geoparsing, notably describing increased issues with annotation ambiguity compared with more traditional text-sources. Additionally, recent work with transformer models, typically only built to be generalisable, have considered moving from fully generalised self-supervised training towards more dataset-specific models (e.g. TweetEval; Barbieri et al. (2020)), with results that outperform generalisable transformer models (Nguyen, Vu, and Nguyen 2020).\nUltimately, the decision to produce a model explicitly designed to be non-generalisable to other corpora may be considered a limitation of the scope of this paper. We have demonstrated a best-case scenario where time-frames allow for manual annotation of task-specific data. Future research may consider the construction of a more generalisable place name extraction model, which takes inspiration from the alternative annotation scheme employed by our paper, allowing for use in general purpose geoparsers.\nAdditionally, while our paper selects Wikipedia for place name extraction, due to its large volume, ease of validation and data retrieval, future work may consider the ability to apply our methodology to other text sources. With suitable models constructed, using annotated training data that is relevant to the corpus being considered, we expect future work applied to other data sources may present the opportunity to further contribute to place names that are absent from gazetteers, as vernacular place names. We believe that given a suitable combination of data sources, our methodology is the first step towards the construction gazetteers from the bottom-up, directly taking place names from passive contributions, without relying on pre-built datasets.\nThe recent development of pre-trained language models and their suitability for fine-tuning in many tasks, including NER, presents a method for the construction of accurate models that are task specific, using relatively small labelled corpora4 that defines entities more suited to the task of place name extraction. The architecture in our paper is more simplistic to implement than other attempts at similar tasks (e.g. Weissenbacher et al. 2019), with most of the complexity hidden within the transformer layers. This, combined with libraries that abstract and implement state of the art models, provides a more accessible approach for research in place name extraction, without requiring a deep understanding of semantic rules, or the construction of deep multi-layered models from the ground up.\nEvaluation against pre-built NER models on Table 3 shows that performance for place name extraction is greatly improved, particularly with respect to recall, a notable issue with past studies (Hu, Mao, and McKenzie 2019; Karimzadeh et al. 2019). The construction of an NER model for the task specific extraction of place names moves towards systems that appropriately consider the geographic elements present in natural language. The large number of place names that are absent from the GeoNames gazetteer suggests that geoparsing and related work likely misses a substantial amount of geographic information present in text. The dataset produced through this work aims to assist with filling these gaps, while the methodology described enables an approach that may be mirrored and applied to further work on other data sources.\nFinally, both ‘place’ focussed annotation schemes describe the use of ‘nominal’ place related entities (Mani et al. 2010; Pustejovsky 2017). While out of the scope of our work, we would like to encourage the focus on extracting this additional geographic information from text. Often in language the use of these non-specific terms are used, for example ‘I visited the shops’, ‘York is a city’, provide geographically specific information. ‘The shops’ with enough context may provide a specific geographic location, and similarly the link between ‘York’ -&gt; ‘city’ could be explored (Couclelis 2010)."
  },
  {
    "objectID": "posts/manuscripts/transformers/index.html#data-and-codes-availability-statement",
    "href": "posts/manuscripts/transformers/index.html#data-and-codes-availability-statement",
    "title": "Transformer based named entity recognition for place name extraction from unstructured text",
    "section": "Data and codes availability statement",
    "text": "Data and codes availability statement\nThe data and codes that support the findings of this study are available at the public FigShare link (https://doi.org/10.6084/m9.figshare.13415255.v1). Instructions for using the data and code are provided as a README within the FigShare repository."
  },
  {
    "objectID": "posts/manuscripts/transformers/index.html#disclosure-statement",
    "href": "posts/manuscripts/transformers/index.html#disclosure-statement",
    "title": "Transformer based named entity recognition for place name extraction from unstructured text",
    "section": "Disclosure statement",
    "text": "Disclosure statement\nNo potential competing interest was reported by the authors."
  },
  {
    "objectID": "posts/manuscripts/transformers/index.html#footnotes",
    "href": "posts/manuscripts/transformers/index.html#footnotes",
    "title": "Transformer based named entity recognition for place name extraction from unstructured text",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCoNLL03: https://www.clips.uantwerpen.be/conll2003/ner/, OntoNotes 5: https://catalog.ldc.upenn.edu/LDC2013T19↩︎\nhttps://github.com/openeventdata/mordecai↩︎\nhttps://www.geonames.org/8714914/m1-motorway.html↩︎\nCompared with the Reuters corpus used for CoNLL03 for example↩︎"
  },
  {
    "objectID": "posts/projects/ahah/index.html",
    "href": "posts/projects/ahah/index.html",
    "title": "Access to Healthy Assets & Hazards (AHAH)",
    "section": "",
    "text": "This project identifies the time-weighted distance required to travel by road between every postcode in Great Britain and a selection of health-related points of interest. A ranked combination of these drive-times is used to create the AHAH index. Using the GPU-accelerated cugraph Python library, this implementation reduces processing time per variable down to ~10 minutes from 8 hours.\nCillian Berragan [@cjberragan]1* Mark Green [@markalangreen]1 Alex Singleton [@alexsingleton]1\n1 Geographic Data Science Lab, University of Liverpool, Liverpool, United Kingdom\n* Correspondence: C.Berragan@liverpool.ac.uk"
  },
  {
    "objectID": "posts/projects/ahah/index.html#overview",
    "href": "posts/projects/ahah/index.html#overview",
    "title": "Access to Healthy Assets & Hazards (AHAH)",
    "section": "Overview",
    "text": "Overview\nThis project identifies the time-weighted distance required to travel by road between every postcode in Great Britain and a selection of health related points of interest. A ranked combination of these drive-times is used to create the AHAH index.\nAccess is defined through the average time-weighted road network distance for each postcode within each LSOA to the nearest point of interest of a particular type. For this, the road highways network and road speed estimates provided through Ordnance Survey was used, alongside the OWNS Postcode Directory for May 2020, which gives centroids for every postcode in the country.\nThis is a computationally intense calculation, with the total road network used having 3,816,897 edges, and 3,215,522 nodes. Access to each nearest health related POI was calculated using the Single Source Shortest Path algorithm, for all 1,659,451 postcodes in Great Britain.\nThis calculation was made possible through the GPU accelerated Python library cugraph, part of the NVIDIA RAPIDS ecosystem, allowing the computation to be highly parallel, taking minutes, rather than days."
  },
  {
    "objectID": "posts/projects/ahah/index.html#project-layout",
    "href": "posts/projects/ahah/index.html#project-layout",
    "title": "Access to Healthy Assets & Hazards (AHAH)",
    "section": "Project layout",
    "text": "Project layout\nahah\n├── aggregate_lsoa.py  # aggregate outputs to LSOA level\n├── create_index.py  # use aggregates to create index\n├── get_nhs.py  # retrieve NHS data\n├── os_highways.py  # process OS open roads data\n├── process_air.py  # process air quality data\n├── process_routing.py  # process all POI data\n├── routing.py  # main routing class\n└── common\n    ├── logger.py  # use rich logging\n    └── utils.py  # utility functions"
  },
  {
    "objectID": "posts/projects/ahah/index.html#methodology",
    "href": "posts/projects/ahah/index.html#methodology",
    "title": "Access to Healthy Assets & Hazards (AHAH)",
    "section": "Methodology",
    "text": "Methodology\nAccessibility measures were created using the cugraph GPU accelerated Python library for parallel processing of graph networks, in conjunction with the OS Open Road network. Unlike Routino, which uses Open Street Map data, the OS Open Road Network provides more accurate road speed estimates for UK roads.\nIn this study, we measured the network distance (travel time) between the centroid of each active postcode in Great Britain to the coordinates of each unique health asset (e.g. GP practice). Measured network distances for each indicator for postcodes were aggregated to the LSOA level, providing average network distance for each indicator (as a measure of accessibility). All other indicators were also summarised for LSOAs. The indicators within each domain were standardised by ranking and transformed to the standard normal distribution. The direction of each variable was dictated by the literature (e.g. accessibility to fast food outlets were identified as health negating, wheras accessibility to GP practices was health promoting).\nTo calculate our overall index (and domain specific values), we followed the methodology of the 2015 IMD. For each domain, we ranked each domain \\(R\\) and any LSOA scaled to the range \\([0,1]\\). \\(R=1/N\\) for the most ‘health promoting’ LSOA and \\(R=N/N\\) for the least promoting, where \\(N\\) is the number of LSOAs in Great Britain. Exponential transformation of the ranked domainscores was then applied to LSOA values to reduce ‘cancellation effects’. So, for example, high levels of accessibility in one domainare not completely cancelled out by low levels of accessibility in a different domain. The exponential transformation applied also puts more emphasis on the LSOAs at theend of the health demoting side of the distribution and so facilitates identification of the neighbourhoods with the worsthealth promoting aspects. The exponential transformed indicator score \\(X\\) is given by:\n\\[\nX=−23ln(1−R(1−exp^{−100/23}))\n\\]\nwhere ‘ln’ denotes natural logarithm and ‘exp’ the exponential transformation.\nThe main domains across our indicators: retail services, health services, physical environment and air quality then were combined to form an overall index of‘Access to Healthy Assets and Hazards’ (AHAH)"
  },
  {
    "objectID": "posts/projects/ahah/index.html#scripts",
    "href": "posts/projects/ahah/index.html#scripts",
    "title": "Access to Healthy Assets & Hazards (AHAH)",
    "section": "Scripts",
    "text": "Scripts\n\n1. Process road network ahah/os_highways.py\n\nSpeed estimates given to each road, based on formOfway and roadClassification\nTime-weighted distance calculated using length of edge and speed estimate\nNode ID converted to sequential integers and saved with edges as parquet files\n\n\n\n2. Process Data ahah/process_routing.py\n\nThis stage prepares the nodes, postcodes, and poi data for use in RAPIDS cugraph. Makes use of utility functions to assist with data preparation from the raw data sources.\n\n\nClean raw data\nFind the nearest road node to each postcode and point of interest using GPU accelerated K Means Clustering\nDetermine minimum buffer distance to use for each point of interest\n\nDistances returned for nearest 10 points of interest to each postcode using K Means\nFor each unique POI the maximum distance to associated postcodes is taken and saved as a buffer for this POI\nEach POI is assigned the postcodes that fall within their KNN, used to determine buffer suitability when converted to a graph\n\nAll processed data written to respective files\n\n\n\n3. Routing ahah/routing.py\n\nThe routing stage of this project primarily makes use of the RAPIDS cugraph library. This stage iterates sequentially over each POI of a certain type and finds routes to every postcode within a certain buffer.\n\n\nIterate over POI of a certain type\nCreate cuspatial.Graph() with subset of road nodes using cuspatial.points_in_spatial_window with buffer\nRun single-source shortest path from POI to each node in the sub graph\n\ncugraph.sssp takes into account weights, which in this case are the time-weighted distance of each connection between nodes as reported by OSM.\n\nSSSP distances subset to return only nodes associated with postcodes, these distances are added iteratively to a complete dataframe of postcodes of which the smallest value for each postcode is taken\n\n\n\n4. Process air quality data ahah/process_air.py\n\nCreate raster of interpolated values from monitoring station points\n\nExclude points that are MISSING\n\nAggregate to LSOA by taking mean values\n\n\n\n5. Combine into index ahah/create_index.py\n\nCombine both processed secure and open data\nIntermediate variables calculated\n\nAll variables ranked\nExponential default calculated for all ranked variables\nPercentiles calculated from ranked variables\n\nDomains Scores calculated\n\nDomain scores calculated from mean of each domains input variables\nDomain scores ranked\nDomain percentiles calculated\nExponential transformation calculated for each domain\n\nAHAH index calculated from mean of domain exponential transformations\n\nRanked AHAH index calculated\nAHAH percentiles calculated"
  },
  {
    "objectID": "posts/projects/ahah/index.html#ahah-data-sources",
    "href": "posts/projects/ahah/index.html#ahah-data-sources",
    "title": "Access to Healthy Assets & Hazards (AHAH)",
    "section": "AHAH Data Sources",
    "text": "AHAH Data Sources\n\nSee the AHAH V2 FigShare Repository for the previous iteration.\n\n\nOS Open Roads\nFerry Routes\nAir quality\nGreenspace (NDVI Classification)\nNHS England\nNHS Scotland\nPostcodes\nLSOA Polygons"
  }
]