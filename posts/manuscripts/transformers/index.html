<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2022-10-02">
<meta name="keywords" content="named entity recognition, volunteered geographic information, natural language processing, place name extraction">

<title>cjber.github.io - Transformer based named entity recognition for place name extraction from unstructured text</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/quarto-contrib/academicons-1.9.2/all.css" rel="stylesheet">
<link href="../../../site_libs/quarto-contrib/academicons-1.9.2/size.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">cjber.github.io</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#sec-trans-review" id="toc-sec-trans-review" class="nav-link" data-scroll-target="#sec-trans-review">Literature review</a>
  <ul class="collapse">
  <li><a href="#named-entity-recognition-in-the-geographic-domain" id="toc-named-entity-recognition-in-the-geographic-domain" class="nav-link" data-scroll-target="#named-entity-recognition-in-the-geographic-domain">Named entity recognition in the geographic domain</a></li>
  </ul></li>
  <li><a href="#sec-trans-methodology" id="toc-sec-trans-methodology" class="nav-link" data-scroll-target="#sec-trans-methodology">Methodology</a>
  <ul class="collapse">
  <li><a href="#software-hardware-infrastructure" id="toc-software-hardware-infrastructure" class="nav-link" data-scroll-target="#software-hardware-infrastructure">Software &amp; hardware infrastructure</a></li>
  <li><a href="#annotation-data-collection" id="toc-annotation-data-collection" class="nav-link" data-scroll-target="#annotation-data-collection">Annotation &amp; data collection</a></li>
  <li><a href="#building-the-entity-recognition-models" id="toc-building-the-entity-recognition-models" class="nav-link" data-scroll-target="#building-the-entity-recognition-models">Building the entity recognition models</a></li>
  <li><a href="#evaluation-against-pre-built-models" id="toc-evaluation-against-pre-built-models" class="nav-link" data-scroll-target="#evaluation-against-pre-built-models">Evaluation against pre-built models</a></li>
  <li><a href="#output-processing" id="toc-output-processing" class="nav-link" data-scroll-target="#output-processing">Output processing</a></li>
  </ul></li>
  <li><a href="#sec-trans-results-discussion" id="toc-sec-trans-results-discussion" class="nav-link" data-scroll-target="#sec-trans-results-discussion">Results &amp; discussion</a>
  <ul class="collapse">
  <li><a href="#model-performance" id="toc-model-performance" class="nav-link" data-scroll-target="#model-performance">Model performance</a></li>
  <li><a href="#identified-place-names-from-wikipedia" id="toc-identified-place-names-from-wikipedia" class="nav-link" data-scroll-target="#identified-place-names-from-wikipedia">Identified place names from Wikipedia</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  <li><a href="#data-and-codes-availability-statement" id="toc-data-and-codes-availability-statement" class="nav-link" data-scroll-target="#data-and-codes-availability-statement">Data and codes availability statement</a></li>
  <li><a href="#disclosure-statement" id="toc-disclosure-statement" class="nav-link" data-scroll-target="#disclosure-statement">Disclosure statement</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="index.docx"><i class="bi bi-file-word"></i>MS Word</a></li><li><a href="index.pdf"><i class="bi bi-file-pdf"></i>PDF (arxiv)</a></li></ul></div><div class="quarto-other-links"><h2>Other Links</h2><ul><li><a href="https://github.com/cjber/ger-wiki" rel="icon"><i class="bi bi-github"></i>Github Repository</a></li><li><a href="https://doi.org/10.5454/JPSv1i220161014" rel="icon"><i class="bi bi-journal-richtext"></i>DOI 10.1080/13658816.2022.2133125</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Transformer based named entity recognition for place name extraction from unstructured text</h1>
<p class="subtitle lead"><i class="ai  ai-open-access"></i> <i class="ai  ai-figshare"></i> <i class="ai  ai-doi"></i></p>
  <div class="quarto-categories">
    <div class="quarto-category">paper</div>
    <div class="quarto-category">code</div>
  </div>
  </div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Authors</div>
  <div class="quarto-title-meta-heading">Affiliations</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Cillian Berragan <a href="mailto:c.berragan@liverpool.ac.uk" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            University of Liverpool
          </p>
      </div>
    <div class="quarto-title-meta-contents">
    <p class="author">Alex Singleton <a href="mailto:alex.singleton@liverpool.ac.uk" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            University of Liverpool
          </p>
      </div>
    <div class="quarto-title-meta-contents">
    <p class="author">Alessia Calafiore <a href="mailto:acalafio@ed.ac.uk" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            University of Edinburgh
          </p>
      </div>
    <div class="quarto-title-meta-contents">
    <p class="author">Jeremy Morley <a href="mailto:Jeremy.Morley@os.uk" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Ordnance Survey
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">October 2, 2022</p>
    </div>
  </div>
  
    
  </div>
  
<div>
  <div class="abstract">
    <div class="block-title">Abstract</div>
    <p>Place names embedded in online natural language text present a useful source of geographic information. Despite this, many methods for the extraction of place names from text use pre-trained models that were not explicitly designed for this task. Our paper builds five custom-built Named Entity Recognition (NER) models and evaluates them against three popular pre-built models for place name extraction. The models are evaluated using a set of manually annotated Wikipedia articles with reference to the F1 score metric. Our best performing model achieves an F1 score of 0.939 compared with 0.730 for the best performing pre-built model. Our model is then used to extract all place names from Wikipedia articles in Great Britain, demonstrating the ability to more accurately capture unknown place names from volunteered sources of online geographic information.</p>
  </div>
</div>

<div>
  <div class="keywords">
    <div class="block-title">Keywords</div>
    <p>named entity recognition, volunteered geographic information, natural language processing, place name extraction</p>
  </div>
</div>

</header>


<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Place names are frequently encountered in natural language and provide an additional geographic dimension to much of the textual information present online, when associated with spatial coordinates and geographic locations. Despite this, research in place name extraction primarily concentrates on entities as described by annotation schemes that do not explicitly consider geographic place names <span class="citation" data-cites="karimzadeh2019 halterman2017 hu2019">(<a href="#ref-karimzadeh2019" role="doc-biblioref">Karimzadeh et al. 2019</a>; <a href="#ref-halterman2017" role="doc-biblioref">Halterman 2017</a>; <a href="#ref-hu2019" role="doc-biblioref">Hu, Mao, and McKenzie 2019</a>)</span>. Pre-built named entity recognition (NER) models based on these schemes are also not task specific; trained on data unrelated to the task they are used for, despite language involving place names varying significantly depending on the context <span class="citation" data-cites="purves2018">(<a href="#ref-purves2018" role="doc-biblioref">Purves et al. 2018</a>)</span>. When identifying place names in text, research typically only considers known administrative names and their associated strict boundaries, despite natural language often containing place names that either do not exist formally, are hyper-localised e.g.&nbsp;street names, or are alternative names that may be absent from administrative databases, which often only consider a single formal name.</p>
<p>The training corpora used by pre-built NER models typically identifies a number of entities that have no relevance to geographic place names, e.g.&nbsp;persons, and those that have some relevance in specific contexts; locations, geopolitical entities or facilities <span class="citation" data-cites="weischedelralph2013 tjongkimsang2003">(<a href="#ref-weischedelralph2013" role="doc-biblioref">Weischedel et al. 2013</a>; <a href="#ref-tjongkimsang2003" role="doc-biblioref">Tjong Kim Sang and De Meulder 2003</a>)</span>. Notably, they do not specifically target a ‘place name’ entity, meaning, while often these three related entity types may often refer to a place name, this is not always the case. Additionally, these corpora consist of text that often differs in structure, compared with the text being processed by models trained using them; for example, social media text is typically more informal compared with the news articles used to build the popular dataset, CoNLL03 <span class="citation" data-cites="tjongkimsang2003">(<a href="#ref-tjongkimsang2003" role="doc-biblioref">Tjong Kim Sang and De Meulder 2003</a>)</span>.</p>
<p>New forms of geographic information online present an opportunity to train and evaluate models on texts that contain a large volume of place names <span class="citation" data-cites="goodchild2011">(<a href="#ref-goodchild2011" role="doc-biblioref">Goodchild and Li 2011</a>)</span>, building models from the ground up, and using annotation schemes that are explicitly designed for the extraction of place names from text. Results from these models are expected to outperform existing pre-built models which use unrelated training data, and do not include a ‘place name’ entity type.</p>
<p>Our paper presents five NER models, trained on manually labelled Wikipedia data and used to identify and extract any span of text considered to be a place name, from articles relating to geographic locations in the United Kingdom. Our model is evaluated against pre-built solutions that are commonly used for this task, demonstrating the importance of model training with task specific data, and the consideration that named entity recognition as a task is not appropriate for place name extraction, due to the exclusion of a ‘place name’ entity type, and the inclusion of a number of unrelated entities. New developments in natural language processing (NLP) are utilised, outlining the benefit of selecting modern architectures that are not yet implemented by off the shelf models. Our paper considers the ability to extract place names from Wikipedia articles for the United Kingdom that do not appear in the GeoNames Gazetteer, with the goal of identifying the additional geographic information that may be effectively extracted from unstructured sources of online text.</p>
<p><a href="#sec-trans-review" class="quarto-xref">Section&nbsp;2</a> outlines the research and concepts associated with geography in NLP, considering its relation to the new forms of geographic data present online, the techniques in natural language processing that explicitly deal with geography, and the developments in NLP that have enabled higher accuracy with limited labelled data. <a href="#sec-trans-methodology" class="quarto-xref">Section&nbsp;3</a> presents the workflow undertaken for the models constructed in this paper, as well as the data collection and analysis of the entities extracted. The performance of each NER model is then presented in <a href="#sec-trans-results-discussion" class="quarto-xref">Section&nbsp;4</a> and evaluated against pre-built solutions using a corpus of labelled test data. Place names are extracted using the model for the entire Wikipedia corpus, and compared against GeoNames, identifying names that are not present, discussing the reasons they may be found within Wikipedia articles, but not in an explicitly geographic gazetteer.</p>
</section>
<section id="sec-trans-review" class="level2">
<h2 class="anchored" data-anchor-id="sec-trans-review">Literature review</h2>
<p>Natural language often describes places using imprecise referents, non-administrative names, and an understanding of place footprints that does not conform with the formal administrative boundaries given to them <span class="citation" data-cites="gao2017 goodchild2011">(<a href="#ref-gao2017" role="doc-biblioref">Gao, Janowicz, and Couclelis 2017</a>; <a href="#ref-goodchild2011" role="doc-biblioref">Goodchild and Li 2011</a>)</span>. Despite this, regions and place names in computational geography are usually formally defined by administrative datasets, meaning any informal place names are unable to be identified, or associated with a position in space. This distinction has given rise to a focus on <em>place</em> based GIS, rather than <em>space</em> based, which considers the ability to capture place references that may not appear in administrative datasets <span class="citation" data-cites="gao2013">(<a href="#ref-gao2013" role="doc-biblioref">Gao et al. 2013</a>)</span>.</p>
<p>Since the advent of Web 2.0, increased access to mobile devices which include passive GPS and open-access mapping information, several scientific disciplines have developed to take advantage of the data being produced, including crowdsourcing, and user-generated content <span class="citation" data-cites="see2016">(<a href="#ref-see2016" role="doc-biblioref">See et al. 2016</a>)</span>. With geographically referenced content through social media, mapping platforms and Wikipedia there is now a wealth of information that <span class="citation" data-cites="goodchild2007">Goodchild (<a href="#ref-goodchild2007" role="doc-biblioref">2007</a>)</span> terms <em>‘Volunteered Geographic Information’</em> (VGI). These data sources present a large collection of continually updated references to places, often providing informal and unstructured geographic information.</p>
<p>Much of the past work using VGI has concentrated either on explicitly geographic crowd-sourced mapping platforms like Open Street Map <span class="citation" data-cites="antoniou2010">(<a href="#ref-antoniou2010" role="doc-biblioref">Antoniou, Morley, and Haklay 2010</a>)</span>, or ‘geotagged’ content which enables, often passively contributed, user-generated data through sites like Twitter or Flickr, used to extract geographic information. <span class="citation" data-cites="gao2017">Gao, Janowicz, and Couclelis (<a href="#ref-gao2017" role="doc-biblioref">2017</a>)</span> for example present an approach for the construction of cognitive regions from various VGI sources, querying place names found in tags with associated geotags to create vague boundaries. A similar approach is taken by <span class="citation" data-cites="hollenstein2010">Hollenstein and Purves (<a href="#ref-hollenstein2010" role="doc-biblioref">2010</a>)</span> who identified tags containing vague spatial concepts like ‘downtown’ and ‘citycentre’, deriving regions from geotags. These methods demonstrate the ability to derive informal geographic information from VGI, while giving similar results to that of manually collected questionnaire data <span class="citation" data-cites="twaroch2019 gao2017">(<a href="#ref-twaroch2019" role="doc-biblioref">Twaroch et al. 2019</a>; <a href="#ref-gao2017" role="doc-biblioref">Gao, Janowicz, and Couclelis 2017</a>)</span>.</p>
<p>While this work concentrates solely on the use of geotags and short single phrase tags associated with social media documents to analyse ‘place’ focussed geographies, another source of online information that is less frequently considered to have geographic properties is unstructured text, which has the potential to provide an even larger source of geographically focussed information. Good results have been reported using basic semantic rules to identify places names found in unstructured text <span class="citation" data-cites="moncla2014">(<a href="#ref-moncla2014" role="doc-biblioref">Moncla et al. 2014</a>)</span>, however, these methods have relied on this text almost solely containing place names as entities. Alternatively to rule-based approaches, <span class="citation" data-cites="hu2019">Hu, Mao, and McKenzie (<a href="#ref-hu2019" role="doc-biblioref">2019</a>)</span> demonstrate the use of four pre-trained NER models to extract local, informal place names from housing advertisements descriptions with associated coordinates, to enrich existing gazetteers with place names not normally present, alongside derived boundaries. The results of this paper show the promising ability for NER models to extract informal place names directly from text, also demonstrating a bottom-up approach to gazetteer construction, enabling informal place definitions to be captured from VGI, that may be absent from administrative datasets. Model evaluation however showed low precision and recall when evaluating against a labelled dataset, reflecting issues with the use of pre-built NER models for this task. Similar evaluation results are observed by <span class="citation" data-cites="karimzadeh2019">Karimzadeh et al. (<a href="#ref-karimzadeh2019" role="doc-biblioref">2019</a>)</span> when considering various pre-built NER models for use in the GeoTxt geoparsing system, which uses either SpaCy or Stanza pre-built models <span class="citation" data-cites="qi2018 honnibal2017">(<a href="#ref-qi2018" role="doc-biblioref">Qi et al. 2018</a>; <a href="#ref-honnibal2017" role="doc-biblioref">Honnibal and Montani 2017</a>)</span>. While the precision of these pre-built NER models can be relatively high for more sophisticated models, they all suffer from low recall. <span class="citation" data-cites="karimzadeh2019">Karimzadeh et al. (<a href="#ref-karimzadeh2019" role="doc-biblioref">2019</a>)</span> note particularly that while improved results would be expected by training a model from the ground up, the amount of labelled training data required to create a suitable model would be very large. To improve the accuracy of systems that rely on place name extraction, NER models should be constructed with more suitable training data, and with annotations tailored for this specific task.</p>
<p>While large, open-access, text-based sources of semantic geographic information are scarce, Wikipedia provides a large collection of articles about almost any subject, many of which relate to geographic locations. This presents an alternative data source for use in geographically focussed NLP applications, with place names, their semantic context, and article geotags providing geographic information. Various studies have used Wikipedia as a data source for the extraction of place names, <span class="citation" data-cites="delozier2015">DeLozier, Baldridge, and London (<a href="#ref-delozier2015" role="doc-biblioref">2015</a>)</span> for example, identify place names in Wikipedia articles and use a clustering technique using document contexts to disambiguate their geographic locations. <span class="citation" data-cites="speriosu2013">Speriosu and Baldridge (<a href="#ref-speriosu2013" role="doc-biblioref">2013</a>)</span> use geotagged Wikipedia articles to provide contextual information regarding a range of place names for disambiguation. Both these works first use a pre-built Named Entity Recognition (NER) model to identify place names found in text, before further analysis. Improvements made to these NER models for place name extraction present a stronger foundation, leading to both better recall, and precision of place names being identified, before they are resolved to coordinates <span class="citation" data-cites="leidner2008 purves2018">(<a href="#ref-leidner2008" role="doc-biblioref">Leidner 2008</a>; <a href="#ref-purves2018" role="doc-biblioref">Purves et al. 2018</a>)</span>. Our paper selects Wikipedia articles to demonstrate the geographic information that may be extracted from unstructured text, presenting a first-stage baseline approach for tasks that rely on accurate place name extraction.</p>
<section id="named-entity-recognition-in-the-geographic-domain" class="level3">
<h3 class="anchored" data-anchor-id="named-entity-recognition-in-the-geographic-domain">Named entity recognition in the geographic domain</h3>
<p>Natural language processing techniques involving geography typically focus around geoparsing; the automated extraction of place names from text, followed by the resolution of the identified place names to geographic coordinates <span class="citation" data-cites="gritta2020 leidner2008 buscaldi2011">(<a href="#ref-gritta2020" role="doc-biblioref">Gritta, Pilehvar, and Collier 2020</a>; <a href="#ref-leidner2008" role="doc-biblioref">Leidner 2008</a>; <a href="#ref-buscaldi2011" role="doc-biblioref">Buscaldi 2011</a>)</span>. Modern place name extraction techniques primarily rely on named entity recognition (NER) to identify place names as entities within text <span class="citation" data-cites="kumar2019 purves2018">(<a href="#ref-kumar2019" role="doc-biblioref">Kumar and Singh 2019</a>; <a href="#ref-purves2018" role="doc-biblioref">Purves et al. 2018</a>)</span>. While most pre-built NER systems are able to identify ‘geopolitical entities’ and ‘locations’ as defined by popular annotation schemes<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, these only act as a proxy for place names in text. The majority of entities recognised by these systems are unrelated to place names, and as such simply contribute to lower overall recall when other entities are preferred by models over geographic place names. For example, a model may consider a named organisational headquarters as an ‘organisation’ entity, rather than a ‘location’, even when used as a locational reference.</p>
<p>The concept of a place name as an entity defined by the labelled corpora NER models were trained on hinders place name extraction, identifying only (and any) administrative place names in text <span class="citation" data-cites="gritta2017a">(<a href="#ref-gritta2017a" role="doc-biblioref">Gritta et al. 2017</a>)</span>. The geoparser <em>Mordecai</em><a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> for example, uses an NER tagger provided through the <code>SpaCy</code> Python library, which provides a variety of entities including those unrelated to place names (e.g.&nbsp;: persons), and three entities that may be considered related, (Geopolitical Entity), (Location), and (Facility). While these categories often do relate to place names, they do not consider whether the entity could be contextually considered a place name that could be geo-located. For example, geopolitical entities are often used in a metonymic sense; a figure of speech where a concept is substituted by a related concept. In the phrase ‘Madrid plays Kiev today’ for example, sports teams are replaced by their associated place name <span class="citation" data-cites="gritta2020">(<a href="#ref-gritta2020" role="doc-biblioref">Gritta, Pilehvar, and Collier 2020</a>)</span>. As place name based metonyms do not explicitly relate to geographic locations, and instead a related entity, we are uninterested in their extraction. Due to the reliance on large labelled corpora for NER training, and limited source of geography specific data <span class="citation" data-cites="karimzadeh2019">(<a href="#ref-karimzadeh2019" role="doc-biblioref">Karimzadeh et al. 2019</a>)</span>, little work has considered explicitly targeting place names through new data, as it is often time-consuming to produce.</p>
<p>While at present pre-built NER models identify entities as defined by widely used annotated corpora, some work has considered the need to identify <em>spatial</em> entities. SpatialML is a natural language annotation scheme that presents the <strong>PLACE</strong> tag for any mention of a location <span class="citation" data-cites="mani2010">(<a href="#ref-mani2010" role="doc-biblioref">Mani et al. 2010</a>)</span>. Tasks identified by the <a href="https://semeval.github.io/">Semantic Evaluation Workshop</a> built on this annotation scheme and defined several entities relating to spatial language <span class="citation" data-cites="pustejovsky2015">(SemEval-2015 Task 8: SpaceEval, <a href="#ref-pustejovsky2015" role="doc-biblioref">Pustejovsky et al. 2015</a>)</span>, described by the ISO-Space annotation specification <span class="citation" data-cites="pustejovsky2017">(<a href="#ref-pustejovsky2017" role="doc-biblioref">Pustejovsky 2017</a>)</span>. In order to more appropriately consider geography when parsing unstructured text for place related entities, models should be built from the ground up, taking into account an alternative annotation scheme that identifies place names, excluding unrelated entities.</p>
<p>Recent progress in NLP and the use of GPU accelerated training has brought with it the ability to process large quantities of unlabelled text. This development has recently led to the creation of general purpose ‘language models’ that implement the ‘transformer’ architecture, using semi-supervised learning to train using very large corpora <span class="citation" data-cites="vaswani2017">(<a href="#ref-vaswani2017" role="doc-biblioref">Vaswani et al. 2017</a>)</span>. For example, Google’s pioneering BERT model was trained using the entirety of English Wikipedia, and over 11,000 books <span class="citation" data-cites="devlin2019">(<a href="#ref-devlin2019" role="doc-biblioref">Devlin et al. 2019</a>)</span>. This development has led to models which perform well for many given tasks, even with relatively limited additional labelled training data.</p>
<p>Our paper proposes fine-tuning transformer-based language models for place name extraction using named entity recognition, to extract all place names from UK ‘place’ classed articles on Wikipedia. 200 of these articles are annotated, labelling place names to train and evaluate model performance. We train and compare the performance of three popular transformer-based NER models; BERT - a large, popular transformer model, RoBERTa - similar to BERT, using a different pre-training procedure, which has had better results on some tasks, and DistilBERT - a much smaller and less complex transformer model based on RoBERTa. In addition to these transformer models, two simpler Bidirectional LSTM (BiLSTM) models are compared, one using pre-trained GloVe embeddings, representing an equivalent complexity model used by Stanza or SpaCy pre-built NER solutions, and another showing a baseline model without any pre-trained word embeddings. These models are then evaluated against three pre-built NER systems that are popular for place name extraction, and used in existing geoparsing systems including GeoTxt and Mordecai.</p>
</section>
</section>
<section id="sec-trans-methodology" class="level2">
<h2 class="anchored" data-anchor-id="sec-trans-methodology">Methodology</h2>
<p><a href="#fig-workflow" class="quarto-xref">Figure&nbsp;1</a> gives an overview of the model and data processing pipeline used in our paper. This section first outlines the computational infrastructure used. The data collection and data processing is then described, obtaining a corpus of Wikipedia articles for locations in Great Britain with place names labelled.</p>
<p>This dataset was then used to train custom NER models of various architectures, which were evaluated using separate test data against each other and popular pre-built NER models. We then selected our DistilBERT transformer model to extract all place names from the full corpus of Wikipedia articles, as this model performed well as indicated by its test F<sub>1</sub> score, despite its smaller size.</p>
<div id="fig-workflow" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-workflow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<embed src="./figs/figure1.pdf" class="img-fluid" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-workflow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Overview of the model processing pipeline
</figcaption>
</figure>
</div>
<section id="software-hardware-infrastructure" class="level3">
<h3 class="anchored" data-anchor-id="software-hardware-infrastructure">Software &amp; hardware infrastructure</h3>
<p>Models used in our paper were written in <a href="https://www.python.org/"><code>Python</code></a> using the <a href="https://allennlp.org/"><code>AllenNLP</code></a> library for deep learning in natural language processing <span class="citation" data-cites="gardner2018">(<a href="#ref-gardner2018" role="doc-biblioref">Gardner et al. 2018</a>)</span>. <code>AllenNLP</code> is built on top of <a href="https://pytorch.org"><code>PyTorch</code></a> <span class="citation" data-cites="paszke2019">(<a href="#ref-paszke2019" role="doc-biblioref">Paszke et al. 2019</a>)</span>, providing abstractions to commonly used operations for working with state-of-the-art deep neural networks in natural language processing.</p>
<p>Model training was GPU accelerated using a single NVIDIA GeForce RTX 2070 SUPER with 8192MB memory paired with a Ryzen 3700x CPU with 8 physical and 16 logical cores. <code>Python</code> version 3.8.5 was used with <code>AllenNLP</code> version 1.5.0.</p>
</section>
<section id="annotation-data-collection" class="level3">
<h3 class="anchored" data-anchor-id="annotation-data-collection">Annotation &amp; data collection</h3>
<section id="wikipedia-data-collection" class="level4">
<h4 class="anchored" data-anchor-id="wikipedia-data-collection">Wikipedia data collection</h4>
<p>Wikipedia presents a large collection of well-formatted text contributed by a variety of users, with frequent instances of place names, a consistent written style and without misspellings. Existing NER models are trained on either CoNLL-03 or OntoNotes 5, both of which are well-formatted text datasets, consisting primarily of news articles. As such, it was considered appropriate to select Wikipedia for a comparison between these models and ours, compared with other sources of VGI that are of lower overall quality.</p>
<p>The Wikipedia text data used in our paper was accessed through <a href="https://wiki.dbpedia.org/">DBpedia</a> <span class="citation" data-cites="auer2007">(<a href="#ref-auer2007" role="doc-biblioref">Auer et al. 2007</a>)</span>, a community gathered database of information from Wikipedia, presented as an open knowledge graph, with ontologies that link and define information in articles. A query was built to obtain English Wikipedia abstracts for each DBpedia article with the <code>Place</code> class in Great Britain, using the <a href="http://dbpedia.org/sparql">DBpedia SPARQL endpoint</a>. Querying just for <code>Place</code> articles within Great Britain ensured that articles extracted contained a large number of place names and language indicative of place names, without additional, unnecessary information.</p>
<p>These abstracts are the text provided at the top of each article, before any headings, sometimes called the summary. As an example, the Wikipedia abstract for <a href="https://en.wikipedia.org/wiki/Rowlatts_Hill">Rowlatts Hill</a>, a suburb of Leicester, UK is as follows, with hyperlinks indicated in bold:</p>
<blockquote class="blockquote">
<p>Rowlatts Hill (also known as Rowlatts Hill Estate, or R.H.E.) is an eastern, residential suburb of the <strong>English</strong> city of <strong>Leicester</strong>. It contains mostly <strong>council-owned housing</strong>.</p>
</blockquote>
<blockquote class="blockquote">
<p>The suburb is roughly bordered by Spencefield Lane to the east and Whitehall Road to the south, which separates it from neighbouring <strong>Evington</strong>. A second boundary within the estate consists of Coleman Road to Ambassador Road through to Green Lane Road; Rowlatts Hill borders <strong>Crown Hills</strong> to the west. To the north, at the bottom of Rowlatts Hill is Humberstone Park which is located within Green Lane Road, Ambassador Road and also leads on to Uppingham Road (the <strong>A47</strong>), which is also Rowlatts Hill.</p>
</blockquote>
<p>Using DBpedia enabled a fast executing query which, when combined with the <code>Place</code> class from the DBpedia ontology, returned a complete dataset of Wikipedia pages for many geographic locations in Great Britain. A total 42,222 article abstracts were extracted.</p>
</section>
<section id="input-format" class="level4">
<h4 class="anchored" data-anchor-id="input-format">Input format</h4>
<p>For use in the models, a random subset of 200 articles were annotated using the CoNLL-03 NER format, which uses line delimitation to separate tokens, with entities associated with each token sharing the same line, separated by a space. Articles were first cleaned using regular expressions to remove quotation marks, text inside parentheses, and non-ascii characters. The <code>SpaCy</code> large web-based pre-trained model pipeline (<code>en_core_web_lg</code>) was used for further processing, using a non-monotonic arc-eager transition-system for sentence segmentation <span class="citation" data-cites="honnibal2015">(<a href="#ref-honnibal2015" role="doc-biblioref">Honnibal and Johnson 2015</a>)</span>, and tokenisation using a rule-based algorithm. Each sentence-length sequence of tokens was treated as a separate instance to be fed as batches into models for training. Each token in every sequence was annotated as being a place name or not, assisted through the open source annotation tool <a href="https://github.com/doccano/doccano">Doccano</a> <span class="citation" data-cites="nakayama2018">(<a href="#ref-nakayama2018" role="doc-biblioref">Nakayama et al. 2018</a>)</span>.</p>
<p>For place names that span multiple tokens, the <em>BIOUL</em> tagging scheme was used, which stands for the ‘<em><strong>B</strong>eginning</em>, <em><strong>I</strong>nside</em> and <em><strong>L</strong>ast</em> tokens of multi-token chunks’; for place names that span more than one token (e.g.&nbsp;<em>B-Place</em>: New, <em>L-Place</em>: York). ‘<em><strong>U</strong>nit-length</em> chunks and <em><strong>O</strong>utside</em>’, place names of only a single token, and outside for any token that isn’t a place name. This scheme was used over the simpler BIO scheme which is more difficult for models to learn <span class="citation" data-cites="ratinov2009">(<a href="#ref-ratinov2009" role="doc-biblioref">Ratinov and Roth 2009</a>)</span>. During annotation it became clear that the length of certain multi-token place names could be considered ambiguous. For example, it may not be clear when a cardinal direction is part of a place name, ‘northern Ireland’ may refer to a northern region in Ireland, while ‘Northern Ireland’ refers to the constituent country in the United Kingdom. To unify labelling decisions we chose to consider capitalisation as an indication of multi-token noun phrases that constituted a single place name. The following sentence shows a sequence of tokens with their corresponding tags, demonstrating the annotation scheme with <em>BIOUL</em> information prepending each tag:</p>
<p>From these 200 labelled Wikipedia abstracts, 10% were kept for both validation and testing, leading to a training set of 21,080 labelled tokens, a validation dataset of 2,907 labelled tokens, and a testing dataset of 3,347 labelled tokens.</p>
</section>
</section>
<section id="building-the-entity-recognition-models" class="level3">
<h3 class="anchored" data-anchor-id="building-the-entity-recognition-models">Building the entity recognition models</h3>
<p>Named entity recognition is a subset of token classification where a sequence of tokens <span class="math inline">\(\mathbf{x} = \{x_{0}, x_{1}\dots x_{n}\}\)</span> are taken as input, and the most likely sequence tags <span class="math inline">\(\mathbf{y} = \{y_0, y_1, \dots y_n\}\)</span> are predicted. The models constructed in our paper may be divided into three main components, outlined on <a href="#fig-workflow" class="quarto-xref">Figure&nbsp;1</a>:</p>
<ul>
<li><strong>Embedding Layer</strong>: Each token in a sequence represented as high dimension numerical space, they may be either:
<ul>
<li>Randomly initialised</li>
<li>Pre-trained: GloVe, transformer</li>
</ul></li>
<li><strong>Intermediate Layers:</strong> A deep neural network that input embeddings propagate through, either:
<ul>
<li>Bidirectional LSTM</li>
<li>Transformer</li>
</ul></li>
<li><strong>Classification layer:</strong> The final layer of the model that takes a high dimensional output from the previous layers, and projects them to the classification dimension. The <code>argmax</code> from this layer corresponds to the label selected for each token. Each model uses a Conditional Random Field (CRF) to classify tokens which are popular in NER tasks, as they consider tagging decisions between all input tokens <span class="citation" data-cites="lample2016">(<a href="#ref-lample2016" role="doc-biblioref">Lample et al. 2016</a>)</span>. This is necessary given the inside tag for a place (I-PLACE), cannot directly follow a unit tag (U-PLACE) for example.</li>
</ul>
<div id="tbl-models" class="quarto-float anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-models-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Overview of the models trained through our paper, detailing the architecture used. Integers in brackets indicate the vector dimensions
</figcaption>
<div aria-describedby="tbl-models-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">

</div>
</figure>
</div>
<p><a href="#tbl-models" class="quarto-xref">Table&nbsp;1</a> gives an overview of the model architectures built through our paper. First a simplistic model was constructed as a baseline, using untrained randomly initialised 50 dimension token embeddings, fed into a two-layer Bidirectional LSTM (BiLSTM) with 200 hidden dimensions. The output from the BiLSTM was input into a conditional random field classifier. A second BiLSTM model was also created based on the architecture described in <span class="citation" data-cites="peters2018">Peters et al. (<a href="#ref-peters2018" role="doc-biblioref">2018</a>)</span>, adding pre-trained GloVe token embeddings <span class="citation" data-cites="pennington2014">(<a href="#ref-pennington2014" role="doc-biblioref">Pennington, Socher, and Manning 2014</a>)</span> with 50 dimensions and 16 dimension character embeddings. Both models used the <code>Adam</code> optimiser which makes use of stochastic gradient descent for weight optimisation <span class="citation" data-cites="kingma2017">(<a href="#ref-kingma2017" role="doc-biblioref">Kingma and Ba 2017</a>)</span>.</p>
<p>Three BERT-based transformer models were also created, using BERT <span class="citation" data-cites="devlin2019">(<a href="#ref-devlin2019" role="doc-biblioref">Devlin et al. 2019</a>)</span>, RoBERTa which attempts to optimise the training process of BERT <span class="citation" data-cites="liu2019">(<a href="#ref-liu2019" role="doc-biblioref">Liu et al. 2019</a>)</span>, and DistilBERT, which distils the data used in pre-training to create a smaller, faster model <span class="citation" data-cites="sanh2020">(<a href="#ref-sanh2020" role="doc-biblioref">Sanh et al. 2020</a>)</span>. The primary architecture of transformers is ‘attention’ which enables them to consider and weight each word in a sequence against each other word simultaneously. This allows them to be highly parallel, providing significant improvements to computational speed with GPUs which can handle highly parallel tasks, and benefits over traditional architectures like Long Short-Term Memory (LSTM) which are only able to consider sequences sequentially <span class="citation" data-cites="vaswani2017">(<a href="#ref-vaswani2017" role="doc-biblioref">Vaswani et al. 2017</a>)</span>. These models were pre-trained on very large general text corpora, enabling ‘transfer learning’, where a pre-trained model like BERT is used as a base and fine-tuned to be task specific. Conceptually, these pre-trained models learn deep embedded weights for words based on comprehensive contextual information extracted from the large general text corpora, these then only require smaller adjustments in fine-tuning to achieve good task-specific results. Fine-tuning these pre-trained models in NLP has produced results that often outperform models using traditional architectures that include manually trained word embeddings <span class="citation" data-cites="mikolov2013">(Word2Vec, <a href="#ref-mikolov2013" role="doc-biblioref">Mikolov et al. 2013</a>)</span>, which are limited by the volume of data provided to them and pre-trained embeddings like GloVe <span class="citation" data-cites="pennington2014">(<a href="#ref-pennington2014" role="doc-biblioref">Pennington, Socher, and Manning 2014</a>)</span>.</p>
<p>Pre-trained transformer models replace both the BiLSTM layers of the previous models and token embeddings, taking encoded sequences, associating each token with a 768 dimension vector representation from a vocabulary, feeding them into sequential transformer layers and outputting into a CRF classifier. Each model was initialised with pre-trained weights provided by the <code>transformers</code> Python library <span class="citation" data-cites="wolf2020">(<a href="#ref-wolf2020" role="doc-biblioref">Wolf et al. 2020</a>)</span>, these weights are initialised in both the embedding layers and intermediate layers. For weight optimisation, these models used the weight decay Adam algorithm <span class="citation" data-cites="loshchilov2019">(<code>AdamW</code>, <a href="#ref-loshchilov2019" role="doc-biblioref">Loshchilov and Hutter 2019</a>)</span>. Every layer of the transformer models was updated during training, which enabled the pre-trained weights to adjust and learn for the specific task. Hyper-parameters selected for each model were largely based on the values as suggested for token classification by their respective implementation papers.</p>
<p>For every model, weights were adjusted each epoch to minimise the training loss. Following the final intermediate layer of a model, a token representation <span class="math inline">\(C\in\mathbb{R}^H\)</span> feeds into the classification layer weights <span class="math inline">\(W\in\mathbb{R}^{K\times H}\)</span>, where <span class="math inline">\(K\)</span> is the number of unique labels. Classification loss is then calculated using <span class="math inline">\(log(softmax(CW^T))\)</span>.</p>
<p>Early stopping was used in each model, stopping training early if no improvement was made to the validation F<sub>1</sub> score in eight subsequent epochs. Automatic Mixed Precision (AMP) was used throughout training to use half-precision (16 bit) floating point numbers in some operations which reduced the memory overhead and increased computation speed. For transformers, the learning rate was optimised towards the end of training, using a <code>reduce on plateau</code> learning rate scheduler, reducing the learning rate by 1/10th once the overall F<sub>1</sub> validation metric had stopped improving after two epochs, this only increased training time on the BiLSTM models with no improvement, so was excluded. Following training, the weights from the best performing epoch were automatically chosen for the final model.</p>
</section>
<section id="evaluation-against-pre-built-models" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-against-pre-built-models">Evaluation against pre-built models</h3>
<p>Following the training of each model, their accuracy, precision, recall and F<sub>1</sub> score was evaluated using a corpus of test data, against three popular modern pre-built NER models provided through the <code>SpaCy</code> and <code>Stanza</code> Python packages. A <code>SpaCy</code> model is used in the <em>Mordecai</em> geoparser and optionally in the <em>GeoTxt</em> geoparser, while the <code>Stanza</code> model is a more recent implementation of the Stanford NLP model used by the <em>GeoTxt</em> geoparser.</p>
<p>As these pre-built models were not trained to recognise ‘place names’, their tags were adjusted so that anything labelled as ‘GPE’ (Geopolitical Entity), ‘LOC’ (Location), or ‘FAC’ (facility) was considered to be a ‘place name’, mirroring the process used to discard unrelated entities by geoparsing systems that use these models. The default <code>Stanza</code> NER model, and two <code>SpaCy</code> models (<code>en_core_web_sm</code>, <code>en_core_web_lg</code>) were evaluated on the labelled test data. <a href="#tbl-prebuilt" class="quarto-xref">Table&nbsp;2</a> gives an overview of these pre-built models.</p>
<p>Each model was evaluated on 3 separate subsets of the annotated test dataset, giving a range of scores for each model. Significance testing was then performed using paired t-tests to test the null hypothesis:</p>
<blockquote class="blockquote">
<p><span class="math inline">\(\mathbf{H_0}\)</span>: There will be no statistically significant difference between the mean F<sub>1</sub> score of each custom built model against the best performing pre-built model (<code>Stanza</code>).</p>
</blockquote>
<p>Significant results that reject this null hypothesis were indicated by <span class="math inline">\(p&lt;0.05\)</span> and are shown on <a href="#tbl-eval" class="quarto-xref">Table&nbsp;3</a>.</p>
<p>The best performing model trained on the annotated Wikipedia data was also evaluated using paired t-tests against each other model trained on the same data, to test the null hypothesis:</p>
<blockquote class="blockquote">
<p><span class="math inline">\(\mathbf{H_0}\)</span>: There will be no statistically significant difference between the mean F<sub>1</sub> score of the best performing custom built model trained on annotated Wikipedia data and each other model trained on this data.</p>
</blockquote>
<p>Significant results that reject this null hypothesis were also indicated by <span class="math inline">\(p&lt;0.05\)</span>.</p>
<p>It should be noted that significance testing is not common in deep learning research <span class="citation" data-cites="dror2018a">(<a href="#ref-dror2018a" role="doc-biblioref">Dror and Reichart 2018</a>)</span>, but papers that do report the significance of mean scores between models tend to use paired t-tests, despite potentially violating the parametric assumptions made. <span class="citation" data-cites="dror2018a">Dror and Reichart (<a href="#ref-dror2018a" role="doc-biblioref">2018</a>)</span> suggest that while normality may be assumed due to the Central Limit Theorem, it is likely that future progress in this field will present more appropriate statistical significance testing.</p>
<div id="tbl-prebuilt" class="quarto-float anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-prebuilt-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2: Pre-built NER models
</figcaption>
<div aria-describedby="tbl-prebuilt-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">

</div>
</figure>
</div>
</section>
<section id="output-processing" class="level3">
<h3 class="anchored" data-anchor-id="output-processing">Output processing</h3>
<p>A predictor was created from the DistilBERT model to run inference over the total corpus of Wikipedia articles. Place names extracted from the Wikipedia articles by this model were saved to a CSV file with the context sentence, the associated article, and coordinate information for the article that contained the place.</p>
<p>Place names were compared against a full corpus of British place names from the GeoNames gazetteer, to examine which names are excluded from the gazetteer, but identified within Wikipedia articles.</p>
</section>
</section>
<section id="sec-trans-results-discussion" class="level2">
<h2 class="anchored" data-anchor-id="sec-trans-results-discussion">Results &amp; discussion</h2>
<p>This section first evaluates the results of the models presented against each other, and in relation to existing pre-built NER solutions. The place names extracted by our best performing model are compared with pre-built models, showing how our method improves on those used in existing place name extraction methods. Following this, examples from the corpus of place names extracted from Wikipedia articles are noted, demonstrating use-cases for the method presented that wouldn’t be possible or as effective, through pre-built NER solutions.</p>
<section id="model-performance" class="level3">
<h3 class="anchored" data-anchor-id="model-performance">Model performance</h3>
<p><a href="#tbl-eval" class="quarto-xref">Table&nbsp;3</a> shows three popular pre-built NER models, evaluated on the labelled Wikipedia test data, compared with the models produced through our paper. The <code>BiLSTM-CRF (basic)</code> model gives a baseline reference for a typical NER model with a simple architecture. Out of the pre-built models, <code>Stanza</code> performs the best, achieving precision and accuracy just below the trained baseline model, with an F<sub>1</sub> score which isn’t significantly worse (paired t-test <span class="math inline">\(p&gt;0.05\)</span>), both <code>SpaCy</code> models however show notably worse results compared with <code>Stanza</code>. The primary issue with the pre-built models is recall, which is far below any of the custom-built models, reflecting a high number of false negatives.</p>
<div id="tbl-eval" class="quarto-float anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-eval-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3: Geographic entity recognition mean (±SD) performance metrics over 3 runs of annotated Wikipedia test data subsets. Pre-built NER models are shown in italics. Bold values indicate statistically significant F1 scores of fine-tuned models in relation to <code>Stanza</code> (Paired t-tests <span class="math inline">\(p&lt;0.05\)</span>).
</figcaption>
<div aria-describedby="tbl-eval-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">

</div>
</figure>
</div>
<p>It is worth noting that due to class imbalances, i.e.&nbsp;many more ‘other’ (<code>O</code>) entities relative to the small number of <code>PLACE</code> entities, accuracy should be considered a poor metric, and is only included for completeness. This class imbalance means that as only approximately 15% of tokens are labelled as entities, it is possible to achieve 85% accuracy and high precision by labelling all tokens as not entities. F<sub>1</sub> score is often used to compensate for these issues in multiple classification tasks, but it should be known that it is not itself a perfect metric. With respect to the best performing pre-built model <code>Stanza</code>, all transformer models fine-tuned on the Wikipedia annotated data, have significantly higher F<sub>1</sub> scores (paired t-test <span class="math inline">\(p&lt;0.05\)</span>).</p>
<p>The DistilBERT transformer model is less complex than both the BERT and RoBERTa model, with a total of 260 MB in model weights, compared with 433 MB and 498 MB respectively. Despite this, the DistilBERT model achieves similar results to RoBERTa on test data (<a href="#tbl-eval" class="quarto-xref">Table&nbsp;3</a>). While all transformer models perform significantly better than the best performing pre-built model, Stanza, both CRF models do not give significantly better F<sub>1</sub> scores (paired t-test <span class="math inline">\(p&gt;0.05\)</span>). BERT performs best overall, with an F<sub>1</sub> score of 0.939 on the test data, a result that is only significantly better than the two CRF models (paired t-test <span class="math inline">\(p&lt;0.05)\)</span>.</p>
<p><a href="#fig-qual" class="quarto-xref">Figure&nbsp;2</a> shows the output of the chosen fine-tuned NER model <code>DistilBERT</code> alongside <code>SpaCy (large)</code> and <code>Stanza</code>, applied to a simple Wikipedia article summary. <a href="#fig-qual" class="quarto-xref">Figure&nbsp;2</a> (A) gives promising results for <code>DistilBERT</code>, with the summary for the Wikipedia page ‘Rowlatts Hill’, correctly identifying all place names.</p>
<p>While evaluation metrics indicate that <code>Stanza</code> performs reasonably well, it primarily suffers from the annotation scheme used, some place names are misidentified as ‘Person’, or ‘Organisation’, meaning a standard geoparsing system would miss several place names here, given they are not otherwise identifiable (<a href="#fig-qual" class="quarto-xref">Figure&nbsp;2</a>).</p>
<div id="fig-qual" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-qual-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./figs/figure2.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-qual-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Comparison of outputs between the best performing fine-tuned transformer model and the two best performing pre-built NER models.
</figcaption>
</figure>
</div>
<div id="fig-qual-china" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-qual-china-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./figs/figure3.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-qual-china-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Ability for trained model to distinguish between metonymic usage of place names.
</figcaption>
</figure>
</div>
<p><a href="#fig-qual-china" class="quarto-xref">Figure&nbsp;3</a> demonstrates the ability for our DistilBERT transformer model to accurately ignore entities that do not relate to place names. This example paragraph only refers to a single geographic location in text, the location of the 1952 Summer Games, in Helsinki, Finland. While Stanza identifies a large number of GPE tags, they either relate to China used in a metynomic sense, meaning the Chinese Olympic team (‘China competed’), or as a related geopolitical noun (‘delegation of ROC’), which is not considered to be a place name referring to a geographic location in this context. Our model correctly infers the single mention of a geographic place name based on the contextual information, meaning a large amount of unrelated information is excluded. Particularly, recognising and ignoring these nouns related to place names is something that is noted as an issue in current geoparsing systems <span class="citation" data-cites="gritta2020">(<a href="#ref-gritta2020" role="doc-biblioref">Gritta, Pilehvar, and Collier 2020</a>)</span>. This figure also demonstrates the importance of using a pre-trained model base for this task, as the BiLSTM CRF performs poorly. It is likely that this issue stems from the limited training data used, as the model is unable to learn more complex cases where place names are less obvious (<a href="#fig-qual-china" class="quarto-xref">Figure&nbsp;3</a> (B)). Using a pre-trained transformer enables the model to correctly identify instances where proper nouns do not relate to place names, taking information learned through its pre-training procedure.</p>
</section>
<section id="identified-place-names-from-wikipedia" class="level3">
<h3 class="anchored" data-anchor-id="identified-place-names-from-wikipedia">Identified place names from Wikipedia</h3>
<div id="tbl-freq" class="quarto-float anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-freq-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4: Top and bottom named places by frequency, excluding any present in the GeoNames gazetteer or mentioned less than 100 times.
</figcaption>
<div aria-describedby="tbl-freq-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">

</div>
</figure>
</div>
<p><a href="#tbl-freq" class="quarto-xref">Table&nbsp;4</a> gives an overview of the most common place names identified by the DistilBERT model and the SpaCy model. Notably, the SpaCy model appears to struggle with correctly aligning entities, including ‘the’ with ‘United Kingdom’, and partially missing place names containing ‘Tyne’ (e.g.&nbsp;‘Tyne and Wear’ or ‘River Tyne’). The DistilBERT model also extracts around 6 times the number of place names compared with SpaCy, reflected by the low recall noted above. One example where the DistilBERT model appears confused is by giving the place name ‘Church of England’, this problem relates to the language used in Wikipedia articles, when churches are described as a ‘Church of England church’, a nominal mention of a place rather than specific.</p>
<p>The total number of place names extracted from the Wikipedia summaries by the <code>DistilBERT</code> model was 614,672, with 99,697 unique place names. In total 62,178 unique place names were extracted that are not found within the GeoNames gazetteer. These entities primarily exist as granular names mentioned in single instances (e.g.&nbsp;road names: Shady Lane, Chapeltown Road), organisational names used in a place related context (e.g.&nbsp;describing locations along the Great Western Railway route), and alternative names that are not captured by GeoNames. For example, ‘M1’ appears in GeoNames as ‘M1 Motorway’<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>. While the ‘M1 motorway’ is used in Wikipedia articles, it is often also referred to as just the ‘M1’.</p>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Our paper demonstrates a new approach towards the extraction of place names from text by building an NER model using data annotated with geographic place names. This work aims to direct geographic NLP research towards the use of models which move away from the generalisable annotation schemes of pre-built NER solutions, to include task-specific, relevant training data. Notably this differs from the perceived generalisability of pre-built models used for general geoparsing. We believe this is an important approach for geographic place name extraction given geographic language differs greatly based on context <span class="citation" data-cites="purves2018">(<a href="#ref-purves2018" role="doc-biblioref">Purves et al. 2018</a>)</span>, with contexts varying greatly based on the corpora used for inference. This is demonstrated by the poor results observed in previous work when applying pre-built NER solutions, which use training data unrelated to the task-specific data they are being applied to <span class="citation" data-cites="hu2019 karimzadeh2019">(<a href="#ref-hu2019" role="doc-biblioref">Hu, Mao, and McKenzie 2019</a>; <a href="#ref-karimzadeh2019" role="doc-biblioref">Karimzadeh et al. 2019</a>)</span>. <span class="citation" data-cites="wallgrun2018">Wallgrün et al. (<a href="#ref-wallgrun2018" role="doc-biblioref">2018</a>)</span> recognise this problem, developing GeoCorpora, a task-specific training dataset for micro-blog geoparsing, notably describing increased issues with annotation ambiguity compared with more traditional text-sources. Additionally, recent work with transformer models, typically only built to be generalisable, have considered moving from fully generalised self-supervised training towards more dataset-specific models (e.g.&nbsp;TweetEval; <span class="citation" data-cites="barbieri2020">Barbieri et al. (<a href="#ref-barbieri2020" role="doc-biblioref">2020</a>)</span>), with results that outperform generalisable transformer models <span class="citation" data-cites="nguyen2020">(<a href="#ref-nguyen2020" role="doc-biblioref">Nguyen, Vu, and Nguyen 2020</a>)</span>.</p>
<p>Ultimately, the decision to produce a model explicitly designed to be non-generalisable to other corpora may be considered a limitation of the scope of this paper. We have demonstrated a best-case scenario where time-frames allow for manual annotation of task-specific data. Future research may consider the construction of a more generalisable place name extraction model, which takes inspiration from the alternative annotation scheme employed by our paper, allowing for use in general purpose geoparsers.</p>
<p>Additionally, while our paper selects Wikipedia for place name extraction, due to its large volume, ease of validation and data retrieval, future work may consider the ability to apply our methodology to other text sources. With suitable models constructed, using annotated training data that is relevant to the corpus being considered, we expect future work applied to other data sources may present the opportunity to further contribute to place names that are absent from gazetteers, as vernacular place names. We believe that given a suitable combination of data sources, our methodology is the first step towards the construction gazetteers from the bottom-up, directly taking place names from passive contributions, without relying on pre-built datasets.</p>
<p>The recent development of pre-trained language models and their suitability for fine-tuning in many tasks, including NER, presents a method for the construction of accurate models that are task specific, using relatively small labelled corpora<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> that defines entities more suited to the task of place name extraction. The architecture in our paper is more simplistic to implement than other attempts at similar tasks <span class="citation" data-cites="weissenbacher2019">(e.g. <a href="#ref-weissenbacher2019" role="doc-biblioref">Weissenbacher et al. 2019</a>)</span>, with most of the complexity hidden within the transformer layers. This, combined with libraries that abstract and implement state of the art models, provides a more accessible approach for research in place name extraction, without requiring a deep understanding of semantic rules, or the construction of deep multi-layered models from the ground up.</p>
<p>Evaluation against pre-built NER models on <a href="#tbl-eval" class="quarto-xref">Table&nbsp;3</a> shows that performance for place name extraction is greatly improved, particularly with respect to recall, a notable issue with past studies <span class="citation" data-cites="hu2019 karimzadeh2019">(<a href="#ref-hu2019" role="doc-biblioref">Hu, Mao, and McKenzie 2019</a>; <a href="#ref-karimzadeh2019" role="doc-biblioref">Karimzadeh et al. 2019</a>)</span>. The construction of an NER model for the task specific extraction of place names moves towards systems that appropriately consider the geographic elements present in natural language. The large number of place names that are absent from the GeoNames gazetteer suggests that geoparsing and related work likely misses a substantial amount of geographic information present in text. The dataset produced through this work aims to assist with filling these gaps, while the methodology described enables an approach that may be mirrored and applied to further work on other data sources.</p>
<p>Finally, both ‘place’ focussed annotation schemes describe the use of ‘nominal’ place related entities <span class="citation" data-cites="mani2010 pustejovsky2017">(<a href="#ref-mani2010" role="doc-biblioref">Mani et al. 2010</a>; <a href="#ref-pustejovsky2017" role="doc-biblioref">Pustejovsky 2017</a>)</span>. While out of the scope of our work, we would like to encourage the focus on extracting this additional geographic information from text. Often in language the use of these non-specific terms are used, for example ‘I visited the shops’, ‘York is a city’, provide geographically specific information. ‘The shops’ with enough context may provide a specific geographic location, and similarly the link between ‘York’ -&gt; ‘city’ could be explored <span class="citation" data-cites="couclelis2010">(<a href="#ref-couclelis2010" role="doc-biblioref">Couclelis 2010</a>)</span>.</p>
</section>
<section id="data-and-codes-availability-statement" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="data-and-codes-availability-statement">Data and codes availability statement</h2>
<p>The data and codes that support the findings of this study are available at the public FigShare link (<a href="https://doi.org/10.6084/m9.figshare.13415255.v1" class="uri">https://doi.org/10.6084/m9.figshare.13415255.v1</a>). Instructions for using the data and code are provided as a README within the FigShare repository.</p>
</section>
<section id="disclosure-statement" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="disclosure-statement">Disclosure statement</h2>
<p>No potential competing interest was reported by the authors.</p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-antoniou2010" class="csl-entry" role="listitem">
Antoniou, Vyron, Jeremy Morley, and Muki Haklay. 2010. <span>“Web 2.0 Geotagged Photos: <span>Assessing</span> the Spatial Dimension of the Phenomenon.”</span> <em>Geomatica</em> 64 (January): 99–110.
</div>
<div id="ref-auer2007" class="csl-entry" role="listitem">
Auer, Sören, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives. 2007. <span>“<span>DBpedia</span>: <span>A Nucleus</span> for a <span>Web</span> of <span>Open Data</span>.”</span> In <em>The <span>Semantic Web</span></em>, edited by David Hutchison, Takeo Kanade, Josef Kittler, Jon M. Kleinberg, Friedemann Mattern, John C. Mitchell, Moni Naor, et al., 4825:722–35. <span>Berlin, Heidelberg</span>: <span>Springer Berlin Heidelberg</span>. <a href="https://doi.org/10.1007/978-3-540-76298-0_52">https://doi.org/10.1007/978-3-540-76298-0_52</a>.
</div>
<div id="ref-barbieri2020" class="csl-entry" role="listitem">
Barbieri, Francesco, Jose Camacho-Collados, Leonardo Neves, and Luis Espinosa-Anke. 2020. <span>“<span>TweetEval</span>: <span>Unified Benchmark</span> and <span>Comparative Evaluation</span> for <span>Tweet Classification</span>.”</span> <em>arXiv:2010.12421 [Cs]</em>, October. <a href="https://arxiv.org/abs/2010.12421">https://arxiv.org/abs/2010.12421</a>.
</div>
<div id="ref-buscaldi2011" class="csl-entry" role="listitem">
Buscaldi, Davide. 2011. <span>“Approaches to Disambiguating Toponyms.”</span> <em>SIGSPATIAL Special</em> 3 (2): 16–19. <a href="https://doi.org/10.1145/2047296.2047300">https://doi.org/10.1145/2047296.2047300</a>.
</div>
<div id="ref-couclelis2010" class="csl-entry" role="listitem">
Couclelis, Helen. 2010. <span>“Ontologies of Geographic Information.”</span> <em>International Journal of Geographical Information Science</em> 24 (12): 1785–1809. <a href="https://doi.org/10.1080/13658816.2010.484392">https://doi.org/10.1080/13658816.2010.484392</a>.
</div>
<div id="ref-delozier2015" class="csl-entry" role="listitem">
DeLozier, Grant, Jason Baldridge, and Loretta London. 2015. <span>“Gazetteer-<span>Independent Toponym Resolution Using Geographic Word Profiles</span>,”</span> 7. <a href="https://doi.org/10.1609/aaai.v29i1.9531">https://doi.org/10.1609/aaai.v29i1.9531</a>.
</div>
<div id="ref-devlin2019" class="csl-entry" role="listitem">
Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. <span>“<span>BERT</span>: <span class="nocase">Pre-training</span> of <span>Deep Bidirectional Transformers</span> for <span>Language Understanding</span>.”</span> <em>arXiv:1810.04805 [Cs]</em>, May. <a href="https://arxiv.org/abs/1810.04805">https://arxiv.org/abs/1810.04805</a>.
</div>
<div id="ref-dror2018a" class="csl-entry" role="listitem">
Dror, Rotem, and Roi Reichart. 2018. <span>“Appendix - <span>Recommended Statistical Significance Tests</span> for <span>NLP Tasks</span>.”</span> <em>arXiv:1809.01448 [Cs]</em>, September. <a href="https://arxiv.org/abs/1809.01448">https://arxiv.org/abs/1809.01448</a>.
</div>
<div id="ref-gao2017" class="csl-entry" role="listitem">
Gao, Song, Krzysztof Janowicz, and Helen Couclelis. 2017. <span>“Extracting Urban Functional Regions from Points of Interest and Human Activities on Location-Based Social Networks.”</span> <em>Transactions in GIS</em> 21 (3): 446–67. <a href="https://doi.org/10.1111/tgis.12289">https://doi.org/10.1111/tgis.12289</a>.
</div>
<div id="ref-gao2013" class="csl-entry" role="listitem">
Gao, Song, Krzysztof Janowicz, Grant McKenzie, and Linna Li. 2013. <span>“Towards <span>Platial Joins</span> and <span>Buffers</span> in <span>Place-Based GIS</span>,”</span> 8.
</div>
<div id="ref-gardner2018" class="csl-entry" role="listitem">
Gardner, Matt, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson F. Liu, Matthew Peters, Michael Schmitz, and Luke Zettlemoyer. 2018. <span>“<span>AllenNLP</span>: <span>A Deep Semantic Natural Language Processing Platform</span>.”</span> In <em>Proceedings of <span>Workshop</span> for <span>NLP Open Source Software</span> (<span>NLP-OSS</span>)</em>, 1–6. <span>Melbourne, Australia</span>: <span>Association for Computational Linguistics</span>. <a href="https://doi.org/10.18653/v1/w18-2501">https://doi.org/10.18653/v1/w18-2501</a>.
</div>
<div id="ref-goodchild2007" class="csl-entry" role="listitem">
Goodchild, Michael F. 2007. <span>“Citizens as Sensors: The World of Volunteered Geography.”</span> <em>GeoJournal</em> 69 (4): 211–21. <a href="https://doi.org/10.1007/s10708-007-9111-y">https://doi.org/10.1007/s10708-007-9111-y</a>.
</div>
<div id="ref-goodchild2011" class="csl-entry" role="listitem">
Goodchild, Michael F., and Linna Li. 2011. <span>“Formalizing Space and Place.”</span>
</div>
<div id="ref-gritta2020" class="csl-entry" role="listitem">
Gritta, Milan, Mohammad Taher Pilehvar, and Nigel Collier. 2020. <span>“A Pragmatic Guide to Geoparsing Evaluation: <span>Toponyms</span>, <span>Named Entity Recognition</span> and Pragmatics.”</span> <em>Language Resources and Evaluation</em> 54 (3): 683–712. <a href="https://doi.org/10.1007/s10579-019-09475-3">https://doi.org/10.1007/s10579-019-09475-3</a>.
</div>
<div id="ref-gritta2017a" class="csl-entry" role="listitem">
Gritta, Milan, Mohammad Taher Pilehvar, Nut Limsopatham, and Nigel Collier. 2017. <span>“What’s Missing in Geographical Parsing?”</span> <em>Language Resources and Evaluation</em> 52 (2): 603–23. <a href="https://doi.org/ggwjt9">https://doi.org/ggwjt9</a>.
</div>
<div id="ref-halterman2017" class="csl-entry" role="listitem">
Halterman, Andrew. 2017. <span>“Mordecai: <span>Full</span> Text Geoparsing and Event Geocoding.”</span> <em>The Journal of Open Source Software</em> 2 (9). <a href="https://doi.org/10.21105/joss.00091">https://doi.org/10.21105/joss.00091</a>.
</div>
<div id="ref-hollenstein2010" class="csl-entry" role="listitem">
Hollenstein, Livia, and Ross Purves. 2010. <span>“Exploring Place Through User-Generated Content: <span>Using Flickr</span> Tags to Describe City Cores.”</span> <em>Journal of Spatial Information Science</em>, no. 1 (December): 21–48.
</div>
<div id="ref-honnibal2015" class="csl-entry" role="listitem">
Honnibal, Matthew, and Mark Johnson. 2015. <span>“An <span class="nocase">Improved Non-monotonic Transition System</span> for <span>Dependency Parsing</span>.”</span> In <em>Proceedings of the 2015 <span>Conference</span> on <span>Empirical Methods</span> in <span>Natural Language Processing</span></em>, 1373–78. <span>Lisbon, Portugal</span>: <span>Association for Computational Linguistics</span>. <a href="https://doi.org/10.18653/v1/d15-1162">https://doi.org/10.18653/v1/d15-1162</a>.
</div>
<div id="ref-honnibal2017" class="csl-entry" role="listitem">
Honnibal, Matthew, and Ines Montani. 2017. <span>“Spacy 2: <span>Natural</span> Language Understanding with Bloom Embeddings, Convolutional Neural Networks and Incremental Parsing.”</span> <em>To Appear</em> 7 (1).
</div>
<div id="ref-hu2019" class="csl-entry" role="listitem">
Hu, Yingjie, Huina Mao, and Grant McKenzie. 2019. <span>“A Natural Language Processing and Geospatial Clustering Framework for Harvesting Local Place Names from Geotagged Housing Advertisements.”</span> <em>International Journal of Geographical Information Science</em> 33 (4): 714–38. <a href="https://doi.org/10.1080/13658816.2018.1458986">https://doi.org/10.1080/13658816.2018.1458986</a>.
</div>
<div id="ref-karimzadeh2019" class="csl-entry" role="listitem">
Karimzadeh, Morteza, Scott Pezanowski, Alan M. MacEachren, and Jan O. Wallgrün. 2019. <span>“<span>GeoTxt</span>: <span>A</span> Scalable Geoparsing System for Unstructured Text Geolocation.”</span> <em>Transactions in GIS</em> 23 (1): 118–36. <a href="https://doi.org/10.1111/tgis.12510">https://doi.org/10.1111/tgis.12510</a>.
</div>
<div id="ref-kingma2017" class="csl-entry" role="listitem">
Kingma, Diederik P., and Jimmy Ba. 2017. <span>“Adam: <span>A Method</span> for <span>Stochastic Optimization</span>.”</span> <em>arXiv:1412.6980 [Cs]</em>, January. <a href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</a>.
</div>
<div id="ref-kumar2019" class="csl-entry" role="listitem">
Kumar, Abhinav, and Jyoti Prakash Singh. 2019. <span>“Location Reference Identification from Tweets During Emergencies: <span>A</span> Deep Learning Approach.”</span> <em>International Journal of Disaster Risk Reduction</em> 33 (February): 365–75. <a href="https://doi.org/10.1016/j.ijdrr.2018.10.021">https://doi.org/10.1016/j.ijdrr.2018.10.021</a>.
</div>
<div id="ref-lample2016" class="csl-entry" role="listitem">
Lample, Guillaume, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer. 2016. <span>“Neural <span>Architectures</span> for <span>Named Entity Recognition</span>.”</span> <em>arXiv:1603.01360 [Cs]</em>, April. <a href="https://arxiv.org/abs/1603.01360">https://arxiv.org/abs/1603.01360</a>.
</div>
<div id="ref-leidner2008" class="csl-entry" role="listitem">
Leidner, Jochen Lothar. 2008. <span>“Toponym <span>Resolution</span> in <span>Text</span>,”</span> 287. <a href="https://doi.org/10.1145/1328964.1328989">https://doi.org/10.1145/1328964.1328989</a>.
</div>
<div id="ref-liu2019" class="csl-entry" role="listitem">
Liu, Yinhan, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. <span>“<span>RoBERTa</span>: <span>A Robustly Optimized BERT Pretraining Approach</span>.”</span> <em>arXiv:1907.11692 [Cs]</em>, July. <a href="https://arxiv.org/abs/1907.11692">https://arxiv.org/abs/1907.11692</a>.
</div>
<div id="ref-loshchilov2019" class="csl-entry" role="listitem">
Loshchilov, Ilya, and Frank Hutter. 2019. <span>“Decoupled <span>Weight Decay Regularization</span>.”</span> <em>arXiv:1711.05101 [Cs, Math]</em>, January. <a href="https://arxiv.org/abs/1711.05101">https://arxiv.org/abs/1711.05101</a>.
</div>
<div id="ref-mani2010" class="csl-entry" role="listitem">
Mani, Inderjeet, Christy Doran, Dave Harris, Janet Hitzeman, Rob Quimby, Justin Richer, Ben Wellner, Scott Mardis, and Seamus Clancy. 2010. <span>“<span>SpatialML</span>: Annotation Scheme, Resources, and Evaluation.”</span> <em>Language Resources and Evaluation</em> 44 (3): 263–80. <a href="https://doi.org/10.1007/s10579-010-9121-0">https://doi.org/10.1007/s10579-010-9121-0</a>.
</div>
<div id="ref-mikolov2013" class="csl-entry" role="listitem">
Mikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. <span>“Efficient Estimation of Word Representations in Vector Space.”</span> <em>arXiv Preprint arXiv:1301.3781</em>. <a href="https://arxiv.org/abs/1301.3781">https://arxiv.org/abs/1301.3781</a>.
</div>
<div id="ref-moncla2014" class="csl-entry" role="listitem">
Moncla, Ludovic, Walter Renteria-Agualimpia, Javier Nogueras-Iso, and Mauro Gaio. 2014. <span>“Geocoding for Texts with Fine-Grain Toponyms: An Experiment on a Geoparsed Hiking Descriptions Corpus.”</span> In <em>Proceedings of the 22nd <span>ACM SIGSPATIAL International Conference</span> on <span>Advances</span> in <span>Geographic Information Systems</span></em>, 183–92. <span>Dallas Texas</span>: <span>ACM</span>. <a href="https://doi.org/10.1145/2666310.2666386">https://doi.org/10.1145/2666310.2666386</a>.
</div>
<div id="ref-nakayama2018" class="csl-entry" role="listitem">
Nakayama, Hiroki, Takahiro Kubo, Junya Kamura, Yasufumi Taniguchi, and Xu Liang. 2018. <span>“Doccano: Text Annotation for Humans.”</span>
</div>
<div id="ref-nguyen2020" class="csl-entry" role="listitem">
Nguyen, Dat Quoc, Thanh Vu, and Anh Tuan Nguyen. 2020. <span>“<span>BERTweet</span>: <span>A</span> Pre-Trained Language Model for <span>English Tweets</span>.”</span> <em>arXiv:2005.10200 [Cs]</em>, October. <a href="https://arxiv.org/abs/2005.10200">https://arxiv.org/abs/2005.10200</a>.
</div>
<div id="ref-paszke2019" class="csl-entry" role="listitem">
Paszke, Adam, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, et al. 2019. <span>“<span>PyTorch</span>: <span>An Imperative Style</span>, <span>High-Performance Deep Learning Library</span>.”</span> <span>arXiv</span>. <a href="https://arxiv.org/abs/1912.01703">https://arxiv.org/abs/1912.01703</a>.
</div>
<div id="ref-pennington2014" class="csl-entry" role="listitem">
Pennington, Jeffrey, Richard Socher, and Christopher Manning. 2014. <span>“<span>GloVe</span>: <span>Global Vectors</span> for <span>Word Representation</span>.”</span> In <em>Proceedings of the 2014 <span>Conference</span> on <span>Empirical Methods</span> in <span>Natural Language Processing</span> (<span>EMNLP</span>)</em>, 1532–43. <span>Doha, Qatar</span>: <span>Association for Computational Linguistics</span>. <a href="https://doi.org/10.3115/v1/d14-1162">https://doi.org/10.3115/v1/d14-1162</a>.
</div>
<div id="ref-peters2018" class="csl-entry" role="listitem">
Peters, Matthew E., Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. <span>“Deep Contextualized Word Representations.”</span> <em>arXiv:1802.05365 [Cs]</em>, March. <a href="https://arxiv.org/abs/1802.05365">https://arxiv.org/abs/1802.05365</a>.
</div>
<div id="ref-purves2018" class="csl-entry" role="listitem">
Purves, Ross S., Paul Clough, Christopher B. Jones, Mark H. Hall, and Vanessa Murdock. 2018. <em>Geographic <span>Information Retrieval</span>: <span>Progress</span> and <span>Challenges</span> in <span>Spatial Search</span> of <span>Text</span></em>. <span>now</span>. <a href="https://doi.org/10.1561/1500000034">https://doi.org/10.1561/1500000034</a>.
</div>
<div id="ref-pustejovsky2017" class="csl-entry" role="listitem">
Pustejovsky, James. 2017. <span>“<span>ISO-Space</span>: <span>Annotating Static</span> and <span>Dynamic Spatial Information</span>.”</span> In <em>Handbook of <span>Linguistic Annotation</span></em>, edited by Nancy Ide and James Pustejovsky, 989–1024. <span>Dordrecht</span>: <span>Springer Netherlands</span>. <a href="https://doi.org/10.1007/978-94-024-0881-2_37">https://doi.org/10.1007/978-94-024-0881-2_37</a>.
</div>
<div id="ref-pustejovsky2015" class="csl-entry" role="listitem">
Pustejovsky, James, Parisa Kordjamshidi, Marie-Francine Moens, Aaron Levine, Seth Dworman, and Zachary Yocum. 2015. <span>“<span>SemEval-2015 Task</span> 8: <span>SpaceEval</span>.”</span> In <em>Proceedings of the 9th <span>International Workshop</span> on <span>Semantic Evaluation</span> (<span>SemEval</span> 2015)</em>, 884–94. <span>Denver, Colorado</span>: <span>Association for Computational Linguistics</span>. <a href="https://doi.org/10.18653/v1/s15-2149">https://doi.org/10.18653/v1/s15-2149</a>.
</div>
<div id="ref-qi2018" class="csl-entry" role="listitem">
Qi, Peng, Timothy Dozat, Yuhao Zhang, and Christopher D. Manning. 2018. <span>“Universal <span>Dependency Parsing</span> from <span>Scratch</span>.”</span> In <em>Proceedings of the</em>, 160–70. <span>Brussels, Belgium</span>: <span>Association for Computational Linguistics</span>. <a href="https://doi.org/10.18653/v1/k18-2016">https://doi.org/10.18653/v1/k18-2016</a>.
</div>
<div id="ref-ratinov2009" class="csl-entry" role="listitem">
Ratinov, Lev, and Dan Roth. 2009. <span>“Design <span>Challenges</span> and <span>Misconceptions</span> in <span>Named Entity Recognition</span>.”</span> In <em>Proceedings of the <span>Thirteenth Conference</span> on <span>Computational Natural Language Learning</span> (<span>CoNLL-2009</span>)</em>, 147–55. <span>Boulder, Colorado</span>: <span>Association for Computational Linguistics</span>.
</div>
<div id="ref-sanh2020" class="csl-entry" role="listitem">
Sanh, Victor, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2020. <span>“<span>DistilBERT</span>, a Distilled Version of <span>BERT</span>: Smaller, Faster, Cheaper and Lighter.”</span> <em>arXiv:1910.01108 [Cs]</em>, February. <a href="https://arxiv.org/abs/1910.01108">https://arxiv.org/abs/1910.01108</a>.
</div>
<div id="ref-see2016" class="csl-entry" role="listitem">
See, Linda, Peter Mooney, Giles Foody, Lucy Bastin, Alexis Comber, Jacinto Estima, Steffen Fritz, et al. 2016. <span>“Crowdsourcing, <span>Citizen Science</span> or <span>Volunteered Geographic Information</span>? <span>The Current State</span> of <span>Crowdsourced Geographic Information</span>.”</span> <em>ISPRS International Journal of Geo-Information</em> 5 (5): 55. <a href="https://doi.org/10.3390/ijgi5050055">https://doi.org/10.3390/ijgi5050055</a>.
</div>
<div id="ref-speriosu2013" class="csl-entry" role="listitem">
Speriosu, Michael, and Jason Baldridge. 2013. <span>“Text-<span>Driven Toponym Resolution</span> Using <span>Indirect Supervision</span>,”</span> 10.
</div>
<div id="ref-tjongkimsang2003" class="csl-entry" role="listitem">
Tjong Kim Sang, Erik F., and Fien De Meulder. 2003. <span>“Introduction to the <span>CoNLL-2003 Shared Task</span>: <span>Language-Independent Named Entity Recognition</span>.”</span> In <em>Proceedings of the <span>Seventh Conference</span> on <span>Natural Language Learning</span> at <span>HLT-NAACL</span> 2003</em>, 142–47. <a href="https://doi.org/10.3115/1119176.1119195">https://doi.org/10.3115/1119176.1119195</a>.
</div>
<div id="ref-twaroch2019" class="csl-entry" role="listitem">
Twaroch, Florian A., Paul Brindley, Paul D. Clough, Christopher B. Jones, Robert C. Pasley, and Sue Mansbridge. 2019. <span>“Investigating Behavioural and Computational Approaches for Defining Imprecise Regions.”</span> <em>Spatial Cognition &amp; Computation</em> 19 (2): 146–71. <a href="https://doi.org/10.1080/13875868.2018.1531871">https://doi.org/10.1080/13875868.2018.1531871</a>.
</div>
<div id="ref-vaswani2017" class="csl-entry" role="listitem">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. <span>“Attention <span>Is All You Need</span>.”</span> <em>arXiv:1706.03762 [Cs]</em>, December. <a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a>.
</div>
<div id="ref-wallgrun2018" class="csl-entry" role="listitem">
Wallgrün, Jan Oliver, Morteza Karimzadeh, Alan M. MacEachren, and Scott Pezanowski. 2018. <span>“<span>GeoCorpora</span>: Building a Corpus to Test and Train Microblog Geoparsers.”</span> <em>International Journal of Geographical Information Science</em> 32 (1): 1–29. <a href="https://doi.org/10.1080/13658816.2017.1368523">https://doi.org/10.1080/13658816.2017.1368523</a>.
</div>
<div id="ref-weischedelralph2013" class="csl-entry" role="listitem">
Weischedel, Ralph, Martha Palmer, Mitchell Marcus, Eduard Hovy, Sameer Pradhan, Lance Ramshaw, Nianwen Xue, et al. 2013. <span>“<span>OntoNotes Release</span> 5.0.”</span> <span>Linguistic Data Consortium</span>. <a href="https://doi.org/10.35111/XMHB-2B84">https://doi.org/10.35111/XMHB-2B84</a>.
</div>
<div id="ref-weissenbacher2019" class="csl-entry" role="listitem">
Weissenbacher, Davy, Arjun Magge, Karen O’Connor, Matthew Scotch, and Graciela Gonzalez-Hernandez. 2019. <span>“<span>SemEval-2019 Task</span> 12: <span>Toponym Resolution</span> in <span>Scientific Papers</span>.”</span> In <em>Proceedings of the 13th <span>International Workshop</span> on <span>Semantic Evaluation</span></em>, 907–16. <span>Minneapolis, Minnesota, USA</span>: <span>Association for Computational Linguistics</span>. <a href="https://doi.org/10.18653/v1/s19-2155">https://doi.org/10.18653/v1/s19-2155</a>.
</div>
<div id="ref-wolf2020" class="csl-entry" role="listitem">
Wolf, Thomas, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, et al. 2020. <span>“Transformers: <span class="nocase">State-of-the-art</span> Natural Language Processing.”</span> In <em>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: <span>System</span> Demonstrations</em>, 38–45. <span>Online</span>: <span>Association for Computational Linguistics</span>.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>CoNLL03: <a href="https://www.clips.uantwerpen.be/conll2003/ner/" class="uri">https://www.clips.uantwerpen.be/conll2003/ner/</a>, OntoNotes 5: <a href="https://catalog.ldc.upenn.edu/LDC2013T19" class="uri">https://catalog.ldc.upenn.edu/LDC2013T19</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p><a href="https://github.com/openeventdata/mordecai" class="uri">https://github.com/openeventdata/mordecai</a><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>https://www.geonames.org/8714914/m1-motorway.html<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Compared with the Reuters corpus used for CoNLL03 for example<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>