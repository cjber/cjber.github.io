<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving
and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">

<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">

<front>


<article-meta>


<title-group>
<article-title>Transformer based named entity recognition for place name
extraction from unstructured text</article-title>
<subtitle>{{}} {{}}</subtitle>
</title-group>

<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Berragan</surname>
<given-names>Cillian</given-names>
</name>
<string-name>Cillian Berragan</string-name>

<email>c.berragan@liverpool.ac.uk</email>
<xref ref-type="aff" rid="aff-1">a</xref>
<xref ref-type="corresp" rid="cor-1">&#x002A;</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Singleton</surname>
<given-names>Alex</given-names>
</name>
<string-name>Alex Singleton</string-name>

<email>alex.singleton@liverpool.ac.uk</email>
<xref ref-type="aff" rid="aff-1">a</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Calafiore</surname>
<given-names>Alessia</given-names>
</name>
<string-name>Alessia Calafiore</string-name>

<email>acalafio@ed.ac.uk</email>
<xref ref-type="aff" rid="aff-2">b</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Morley</surname>
<given-names>Jeremy</given-names>
</name>
<string-name>Jeremy Morley</string-name>

<email>Jeremy.Morley@os.uk</email>
<xref ref-type="aff" rid="aff-3">c</xref>
</contrib>
</contrib-group>
<aff id="aff-1">
<institution-wrap>
<institution>University of Liverpool</institution>
</institution-wrap>







</aff>
<aff id="aff-2">
<institution-wrap>
<institution>University of Edinburgh</institution>
</institution-wrap>







</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Ordnance Survey</institution>
</institution-wrap>







</aff>
<author-notes>
<corresp id="cor-1">c.berragan@liverpool.ac.uk</corresp>
</author-notes>









<history></history>


<abstract>
<p>Place names embedded in online natural language text present a useful
source of geographic information. Despite this, many methods for the
extraction of place names from text use pre-trained models that were not
explicitly designed for this task. Our paper builds five custom-built
Named Entity Recognition (NER) models and evaluates them against three
popular pre-built models for place name extraction. The models are
evaluated using a set of manually annotated Wikipedia articles with
reference to the F1 score metric. Our best performing model achieves an
F1 score of 0.939 compared with 0.730 for the best performing pre-built
model. Our model is then used to extract all place names from Wikipedia
articles in Great Britain, demonstrating the ability to more accurately
capture unknown place names from volunteered sources of online
geographic information.</p>
</abstract>
<kwd-group kwd-group-type="author">
<kwd>named entity recognition</kwd>
<kwd>volunteered geographic information</kwd>
<kwd>natural language processing</kwd>
<kwd>place name extraction</kwd>
</kwd-group>




</article-meta>

</front>

<body>
<sec id="introduction">
  <title>Introduction</title>
  <p>Place names are frequently encountered in natural language and
  provide an additional geographic dimension to much of the textual
  information present online, when associated with spatial coordinates
  and geographic locations. Despite this, research in place name
  extraction primarily concentrates on entities as described by
  annotation schemes that do not explicitly consider geographic place
  names
  (<xref alt="Karimzadeh et al. 2019" rid="ref-karimzadeh2019" ref-type="bibr">Karimzadeh
  et al. 2019</xref>;
  <xref alt="Halterman 2017" rid="ref-halterman2017" ref-type="bibr">Halterman
  2017</xref>;
  <xref alt="Hu, Mao, and McKenzie 2019" rid="ref-hu2019" ref-type="bibr">Hu,
  Mao, and McKenzie 2019</xref>). Pre-built named entity recognition
  (NER) models based on these schemes are also not task specific;
  trained on data unrelated to the task they are used for, despite
  language involving place names varying significantly depending on the
  context
  (<xref alt="Purves et al. 2018" rid="ref-purves2018" ref-type="bibr">Purves
  et al. 2018</xref>). When identifying place names in text, research
  typically only considers known administrative names and their
  associated strict boundaries, despite natural language often
  containing place names that either do not exist formally, are
  hyper-localised e.g.¬†street names, or are alternative names that may
  be absent from administrative databases, which often only consider a
  single formal name.</p>
  <p>The training corpora used by pre-built NER models typically
  identifies a number of entities that have no relevance to geographic
  place names, e.g.¬†persons, and those that have some relevance in
  specific contexts; locations, geopolitical entities or facilities
  (<xref alt="Weischedel et al. 2013" rid="ref-weischedelralph2013" ref-type="bibr">Weischedel
  et al. 2013</xref>;
  <xref alt="Tjong Kim Sang and De Meulder 2003" rid="ref-tjongkimsang2003" ref-type="bibr">Tjong
  Kim Sang and De Meulder 2003</xref>). Notably, they do not
  specifically target a ‚Äòplace name‚Äô entity, meaning, while often these
  three related entity types may often refer to a place name, this is
  not always the case. Additionally, these corpora consist of text that
  often differs in structure, compared with the text being processed by
  models trained using them; for example, social media text is typically
  more informal compared with the news articles used to build the
  popular dataset, CoNLL03
  (<xref alt="Tjong Kim Sang and De Meulder 2003" rid="ref-tjongkimsang2003" ref-type="bibr">Tjong
  Kim Sang and De Meulder 2003</xref>).</p>
  <p>New forms of geographic information online present an opportunity
  to train and evaluate models on texts that contain a large volume of
  place names
  (<xref alt="Goodchild and Li 2011" rid="ref-goodchild2011" ref-type="bibr">Goodchild
  and Li 2011</xref>), building models from the ground up, and using
  annotation schemes that are explicitly designed for the extraction of
  place names from text. Results from these models are expected to
  outperform existing pre-built models which use unrelated training
  data, and do not include a ‚Äòplace name‚Äô entity type.</p>
  <p>Our paper presents five NER models, trained on manually labelled
  Wikipedia data and used to identify and extract any span of text
  considered to be a place name, from articles relating to geographic
  locations in the United Kingdom. Our model is evaluated against
  pre-built solutions that are commonly used for this task,
  demonstrating the importance of model training with task specific
  data, and the consideration that named entity recognition as a task is
  not appropriate for place name extraction, due to the exclusion of a
  ‚Äòplace name‚Äô entity type, and the inclusion of a number of unrelated
  entities. New developments in natural language processing (NLP) are
  utilised, outlining the benefit of selecting modern architectures that
  are not yet implemented by off the shelf models. Our paper considers
  the ability to extract place names from Wikipedia articles for the
  United Kingdom that do not appear in the GeoNames Gazetteer, with the
  goal of identifying the additional geographic information that may be
  effectively extracted from unstructured sources of online text.</p>
  <p><xref alt="Section¬†2" rid="sec-trans-review">Section¬†2</xref>
  outlines the research and concepts associated with geography in NLP,
  considering its relation to the new forms of geographic data present
  online, the techniques in natural language processing that explicitly
  deal with geography, and the developments in NLP that have enabled
  higher accuracy with limited labelled data.
  <xref alt="Section¬†3" rid="sec-trans-methodology">Section¬†3</xref>
  presents the workflow undertaken for the models constructed in this
  paper, as well as the data collection and analysis of the entities
  extracted. The performance of each NER model is then presented in
  <xref alt="Section¬†4" rid="sec-trans-results-discussion">Section¬†4</xref>
  and evaluated against pre-built solutions using a corpus of labelled
  test data. Place names are extracted using the model for the entire
  Wikipedia corpus, and compared against GeoNames, identifying names
  that are not present, discussing the reasons they may be found within
  Wikipedia articles, but not in an explicitly geographic gazetteer.</p>
</sec>
<sec id="sec-trans-review">
  <title>Literature review</title>
  <p>Natural language often describes places using imprecise referents,
  non-administrative names, and an understanding of place footprints
  that does not conform with the formal administrative boundaries given
  to them
  (<xref alt="Gao, Janowicz, and Couclelis 2017" rid="ref-gao2017" ref-type="bibr">Gao,
  Janowicz, and Couclelis 2017</xref>;
  <xref alt="Goodchild and Li 2011" rid="ref-goodchild2011" ref-type="bibr">Goodchild
  and Li 2011</xref>). Despite this, regions and place names in
  computational geography are usually formally defined by administrative
  datasets, meaning any informal place names are unable to be
  identified, or associated with a position in space. This distinction
  has given rise to a focus on <italic>place</italic> based GIS, rather
  than <italic>space</italic> based, which considers the ability to
  capture place references that may not appear in administrative
  datasets
  (<xref alt="Gao et al. 2013" rid="ref-gao2013" ref-type="bibr">Gao et
  al. 2013</xref>).</p>
  <p>Since the advent of Web 2.0, increased access to mobile devices
  which include passive GPS and open-access mapping information, several
  scientific disciplines have developed to take advantage of the data
  being produced, including crowdsourcing, and user-generated content
  (<xref alt="See et al. 2016" rid="ref-see2016" ref-type="bibr">See et
  al. 2016</xref>). With geographically referenced content through
  social media, mapping platforms and Wikipedia there is now a wealth of
  information that Goodchild
  (<xref alt="2007" rid="ref-goodchild2007" ref-type="bibr">2007</xref>)
  terms <italic>‚ÄòVolunteered Geographic Information‚Äô</italic> (VGI).
  These data sources present a large collection of continually updated
  references to places, often providing informal and unstructured
  geographic information.</p>
  <p>Much of the past work using VGI has concentrated either on
  explicitly geographic crowd-sourced mapping platforms like Open Street
  Map
  (<xref alt="Antoniou, Morley, and Haklay 2010" rid="ref-antoniou2010" ref-type="bibr">Antoniou,
  Morley, and Haklay 2010</xref>), or ‚Äògeotagged‚Äô content which enables,
  often passively contributed, user-generated data through sites like
  Twitter or Flickr, used to extract geographic information. Gao,
  Janowicz, and Couclelis
  (<xref alt="2017" rid="ref-gao2017" ref-type="bibr">2017</xref>) for
  example present an approach for the construction of cognitive regions
  from various VGI sources, querying place names found in tags with
  associated geotags to create vague boundaries. A similar approach is
  taken by Hollenstein and Purves
  (<xref alt="2010" rid="ref-hollenstein2010" ref-type="bibr">2010</xref>)
  who identified tags containing vague spatial concepts like ‚Äòdowntown‚Äô
  and ‚Äòcitycentre‚Äô, deriving regions from geotags. These methods
  demonstrate the ability to derive informal geographic information from
  VGI, while giving similar results to that of manually collected
  questionnaire data
  (<xref alt="Twaroch et al. 2019" rid="ref-twaroch2019" ref-type="bibr">Twaroch
  et al. 2019</xref>;
  <xref alt="Gao, Janowicz, and Couclelis 2017" rid="ref-gao2017" ref-type="bibr">Gao,
  Janowicz, and Couclelis 2017</xref>).</p>
  <p>While this work concentrates solely on the use of geotags and short
  single phrase tags associated with social media documents to analyse
  ‚Äòplace‚Äô focussed geographies, another source of online information
  that is less frequently considered to have geographic properties is
  unstructured text, which has the potential to provide an even larger
  source of geographically focussed information. Good results have been
  reported using basic semantic rules to identify places names found in
  unstructured text
  (<xref alt="Moncla et al. 2014" rid="ref-moncla2014" ref-type="bibr">Moncla
  et al. 2014</xref>), however, these methods have relied on this text
  almost solely containing place names as entities. Alternatively to
  rule-based approaches, Hu, Mao, and McKenzie
  (<xref alt="2019" rid="ref-hu2019" ref-type="bibr">2019</xref>)
  demonstrate the use of four pre-trained NER models to extract local,
  informal place names from housing advertisements descriptions with
  associated coordinates, to enrich existing gazetteers with place names
  not normally present, alongside derived boundaries. The results of
  this paper show the promising ability for NER models to extract
  informal place names directly from text, also demonstrating a
  bottom-up approach to gazetteer construction, enabling informal place
  definitions to be captured from VGI, that may be absent from
  administrative datasets. Model evaluation however showed low precision
  and recall when evaluating against a labelled dataset, reflecting
  issues with the use of pre-built NER models for this task. Similar
  evaluation results are observed by Karimzadeh et al.
  (<xref alt="2019" rid="ref-karimzadeh2019" ref-type="bibr">2019</xref>)
  when considering various pre-built NER models for use in the GeoTxt
  geoparsing system, which uses either SpaCy or Stanza pre-built models
  (<xref alt="Qi et al. 2018" rid="ref-qi2018" ref-type="bibr">Qi et al.
  2018</xref>;
  <xref alt="Honnibal and Montani 2017" rid="ref-honnibal2017" ref-type="bibr">Honnibal
  and Montani 2017</xref>). While the precision of these pre-built NER
  models can be relatively high for more sophisticated models, they all
  suffer from low recall. Karimzadeh et al.
  (<xref alt="2019" rid="ref-karimzadeh2019" ref-type="bibr">2019</xref>)
  note particularly that while improved results would be expected by
  training a model from the ground up, the amount of labelled training
  data required to create a suitable model would be very large. To
  improve the accuracy of systems that rely on place name extraction,
  NER models should be constructed with more suitable training data, and
  with annotations tailored for this specific task.</p>
  <p>While large, open-access, text-based sources of semantic geographic
  information are scarce, Wikipedia provides a large collection of
  articles about almost any subject, many of which relate to geographic
  locations. This presents an alternative data source for use in
  geographically focussed NLP applications, with place names, their
  semantic context, and article geotags providing geographic
  information. Various studies have used Wikipedia as a data source for
  the extraction of place names, DeLozier, Baldridge, and London
  (<xref alt="2015" rid="ref-delozier2015" ref-type="bibr">2015</xref>)
  for example, identify place names in Wikipedia articles and use a
  clustering technique using document contexts to disambiguate their
  geographic locations. Speriosu and Baldridge
  (<xref alt="2013" rid="ref-speriosu2013" ref-type="bibr">2013</xref>)
  use geotagged Wikipedia articles to provide contextual information
  regarding a range of place names for disambiguation. Both these works
  first use a pre-built Named Entity Recognition (NER) model to identify
  place names found in text, before further analysis. Improvements made
  to these NER models for place name extraction present a stronger
  foundation, leading to both better recall, and precision of place
  names being identified, before they are resolved to coordinates
  (<xref alt="Leidner 2008" rid="ref-leidner2008" ref-type="bibr">Leidner
  2008</xref>;
  <xref alt="Purves et al. 2018" rid="ref-purves2018" ref-type="bibr">Purves
  et al. 2018</xref>). Our paper selects Wikipedia articles to
  demonstrate the geographic information that may be extracted from
  unstructured text, presenting a first-stage baseline approach for
  tasks that rely on accurate place name extraction.</p>
  <sec id="named-entity-recognition-in-the-geographic-domain">
    <title>Named entity recognition in the geographic domain</title>
    <p>Natural language processing techniques involving geography
    typically focus around geoparsing; the automated extraction of place
    names from text, followed by the resolution of the identified place
    names to geographic coordinates
    (<xref alt="Gritta, Pilehvar, and Collier 2020" rid="ref-gritta2020" ref-type="bibr">Gritta,
    Pilehvar, and Collier 2020</xref>;
    <xref alt="Leidner 2008" rid="ref-leidner2008" ref-type="bibr">Leidner
    2008</xref>;
    <xref alt="Buscaldi 2011" rid="ref-buscaldi2011" ref-type="bibr">Buscaldi
    2011</xref>). Modern place name extraction techniques primarily rely
    on named entity recognition (NER) to identify place names as
    entities within text
    (<xref alt="Kumar and Singh 2019" rid="ref-kumar2019" ref-type="bibr">Kumar
    and Singh 2019</xref>;
    <xref alt="Purves et al. 2018" rid="ref-purves2018" ref-type="bibr">Purves
    et al. 2018</xref>). While most pre-built NER systems are able to
    identify ‚Äògeopolitical entities‚Äô and ‚Äòlocations‚Äô as defined by
    popular annotation schemes<xref ref-type="fn" rid="fn1">1</xref>,
    these only act as a proxy for place names in text. The majority of
    entities recognised by these systems are unrelated to place names,
    and as such simply contribute to lower overall recall when other
    entities are preferred by models over geographic place names. For
    example, a model may consider a named organisational headquarters as
    an ‚Äòorganisation‚Äô entity, rather than a ‚Äòlocation‚Äô, even when used
    as a locational reference.</p>
    <p>The concept of a place name as an entity defined by the labelled
    corpora NER models were trained on hinders place name extraction,
    identifying only (and any) administrative place names in text
    (<xref alt="Gritta et al. 2017" rid="ref-gritta2017a" ref-type="bibr">Gritta
    et al. 2017</xref>). The geoparser
    <italic>Mordecai</italic><xref ref-type="fn" rid="fn2">2</xref> for
    example, uses an NER tagger provided through the
    <monospace>SpaCy</monospace> Python library, which provides a
    variety of entities including those unrelated to place names (e.g.¬†:
    persons), and three entities that may be considered related,
    (Geopolitical Entity), (Location), and (Facility). While these
    categories often do relate to place names, they do not consider
    whether the entity could be contextually considered a place name
    that could be geo-located. For example, geopolitical entities are
    often used in a metonymic sense; a figure of speech where a concept
    is substituted by a related concept. In the phrase ‚ÄòMadrid plays
    Kiev today‚Äô for example, sports teams are replaced by their
    associated place name
    (<xref alt="Gritta, Pilehvar, and Collier 2020" rid="ref-gritta2020" ref-type="bibr">Gritta,
    Pilehvar, and Collier 2020</xref>). As place name based metonyms do
    not explicitly relate to geographic locations, and instead a related
    entity, we are uninterested in their extraction. Due to the reliance
    on large labelled corpora for NER training, and limited source of
    geography specific data
    (<xref alt="Karimzadeh et al. 2019" rid="ref-karimzadeh2019" ref-type="bibr">Karimzadeh
    et al. 2019</xref>), little work has considered explicitly targeting
    place names through new data, as it is often time-consuming to
    produce.</p>
    <p>While at present pre-built NER models identify entities as
    defined by widely used annotated corpora, some work has considered
    the need to identify <italic>spatial</italic> entities. SpatialML is
    a natural language annotation scheme that presents the
    <bold>PLACE</bold> tag for any mention of a location
    (<xref alt="Mani et al. 2010" rid="ref-mani2010" ref-type="bibr">Mani
    et al. 2010</xref>). Tasks identified by the
    <ext-link ext-link-type="uri" xlink:href="https://semeval.github.io/">Semantic
    Evaluation Workshop</ext-link> built on this annotation scheme and
    defined several entities relating to spatial language (SemEval-2015
    Task 8: SpaceEval,
    <xref alt="Pustejovsky et al. 2015" rid="ref-pustejovsky2015" ref-type="bibr">Pustejovsky
    et al. 2015</xref>), described by the ISO-Space annotation
    specification
    (<xref alt="Pustejovsky 2017" rid="ref-pustejovsky2017" ref-type="bibr">Pustejovsky
    2017</xref>). In order to more appropriately consider geography when
    parsing unstructured text for place related entities, models should
    be built from the ground up, taking into account an alternative
    annotation scheme that identifies place names, excluding unrelated
    entities.</p>
    <p>Recent progress in NLP and the use of GPU accelerated training
    has brought with it the ability to process large quantities of
    unlabelled text. This development has recently led to the creation
    of general purpose ‚Äòlanguage models‚Äô that implement the
    ‚Äòtransformer‚Äô architecture, using semi-supervised learning to train
    using very large corpora
    (<xref alt="Vaswani et al. 2017" rid="ref-vaswani2017" ref-type="bibr">Vaswani
    et al. 2017</xref>). For example, Google‚Äôs pioneering BERT model was
    trained using the entirety of English Wikipedia, and over 11,000
    books
    (<xref alt="Devlin et al. 2019" rid="ref-devlin2019" ref-type="bibr">Devlin
    et al. 2019</xref>). This development has led to models which
    perform well for many given tasks, even with relatively limited
    additional labelled training data.</p>
    <p>Our paper proposes fine-tuning transformer-based language models
    for place name extraction using named entity recognition, to extract
    all place names from UK ‚Äòplace‚Äô classed articles on Wikipedia. 200
    of these articles are annotated, labelling place names to train and
    evaluate model performance. We train and compare the performance of
    three popular transformer-based NER models; BERT - a large, popular
    transformer model, RoBERTa - similar to BERT, using a different
    pre-training procedure, which has had better results on some tasks,
    and DistilBERT - a much smaller and less complex transformer model
    based on RoBERTa. In addition to these transformer models, two
    simpler Bidirectional LSTM (BiLSTM) models are compared, one using
    pre-trained GloVe embeddings, representing an equivalent complexity
    model used by Stanza or SpaCy pre-built NER solutions, and another
    showing a baseline model without any pre-trained word embeddings.
    These models are then evaluated against three pre-built NER systems
    that are popular for place name extraction, and used in existing
    geoparsing systems including GeoTxt and Mordecai.</p>
  </sec>
</sec>
<sec id="sec-trans-methodology">
  <title>Methodology</title>
  <p><xref alt="Figure¬†1" rid="fig-workflow">Figure¬†1</xref> gives an
  overview of the model and data processing pipeline used in our paper.
  This section first outlines the computational infrastructure used. The
  data collection and data processing is then described, obtaining a
  corpus of Wikipedia articles for locations in Great Britain with place
  names labelled.</p>
  <p>This dataset was then used to train custom NER models of various
  architectures, which were evaluated using separate test data against
  each other and popular pre-built NER models. We then selected our
  DistilBERT transformer model to extract all place names from the full
  corpus of Wikipedia articles, as this model performed well as
  indicated by its test F<sub>1</sub> score, despite its smaller
  size.</p>
  <fig id="fig-workflow">
    <caption><p>Figure¬†1: Overview of the model processing
    pipeline</p></caption>
    <graphic mimetype="application" mime-subtype="pdf" xlink:href="./figs/figure1.pdf" />
  </fig>
  <sec id="software-hardware-infrastructure">
    <title>Software &amp; hardware infrastructure</title>
    <p>Models used in our paper were written in
    <ext-link ext-link-type="uri" xlink:href="https://www.python.org/"><monospace>Python</monospace></ext-link>
    using the
    <ext-link ext-link-type="uri" xlink:href="https://allennlp.org/"><monospace>AllenNLP</monospace></ext-link>
    library for deep learning in natural language processing
    (<xref alt="Gardner et al. 2018" rid="ref-gardner2018" ref-type="bibr">Gardner
    et al. 2018</xref>). <monospace>AllenNLP</monospace> is built on top
    of
    <ext-link ext-link-type="uri" xlink:href="https://pytorch.org"><monospace>PyTorch</monospace></ext-link>
    (<xref alt="Paszke et al. 2019" rid="ref-paszke2019" ref-type="bibr">Paszke
    et al. 2019</xref>), providing abstractions to commonly used
    operations for working with state-of-the-art deep neural networks in
    natural language processing.</p>
    <p>Model training was GPU accelerated using a single NVIDIA GeForce
    RTX 2070 SUPER with 8192MB memory paired with a Ryzen 3700x CPU with
    8 physical and 16 logical cores. <monospace>Python</monospace>
    version 3.8.5 was used with <monospace>AllenNLP</monospace> version
    1.5.0.</p>
  </sec>
  <sec id="annotation-data-collection">
    <title>Annotation &amp; data collection</title>
    <sec id="wikipedia-data-collection">
      <title>Wikipedia data collection</title>
      <p>Wikipedia presents a large collection of well-formatted text
      contributed by a variety of users, with frequent instances of
      place names, a consistent written style and without misspellings.
      Existing NER models are trained on either CoNLL-03 or OntoNotes 5,
      both of which are well-formatted text datasets, consisting
      primarily of news articles. As such, it was considered appropriate
      to select Wikipedia for a comparison between these models and
      ours, compared with other sources of VGI that are of lower overall
      quality.</p>
      <p>The Wikipedia text data used in our paper was accessed through
      <ext-link ext-link-type="uri" xlink:href="https://wiki.dbpedia.org/">DBpedia</ext-link>
      (<xref alt="Auer et al. 2007" rid="ref-auer2007" ref-type="bibr">Auer
      et al. 2007</xref>), a community gathered database of information
      from Wikipedia, presented as an open knowledge graph, with
      ontologies that link and define information in articles. A query
      was built to obtain English Wikipedia abstracts for each DBpedia
      article with the <monospace>Place</monospace> class in Great
      Britain, using the
      <ext-link ext-link-type="uri" xlink:href="http://dbpedia.org/sparql">DBpedia
      SPARQL endpoint</ext-link>. Querying just for
      <monospace>Place</monospace> articles within Great Britain ensured
      that articles extracted contained a large number of place names
      and language indicative of place names, without additional,
      unnecessary information.</p>
      <p>These abstracts are the text provided at the top of each
      article, before any headings, sometimes called the summary. As an
      example, the Wikipedia abstract for
      <ext-link ext-link-type="uri" xlink:href="https://en.wikipedia.org/wiki/Rowlatts_Hill">Rowlatts
      Hill</ext-link>, a suburb of Leicester, UK is as follows, with
      hyperlinks indicated in bold:</p>
      <disp-quote>
        <p>Rowlatts Hill (also known as Rowlatts Hill Estate, or R.H.E.)
        is an eastern, residential suburb of the <bold>English</bold>
        city of <bold>Leicester</bold>. It contains mostly
        <bold>council-owned housing</bold>.</p>
      </disp-quote>
      <disp-quote>
        <p>The suburb is roughly bordered by Spencefield Lane to the
        east and Whitehall Road to the south, which separates it from
        neighbouring <bold>Evington</bold>. A second boundary within the
        estate consists of Coleman Road to Ambassador Road through to
        Green Lane Road; Rowlatts Hill borders <bold>Crown Hills</bold>
        to the west. To the north, at the bottom of Rowlatts Hill is
        Humberstone Park which is located within Green Lane Road,
        Ambassador Road and also leads on to Uppingham Road (the
        <bold>A47</bold>), which is also Rowlatts Hill.</p>
      </disp-quote>
      <p>Using DBpedia enabled a fast executing query which, when
      combined with the <monospace>Place</monospace> class from the
      DBpedia ontology, returned a complete dataset of Wikipedia pages
      for many geographic locations in Great Britain. A total 42,222
      article abstracts were extracted.</p>
    </sec>
    <sec id="input-format">
      <title>Input format</title>
      <p>For use in the models, a random subset of 200 articles were
      annotated using the CoNLL-03 NER format, which uses line
      delimitation to separate tokens, with entities associated with
      each token sharing the same line, separated by a space. Articles
      were first cleaned using regular expressions to remove quotation
      marks, text inside parentheses, and non-ascii characters. The
      <monospace>SpaCy</monospace> large web-based pre-trained model
      pipeline (<monospace>en_core_web_lg</monospace>) was used for
      further processing, using a non-monotonic arc-eager
      transition-system for sentence segmentation
      (<xref alt="Honnibal and Johnson 2015" rid="ref-honnibal2015" ref-type="bibr">Honnibal
      and Johnson 2015</xref>), and tokenisation using a rule-based
      algorithm. Each sentence-length sequence of tokens was treated as
      a separate instance to be fed as batches into models for training.
      Each token in every sequence was annotated as being a place name
      or not, assisted through the open source annotation tool
      <ext-link ext-link-type="uri" xlink:href="https://github.com/doccano/doccano">Doccano</ext-link>
      (<xref alt="Nakayama et al. 2018" rid="ref-nakayama2018" ref-type="bibr">Nakayama
      et al. 2018</xref>).</p>
      <p>For place names that span multiple tokens, the
      <italic>BIOUL</italic> tagging scheme was used, which stands for
      the ‚Äò<italic><bold>B</bold>eginning</italic>,
      <italic><bold>I</bold>nside</italic> and
      <italic><bold>L</bold>ast</italic> tokens of multi-token chunks‚Äô;
      for place names that span more than one token
      (e.g.¬†<italic>B-Place</italic>: New, <italic>L-Place</italic>:
      York). ‚Äò<italic><bold>U</bold>nit-length</italic> chunks and
      <italic><bold>O</bold>utside</italic>‚Äô, place names of only a
      single token, and outside for any token that isn‚Äôt a place name.
      This scheme was used over the simpler BIO scheme which is more
      difficult for models to learn
      (<xref alt="Ratinov and Roth 2009" rid="ref-ratinov2009" ref-type="bibr">Ratinov
      and Roth 2009</xref>). During annotation it became clear that the
      length of certain multi-token place names could be considered
      ambiguous. For example, it may not be clear when a cardinal
      direction is part of a place name, ‚Äònorthern Ireland‚Äô may refer to
      a northern region in Ireland, while ‚ÄòNorthern Ireland‚Äô refers to
      the constituent country in the United Kingdom. To unify labelling
      decisions we chose to consider capitalisation as an indication of
      multi-token noun phrases that constituted a single place name. The
      following sentence shows a sequence of tokens with their
      corresponding tags, demonstrating the annotation scheme with
      <italic>BIOUL</italic> information prepending each tag:</p>
      <p>From these 200 labelled Wikipedia abstracts, 10% were kept for
      both validation and testing, leading to a training set of 21,080
      labelled tokens, a validation dataset of 2,907 labelled tokens,
      and a testing dataset of 3,347 labelled tokens.</p>
    </sec>
  </sec>
  <sec id="building-the-entity-recognition-models">
    <title>Building the entity recognition models</title>
    <p>Named entity recognition is a subset of token classification
    where a sequence of tokens <inline-formula><alternatives>
    <tex-math><![CDATA[\mathbf{x} = \{x_{0}, x_{1}\dots x_{n}\}]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>ùê±</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false" form="prefix">{</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>‚Ä¶</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false" form="postfix">}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
    are taken as input, and the most likely sequence tags
    <inline-formula><alternatives>
    <tex-math><![CDATA[\mathbf{y} = \{y_0, y_1, \dots y_n\}]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>ùê≤</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false" form="prefix">{</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>‚Ä¶</mml:mi><mml:msub><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false" form="postfix">}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
    are predicted. The models constructed in our paper may be divided
    into three main components, outlined on
    <xref alt="Figure¬†1" rid="fig-workflow">Figure¬†1</xref>:</p>
    <list list-type="bullet">
      <list-item>
        <p><bold>Embedding Layer</bold>: Each token in a sequence
        represented as high dimension numerical space, they may be
        either:</p>
        <list list-type="bullet">
          <list-item>
            <p>Randomly initialised</p>
          </list-item>
          <list-item>
            <p>Pre-trained: GloVe, transformer</p>
          </list-item>
        </list>
      </list-item>
      <list-item>
        <p><bold>Intermediate Layers:</bold> A deep neural network that
        input embeddings propagate through, either:</p>
        <list list-type="bullet">
          <list-item>
            <p>Bidirectional LSTM</p>
          </list-item>
          <list-item>
            <p>Transformer</p>
          </list-item>
        </list>
      </list-item>
      <list-item>
        <p><bold>Classification layer:</bold> The final layer of the
        model that takes a high dimensional output from the previous
        layers, and projects them to the classification dimension. The
        <monospace>argmax</monospace> from this layer corresponds to the
        label selected for each token. Each model uses a Conditional
        Random Field (CRF) to classify tokens which are popular in NER
        tasks, as they consider tagging decisions between all input
        tokens
        (<xref alt="Lample et al. 2016" rid="ref-lample2016" ref-type="bibr">Lample
        et al. 2016</xref>). This is necessary given the inside tag for
        a place (I-PLACE), cannot directly follow a unit tag (U-PLACE)
        for example.</p>
      </list-item>
    </list>
    <fig id="tbl-models">
      <caption><p>Table¬†1: Overview of the models trained through our
      paper, detailing the architecture used. Integers in brackets
      indicate the vector dimensions</p></caption>
    </fig>
    <p><xref alt="Table¬†1" rid="tbl-models">Table¬†1</xref> gives an
    overview of the model architectures built through our paper. First a
    simplistic model was constructed as a baseline, using untrained
    randomly initialised 50 dimension token embeddings, fed into a
    two-layer Bidirectional LSTM (BiLSTM) with 200 hidden dimensions.
    The output from the BiLSTM was input into a conditional random field
    classifier. A second BiLSTM model was also created based on the
    architecture described in Peters et al.
    (<xref alt="2018" rid="ref-peters2018" ref-type="bibr">2018</xref>),
    adding pre-trained GloVe token embeddings
    (<xref alt="Pennington, Socher, and Manning 2014" rid="ref-pennington2014" ref-type="bibr">Pennington,
    Socher, and Manning 2014</xref>) with 50 dimensions and 16 dimension
    character embeddings. Both models used the
    <monospace>Adam</monospace> optimiser which makes use of stochastic
    gradient descent for weight optimisation
    (<xref alt="Kingma and Ba 2017" rid="ref-kingma2017" ref-type="bibr">Kingma
    and Ba 2017</xref>).</p>
    <p>Three BERT-based transformer models were also created, using BERT
    (<xref alt="Devlin et al. 2019" rid="ref-devlin2019" ref-type="bibr">Devlin
    et al. 2019</xref>), RoBERTa which attempts to optimise the training
    process of BERT
    (<xref alt="Liu et al. 2019" rid="ref-liu2019" ref-type="bibr">Liu
    et al. 2019</xref>), and DistilBERT, which distils the data used in
    pre-training to create a smaller, faster model
    (<xref alt="Sanh et al. 2020" rid="ref-sanh2020" ref-type="bibr">Sanh
    et al. 2020</xref>). The primary architecture of transformers is
    ‚Äòattention‚Äô which enables them to consider and weight each word in a
    sequence against each other word simultaneously. This allows them to
    be highly parallel, providing significant improvements to
    computational speed with GPUs which can handle highly parallel
    tasks, and benefits over traditional architectures like Long
    Short-Term Memory (LSTM) which are only able to consider sequences
    sequentially
    (<xref alt="Vaswani et al. 2017" rid="ref-vaswani2017" ref-type="bibr">Vaswani
    et al. 2017</xref>). These models were pre-trained on very large
    general text corpora, enabling ‚Äòtransfer learning‚Äô, where a
    pre-trained model like BERT is used as a base and fine-tuned to be
    task specific. Conceptually, these pre-trained models learn deep
    embedded weights for words based on comprehensive contextual
    information extracted from the large general text corpora, these
    then only require smaller adjustments in fine-tuning to achieve good
    task-specific results. Fine-tuning these pre-trained models in NLP
    has produced results that often outperform models using traditional
    architectures that include manually trained word embeddings
    (Word2Vec,
    <xref alt="Mikolov et al. 2013" rid="ref-mikolov2013" ref-type="bibr">Mikolov
    et al. 2013</xref>), which are limited by the volume of data
    provided to them and pre-trained embeddings like GloVe
    (<xref alt="Pennington, Socher, and Manning 2014" rid="ref-pennington2014" ref-type="bibr">Pennington,
    Socher, and Manning 2014</xref>).</p>
    <p>Pre-trained transformer models replace both the BiLSTM layers of
    the previous models and token embeddings, taking encoded sequences,
    associating each token with a 768 dimension vector representation
    from a vocabulary, feeding them into sequential transformer layers
    and outputting into a CRF classifier. Each model was initialised
    with pre-trained weights provided by the
    <monospace>transformers</monospace> Python library
    (<xref alt="Wolf et al. 2020" rid="ref-wolf2020" ref-type="bibr">Wolf
    et al. 2020</xref>), these weights are initialised in both the
    embedding layers and intermediate layers. For weight optimisation,
    these models used the weight decay Adam algorithm
    (<monospace>AdamW</monospace>,
    <xref alt="Loshchilov and Hutter 2019" rid="ref-loshchilov2019" ref-type="bibr">Loshchilov
    and Hutter 2019</xref>). Every layer of the transformer models was
    updated during training, which enabled the pre-trained weights to
    adjust and learn for the specific task. Hyper-parameters selected
    for each model were largely based on the values as suggested for
    token classification by their respective implementation papers.</p>
    <p>For every model, weights were adjusted each epoch to minimise the
    training loss. Following the final intermediate layer of a model, a
    token representation <inline-formula><alternatives>
    <tex-math><![CDATA[C\in\mathbb{R}^H]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>C</mml:mi><mml:mo>‚àà</mml:mo><mml:msup><mml:mi>‚Ñù</mml:mi><mml:mi>H</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>
    feeds into the classification layer weights
    <inline-formula><alternatives>
    <tex-math><![CDATA[W\in\mathbb{R}^{K\times H}]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>W</mml:mi><mml:mo>‚àà</mml:mo><mml:msup><mml:mi>‚Ñù</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>√ó</mml:mo><mml:mi>H</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>,
    where <inline-formula><alternatives>
    <tex-math><![CDATA[K]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>K</mml:mi></mml:math></alternatives></inline-formula>
    is the number of unique labels. Classification loss is then
    calculated using <inline-formula><alternatives>
    <tex-math><![CDATA[log(softmax(CW^T))]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>C</mml:mi><mml:msup><mml:mi>W</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>.</p>
    <p>Early stopping was used in each model, stopping training early if
    no improvement was made to the validation F<sub>1</sub> score in
    eight subsequent epochs. Automatic Mixed Precision (AMP) was used
    throughout training to use half-precision (16 bit) floating point
    numbers in some operations which reduced the memory overhead and
    increased computation speed. For transformers, the learning rate was
    optimised towards the end of training, using a
    <monospace>reduce on plateau</monospace> learning rate scheduler,
    reducing the learning rate by 1/10th once the overall F<sub>1</sub>
    validation metric had stopped improving after two epochs, this only
    increased training time on the BiLSTM models with no improvement, so
    was excluded. Following training, the weights from the best
    performing epoch were automatically chosen for the final model.</p>
  </sec>
  <sec id="evaluation-against-pre-built-models">
    <title>Evaluation against pre-built models</title>
    <p>Following the training of each model, their accuracy, precision,
    recall and F<sub>1</sub> score was evaluated using a corpus of test
    data, against three popular modern pre-built NER models provided
    through the <monospace>SpaCy</monospace> and
    <monospace>Stanza</monospace> Python packages. A
    <monospace>SpaCy</monospace> model is used in the
    <italic>Mordecai</italic> geoparser and optionally in the
    <italic>GeoTxt</italic> geoparser, while the
    <monospace>Stanza</monospace> model is a more recent implementation
    of the Stanford NLP model used by the <italic>GeoTxt</italic>
    geoparser.</p>
    <p>As these pre-built models were not trained to recognise ‚Äòplace
    names‚Äô, their tags were adjusted so that anything labelled as ‚ÄòGPE‚Äô
    (Geopolitical Entity), ‚ÄòLOC‚Äô (Location), or ‚ÄòFAC‚Äô (facility) was
    considered to be a ‚Äòplace name‚Äô, mirroring the process used to
    discard unrelated entities by geoparsing systems that use these
    models. The default <monospace>Stanza</monospace> NER model, and two
    <monospace>SpaCy</monospace> models
    (<monospace>en_core_web_sm</monospace>,
    <monospace>en_core_web_lg</monospace>) were evaluated on the
    labelled test data.
    <xref alt="Table¬†2" rid="tbl-prebuilt">Table¬†2</xref> gives an
    overview of these pre-built models.</p>
    <p>Each model was evaluated on 3 separate subsets of the annotated
    test dataset, giving a range of scores for each model. Significance
    testing was then performed using paired t-tests to test the null
    hypothesis:</p>
    <disp-quote>
      <p><inline-formula><alternatives>
      <tex-math><![CDATA[\mathbf{H_0}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>ùêá</mml:mi><mml:mn>ùüé</mml:mn></mml:msub></mml:math></alternatives></inline-formula>:
      There will be no statistically significant difference between the
      mean F<sub>1</sub> score of each custom built model against the
      best performing pre-built model
      (<monospace>Stanza</monospace>).</p>
    </disp-quote>
    <p>Significant results that reject this null hypothesis were
    indicated by <inline-formula><alternatives>
    <tex-math><![CDATA[p<0.05]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
    and are shown on
    <xref alt="Table¬†3" rid="tbl-eval">Table¬†3</xref>.</p>
    <p>The best performing model trained on the annotated Wikipedia data
    was also evaluated using paired t-tests against each other model
    trained on the same data, to test the null hypothesis:</p>
    <disp-quote>
      <p><inline-formula><alternatives>
      <tex-math><![CDATA[\mathbf{H_0}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>ùêá</mml:mi><mml:mn>ùüé</mml:mn></mml:msub></mml:math></alternatives></inline-formula>:
      There will be no statistically significant difference between the
      mean F<sub>1</sub> score of the best performing custom built model
      trained on annotated Wikipedia data and each other model trained
      on this data.</p>
    </disp-quote>
    <p>Significant results that reject this null hypothesis were also
    indicated by <inline-formula><alternatives>
    <tex-math><![CDATA[p<0.05]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>.</p>
    <p>It should be noted that significance testing is not common in
    deep learning research
    (<xref alt="Dror and Reichart 2018" rid="ref-dror2018a" ref-type="bibr">Dror
    and Reichart 2018</xref>), but papers that do report the
    significance of mean scores between models tend to use paired
    t-tests, despite potentially violating the parametric assumptions
    made. Dror and Reichart
    (<xref alt="2018" rid="ref-dror2018a" ref-type="bibr">2018</xref>)
    suggest that while normality may be assumed due to the Central Limit
    Theorem, it is likely that future progress in this field will
    present more appropriate statistical significance testing.</p>
    <fig id="tbl-prebuilt">
      <caption><p>Table¬†2: Pre-built NER models</p></caption>
    </fig>
  </sec>
  <sec id="output-processing">
    <title>Output processing</title>
    <p>A predictor was created from the DistilBERT model to run
    inference over the total corpus of Wikipedia articles. Place names
    extracted from the Wikipedia articles by this model were saved to a
    CSV file with the context sentence, the associated article, and
    coordinate information for the article that contained the place.</p>
    <p>Place names were compared against a full corpus of British place
    names from the GeoNames gazetteer, to examine which names are
    excluded from the gazetteer, but identified within Wikipedia
    articles.</p>
  </sec>
</sec>
<sec id="sec-trans-results-discussion">
  <title>Results &amp; discussion</title>
  <p>This section first evaluates the results of the models presented
  against each other, and in relation to existing pre-built NER
  solutions. The place names extracted by our best performing model are
  compared with pre-built models, showing how our method improves on
  those used in existing place name extraction methods. Following this,
  examples from the corpus of place names extracted from Wikipedia
  articles are noted, demonstrating use-cases for the method presented
  that wouldn‚Äôt be possible or as effective, through pre-built NER
  solutions.</p>
  <sec id="model-performance">
    <title>Model performance</title>
    <p><xref alt="Table¬†3" rid="tbl-eval">Table¬†3</xref> shows three
    popular pre-built NER models, evaluated on the labelled Wikipedia
    test data, compared with the models produced through our paper. The
    <monospace>BiLSTM-CRF (basic)</monospace> model gives a baseline
    reference for a typical NER model with a simple architecture. Out of
    the pre-built models, <monospace>Stanza</monospace> performs the
    best, achieving precision and accuracy just below the trained
    baseline model, with an F<sub>1</sub> score which isn‚Äôt
    significantly worse (paired t-test <inline-formula><alternatives>
    <tex-math><![CDATA[p>0.05]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>),
    both <monospace>SpaCy</monospace> models however show notably worse
    results compared with <monospace>Stanza</monospace>. The primary
    issue with the pre-built models is recall, which is far below any of
    the custom-built models, reflecting a high number of false
    negatives.</p>
    <fig id="tbl-eval">
      <caption><p>Table¬†3: Geographic entity recognition mean (¬±SD)
      performance metrics over 3 runs of annotated Wikipedia test data
      subsets. Pre-built NER models are shown in italics. Bold values
      indicate statistically significant F1 scores of fine-tuned models
      in relation to <monospace>Stanza</monospace> (Paired t-tests
      <inline-formula><alternatives>
      <tex-math><![CDATA[p<0.05]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>).</p></caption>
    </fig>
    <p>It is worth noting that due to class imbalances, i.e.¬†many more
    ‚Äòother‚Äô (<monospace>O</monospace>) entities relative to the small
    number of <monospace>PLACE</monospace> entities, accuracy should be
    considered a poor metric, and is only included for completeness.
    This class imbalance means that as only approximately 15% of tokens
    are labelled as entities, it is possible to achieve 85% accuracy and
    high precision by labelling all tokens as not entities.
    F<sub>1</sub> score is often used to compensate for these issues in
    multiple classification tasks, but it should be known that it is not
    itself a perfect metric. With respect to the best performing
    pre-built model <monospace>Stanza</monospace>, all transformer
    models fine-tuned on the Wikipedia annotated data, have
    significantly higher F<sub>1</sub> scores (paired t-test
    <inline-formula><alternatives>
    <tex-math><![CDATA[p<0.05]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>).</p>
    <p>The DistilBERT transformer model is less complex than both the
    BERT and RoBERTa model, with a total of 260 MB in model weights,
    compared with 433 MB and 498 MB respectively. Despite this, the
    DistilBERT model achieves similar results to RoBERTa on test data
    (<xref alt="Table¬†3" rid="tbl-eval">Table¬†3</xref>). While all
    transformer models perform significantly better than the best
    performing pre-built model, Stanza, both CRF models do not give
    significantly better F<sub>1</sub> scores (paired t-test
    <inline-formula><alternatives>
    <tex-math><![CDATA[p>0.05]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>).
    BERT performs best overall, with an F<sub>1</sub> score of 0.939 on
    the test data, a result that is only significantly better than the
    two CRF models (paired t-test <inline-formula><alternatives>
    <tex-math><![CDATA[p<0.05)]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.05</mml:mn><mml:mo stretchy="false" form="postfix">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>.</p>
    <p><xref alt="Figure¬†2" rid="fig-qual">Figure¬†2</xref> shows the
    output of the chosen fine-tuned NER model
    <monospace>DistilBERT</monospace> alongside
    <monospace>SpaCy (large)</monospace> and
    <monospace>Stanza</monospace>, applied to a simple Wikipedia article
    summary. <xref alt="Figure¬†2" rid="fig-qual">Figure¬†2</xref> (A)
    gives promising results for <monospace>DistilBERT</monospace>, with
    the summary for the Wikipedia page ‚ÄòRowlatts Hill‚Äô, correctly
    identifying all place names.</p>
    <p>While evaluation metrics indicate that
    <monospace>Stanza</monospace> performs reasonably well, it primarily
    suffers from the annotation scheme used, some place names are
    misidentified as ‚ÄòPerson‚Äô, or ‚ÄòOrganisation‚Äô, meaning a standard
    geoparsing system would miss several place names here, given they
    are not otherwise identifiable
    (<xref alt="Figure¬†2" rid="fig-qual">Figure¬†2</xref>).</p>
    <fig id="fig-qual">
      <caption><p>Figure¬†2: Comparison of outputs between the best
      performing fine-tuned transformer model and the two best
      performing pre-built NER models.</p></caption>
      <graphic mimetype="image" mime-subtype="jpeg" xlink:href="./figs/figure2.jpg" />
    </fig>
    <fig id="fig-qual-china">
      <caption><p>Figure¬†3: Ability for trained model to distinguish
      between metonymic usage of place names.</p></caption>
      <graphic mimetype="image" mime-subtype="jpeg" xlink:href="./figs/figure3.jpg" />
    </fig>
    <p><xref alt="Figure¬†3" rid="fig-qual-china">Figure¬†3</xref>
    demonstrates the ability for our DistilBERT transformer model to
    accurately ignore entities that do not relate to place names. This
    example paragraph only refers to a single geographic location in
    text, the location of the 1952 Summer Games, in Helsinki, Finland.
    While Stanza identifies a large number of GPE tags, they either
    relate to China used in a metynomic sense, meaning the Chinese
    Olympic team (‚ÄòChina competed‚Äô), or as a related geopolitical noun
    (‚Äòdelegation of ROC‚Äô), which is not considered to be a place name
    referring to a geographic location in this context. Our model
    correctly infers the single mention of a geographic place name based
    on the contextual information, meaning a large amount of unrelated
    information is excluded. Particularly, recognising and ignoring
    these nouns related to place names is something that is noted as an
    issue in current geoparsing systems
    (<xref alt="Gritta, Pilehvar, and Collier 2020" rid="ref-gritta2020" ref-type="bibr">Gritta,
    Pilehvar, and Collier 2020</xref>). This figure also demonstrates
    the importance of using a pre-trained model base for this task, as
    the BiLSTM CRF performs poorly. It is likely that this issue stems
    from the limited training data used, as the model is unable to learn
    more complex cases where place names are less obvious
    (<xref alt="Figure¬†3" rid="fig-qual-china">Figure¬†3</xref> (B)).
    Using a pre-trained transformer enables the model to correctly
    identify instances where proper nouns do not relate to place names,
    taking information learned through its pre-training procedure.</p>
  </sec>
  <sec id="identified-place-names-from-wikipedia">
    <title>Identified place names from Wikipedia</title>
    <fig id="tbl-freq">
      <caption><p>Table¬†4: Top and bottom named places by frequency,
      excluding any present in the GeoNames gazetteer or mentioned less
      than 100 times.</p></caption>
    </fig>
    <p><xref alt="Table¬†4" rid="tbl-freq">Table¬†4</xref> gives an
    overview of the most common place names identified by the DistilBERT
    model and the SpaCy model. Notably, the SpaCy model appears to
    struggle with correctly aligning entities, including ‚Äòthe‚Äô with
    ‚ÄòUnited Kingdom‚Äô, and partially missing place names containing
    ‚ÄòTyne‚Äô (e.g.¬†‚ÄòTyne and Wear‚Äô or ‚ÄòRiver Tyne‚Äô). The DistilBERT model
    also extracts around 6 times the number of place names compared with
    SpaCy, reflected by the low recall noted above. One example where
    the DistilBERT model appears confused is by giving the place name
    ‚ÄòChurch of England‚Äô, this problem relates to the language used in
    Wikipedia articles, when churches are described as a ‚ÄòChurch of
    England church‚Äô, a nominal mention of a place rather than
    specific.</p>
    <p>The total number of place names extracted from the Wikipedia
    summaries by the <monospace>DistilBERT</monospace> model was
    614,672, with 99,697 unique place names. In total 62,178 unique
    place names were extracted that are not found within the GeoNames
    gazetteer. These entities primarily exist as granular names
    mentioned in single instances (e.g.¬†road names: Shady Lane,
    Chapeltown Road), organisational names used in a place related
    context (e.g.¬†describing locations along the Great Western Railway
    route), and alternative names that are not captured by GeoNames. For
    example, ‚ÄòM1‚Äô appears in GeoNames as ‚ÄòM1
    Motorway‚Äô<xref ref-type="fn" rid="fn3">3</xref>. While the ‚ÄòM1
    motorway‚Äô is used in Wikipedia articles, it is often also referred
    to as just the ‚ÄòM1‚Äô.</p>
  </sec>
</sec>
<sec id="conclusion">
  <title>Conclusion</title>
  <p>Our paper demonstrates a new approach towards the extraction of
  place names from text by building an NER model using data annotated
  with geographic place names. This work aims to direct geographic NLP
  research towards the use of models which move away from the
  generalisable annotation schemes of pre-built NER solutions, to
  include task-specific, relevant training data. Notably this differs
  from the perceived generalisability of pre-built models used for
  general geoparsing. We believe this is an important approach for
  geographic place name extraction given geographic language differs
  greatly based on context
  (<xref alt="Purves et al. 2018" rid="ref-purves2018" ref-type="bibr">Purves
  et al. 2018</xref>), with contexts varying greatly based on the
  corpora used for inference. This is demonstrated by the poor results
  observed in previous work when applying pre-built NER solutions, which
  use training data unrelated to the task-specific data they are being
  applied to
  (<xref alt="Hu, Mao, and McKenzie 2019" rid="ref-hu2019" ref-type="bibr">Hu,
  Mao, and McKenzie 2019</xref>;
  <xref alt="Karimzadeh et al. 2019" rid="ref-karimzadeh2019" ref-type="bibr">Karimzadeh
  et al. 2019</xref>). Wallgr√ºn et al.
  (<xref alt="2018" rid="ref-wallgrun2018" ref-type="bibr">2018</xref>)
  recognise this problem, developing GeoCorpora, a task-specific
  training dataset for micro-blog geoparsing, notably describing
  increased issues with annotation ambiguity compared with more
  traditional text-sources. Additionally, recent work with transformer
  models, typically only built to be generalisable, have considered
  moving from fully generalised self-supervised training towards more
  dataset-specific models (e.g.¬†TweetEval; Barbieri et al.
  (<xref alt="2020" rid="ref-barbieri2020" ref-type="bibr">2020</xref>)),
  with results that outperform generalisable transformer models
  (<xref alt="Nguyen, Vu, and Nguyen 2020" rid="ref-nguyen2020" ref-type="bibr">Nguyen,
  Vu, and Nguyen 2020</xref>).</p>
  <p>Ultimately, the decision to produce a model explicitly designed to
  be non-generalisable to other corpora may be considered a limitation
  of the scope of this paper. We have demonstrated a best-case scenario
  where time-frames allow for manual annotation of task-specific data.
  Future research may consider the construction of a more generalisable
  place name extraction model, which takes inspiration from the
  alternative annotation scheme employed by our paper, allowing for use
  in general purpose geoparsers.</p>
  <p>Additionally, while our paper selects Wikipedia for place name
  extraction, due to its large volume, ease of validation and data
  retrieval, future work may consider the ability to apply our
  methodology to other text sources. With suitable models constructed,
  using annotated training data that is relevant to the corpus being
  considered, we expect future work applied to other data sources may
  present the opportunity to further contribute to place names that are
  absent from gazetteers, as vernacular place names. We believe that
  given a suitable combination of data sources, our methodology is the
  first step towards the construction gazetteers from the bottom-up,
  directly taking place names from passive contributions, without
  relying on pre-built datasets.</p>
  <p>The recent development of pre-trained language models and their
  suitability for fine-tuning in many tasks, including NER, presents a
  method for the construction of accurate models that are task specific,
  using relatively small labelled
  corpora<xref ref-type="fn" rid="fn4">4</xref> that defines entities
  more suited to the task of place name extraction. The architecture in
  our paper is more simplistic to implement than other attempts at
  similar tasks (e.g.
  <xref alt="Weissenbacher et al. 2019" rid="ref-weissenbacher2019" ref-type="bibr">Weissenbacher
  et al. 2019</xref>), with most of the complexity hidden within the
  transformer layers. This, combined with libraries that abstract and
  implement state of the art models, provides a more accessible approach
  for research in place name extraction, without requiring a deep
  understanding of semantic rules, or the construction of deep
  multi-layered models from the ground up.</p>
  <p>Evaluation against pre-built NER models on
  <xref alt="Table¬†3" rid="tbl-eval">Table¬†3</xref> shows that
  performance for place name extraction is greatly improved,
  particularly with respect to recall, a notable issue with past studies
  (<xref alt="Hu, Mao, and McKenzie 2019" rid="ref-hu2019" ref-type="bibr">Hu,
  Mao, and McKenzie 2019</xref>;
  <xref alt="Karimzadeh et al. 2019" rid="ref-karimzadeh2019" ref-type="bibr">Karimzadeh
  et al. 2019</xref>). The construction of an NER model for the task
  specific extraction of place names moves towards systems that
  appropriately consider the geographic elements present in natural
  language. The large number of place names that are absent from the
  GeoNames gazetteer suggests that geoparsing and related work likely
  misses a substantial amount of geographic information present in text.
  The dataset produced through this work aims to assist with filling
  these gaps, while the methodology described enables an approach that
  may be mirrored and applied to further work on other data sources.</p>
  <p>Finally, both ‚Äòplace‚Äô focussed annotation schemes describe the use
  of ‚Äònominal‚Äô place related entities
  (<xref alt="Mani et al. 2010" rid="ref-mani2010" ref-type="bibr">Mani
  et al. 2010</xref>;
  <xref alt="Pustejovsky 2017" rid="ref-pustejovsky2017" ref-type="bibr">Pustejovsky
  2017</xref>). While out of the scope of our work, we would like to
  encourage the focus on extracting this additional geographic
  information from text. Often in language the use of these non-specific
  terms are used, for example ‚ÄòI visited the shops‚Äô, ‚ÄòYork is a city‚Äô,
  provide geographically specific information. ‚ÄòThe shops‚Äô with enough
  context may provide a specific geographic location, and similarly the
  link between ‚ÄòYork‚Äô -&gt; ‚Äòcity‚Äô could be explored
  (<xref alt="Couclelis 2010" rid="ref-couclelis2010" ref-type="bibr">Couclelis
  2010</xref>).</p>
</sec>
<sec id="data-and-codes-availability-statement">
  <title>Data and codes availability statement</title>
  <p>The data and codes that support the findings of this study are
  available at the public FigShare link
  (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.13415255.v1">https://doi.org/10.6084/m9.figshare.13415255.v1</ext-link>).
  Instructions for using the data and code are provided as a README
  within the FigShare repository.</p>
</sec>
<sec id="disclosure-statement">
  <title>Disclosure statement</title>
  <p>No potential competing interest was reported by the authors.</p>
</sec>
</body>

<back>
<ref-list>
  <title></title>
  <ref id="ref-antoniou2010">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Antoniou</surname><given-names>Vyron</given-names></name>
        <name><surname>Morley</surname><given-names>Jeremy</given-names></name>
        <name><surname>Haklay</surname><given-names>Muki</given-names></name>
      </person-group>
      <article-title>Web 2.0 geotagged photos: Assessing the spatial dimension of the phenomenon</article-title>
      <source>Geomatica</source>
      <year iso-8601-date="2010-01">2010</year><month>01</month>
      <volume>64</volume>
      <fpage>99</fpage>
      <lpage>110</lpage>
    </element-citation>
  </ref>
  <ref id="ref-auer2007">
    <element-citation publication-type="chapter">
      <person-group person-group-type="author">
        <name><surname>Auer</surname><given-names>S√∂ren</given-names></name>
        <name><surname>Bizer</surname><given-names>Christian</given-names></name>
        <name><surname>Kobilarov</surname><given-names>Georgi</given-names></name>
        <name><surname>Lehmann</surname><given-names>Jens</given-names></name>
        <name><surname>Cyganiak</surname><given-names>Richard</given-names></name>
        <name><surname>Ives</surname><given-names>Zachary</given-names></name>
      </person-group>
      <article-title>DBpedia: A Nucleus for a Web of Open Data</article-title>
      <source>The Semantic Web</source>
      <person-group person-group-type="editor">
        <name><surname>Hutchison</surname><given-names>David</given-names></name>
        <name><surname>Kanade</surname><given-names>Takeo</given-names></name>
        <name><surname>Kittler</surname><given-names>Josef</given-names></name>
        <name><surname>Kleinberg</surname><given-names>Jon M.</given-names></name>
        <name><surname>Mattern</surname><given-names>Friedemann</given-names></name>
        <name><surname>Mitchell</surname><given-names>John C.</given-names></name>
        <name><surname>Naor</surname><given-names>Moni</given-names></name>
        <name><surname>Nierstrasz</surname><given-names>Oscar</given-names></name>
        <name><surname>Pandu Rangan</surname><given-names>C.</given-names></name>
        <name><surname>Steffen</surname><given-names>Bernhard</given-names></name>
        <name><surname>Sudan</surname><given-names>Madhu</given-names></name>
        <name><surname>Terzopoulos</surname><given-names>Demetri</given-names></name>
        <name><surname>Tygar</surname><given-names>Doug</given-names></name>
        <name><surname>Vardi</surname><given-names>Moshe Y.</given-names></name>
        <name><surname>Weikum</surname><given-names>Gerhard</given-names></name>
        <name><surname>Aberer</surname><given-names>Karl</given-names></name>
        <name><surname>Choi</surname><given-names>Key-Sun</given-names></name>
        <name><surname>Noy</surname><given-names>Natasha</given-names></name>
        <name><surname>Allemang</surname><given-names>Dean</given-names></name>
        <name><surname>Lee</surname><given-names>Kyung-Il</given-names></name>
        <name><surname>Nixon</surname><given-names>Lyndon</given-names></name>
        <name><surname>Golbeck</surname><given-names>Jennifer</given-names></name>
        <name><surname>Mika</surname><given-names>Peter</given-names></name>
        <name><surname>Maynard</surname><given-names>Diana</given-names></name>
        <name><surname>Mizoguchi</surname><given-names>Riichiro</given-names></name>
        <name><surname>Schreiber</surname><given-names>Guus</given-names></name>
        <name><surname>Cudr√©-Mauroux</surname><given-names>Philippe</given-names></name>
      </person-group>
      <publisher-name>Springer Berlin Heidelberg</publisher-name>
      <publisher-loc>Berlin, Heidelberg</publisher-loc>
      <year iso-8601-date="2007">2007</year>
      <date-in-citation content-type="access-date"><year iso-8601-date="2020-08-26">2020</year><month>08</month><day>26</day></date-in-citation>
      <volume>4825</volume>
      <isbn>978-3-540-76297-3 978-3-540-76298-0</isbn>
      <pub-id pub-id-type="doi">10.1007/978-3-540-76298-0_52</pub-id>
      <fpage>722</fpage>
      <lpage>735</lpage>
    </element-citation>
  </ref>
  <ref id="ref-barbieri2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Barbieri</surname><given-names>Francesco</given-names></name>
        <name><surname>Camacho-Collados</surname><given-names>Jose</given-names></name>
        <name><surname>Neves</surname><given-names>Leonardo</given-names></name>
        <name><surname>Espinosa-Anke</surname><given-names>Luis</given-names></name>
      </person-group>
      <article-title>TweetEval: Unified Benchmark and Comparative Evaluation for Tweet Classification</article-title>
      <source>arXiv:2010.12421 [cs]</source>
      <year iso-8601-date="2020-10">2020</year><month>10</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2021-08-12">2021</year><month>08</month><day>12</day></date-in-citation>
      <uri>https://arxiv.org/abs/2010.12421</uri>
    </element-citation>
  </ref>
  <ref id="ref-buscaldi2011">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Buscaldi</surname><given-names>Davide</given-names></name>
      </person-group>
      <article-title>Approaches to disambiguating toponyms</article-title>
      <source>SIGSPATIAL Special</source>
      <year iso-8601-date="2011-07">2011</year><month>07</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2019-11-28">2019</year><month>11</month><day>28</day></date-in-citation>
      <volume>3</volume>
      <issue>2</issue>
      <issn>19467729</issn>
      <pub-id pub-id-type="doi">10.1145/2047296.2047300</pub-id>
      <fpage>16</fpage>
      <lpage>19</lpage>
    </element-citation>
  </ref>
  <ref id="ref-couclelis2010">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Couclelis</surname><given-names>Helen</given-names></name>
      </person-group>
      <article-title>Ontologies of geographic information</article-title>
      <source>International Journal of Geographical Information Science</source>
      <year iso-8601-date="2010-11">2010</year><month>11</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2022-07-11">2022</year><month>07</month><day>11</day></date-in-citation>
      <volume>24</volume>
      <issue>12</issue>
      <issn>1365-8816, 1362-3087</issn>
      <pub-id pub-id-type="doi">10.1080/13658816.2010.484392</pub-id>
      <fpage>1785</fpage>
      <lpage>1809</lpage>
    </element-citation>
  </ref>
  <ref id="ref-delozier2015">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>DeLozier</surname><given-names>Grant</given-names></name>
        <name><surname>Baldridge</surname><given-names>Jason</given-names></name>
        <name><surname>London</surname><given-names>Loretta</given-names></name>
      </person-group>
      <article-title>Gazetteer-Independent Toponym Resolution Using Geographic Word Profiles</article-title>
      <year iso-8601-date="2015">2015</year>
      <pub-id pub-id-type="doi">10.1609/aaai.v29i1.9531</pub-id>
      <fpage>7</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-devlin2019">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Devlin</surname><given-names>Jacob</given-names></name>
        <name><surname>Chang</surname><given-names>Ming-Wei</given-names></name>
        <name><surname>Lee</surname><given-names>Kenton</given-names></name>
        <name><surname>Toutanova</surname><given-names>Kristina</given-names></name>
      </person-group>
      <article-title>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</article-title>
      <source>arXiv:1810.04805 [cs]</source>
      <year iso-8601-date="2019-05">2019</year><month>05</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2020-02-11">2020</year><month>02</month><day>11</day></date-in-citation>
      <uri>https://arxiv.org/abs/1810.04805</uri>
    </element-citation>
  </ref>
  <ref id="ref-dror2018a">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Dror</surname><given-names>Rotem</given-names></name>
        <name><surname>Reichart</surname><given-names>Roi</given-names></name>
      </person-group>
      <article-title>Appendix - Recommended Statistical Significance Tests for NLP Tasks</article-title>
      <source>arXiv:1809.01448 [cs]</source>
      <year iso-8601-date="2018-09">2018</year><month>09</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2021-04-02">2021</year><month>04</month><day>02</day></date-in-citation>
      <uri>https://arxiv.org/abs/1809.01448</uri>
    </element-citation>
  </ref>
  <ref id="ref-gao2013">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Gao</surname><given-names>Song</given-names></name>
        <name><surname>Janowicz</surname><given-names>Krzysztof</given-names></name>
        <name><surname>McKenzie</surname><given-names>Grant</given-names></name>
        <name><surname>Li</surname><given-names>Linna</given-names></name>
      </person-group>
      <article-title>Towards Platial Joins and Buffers in Place-Based GIS</article-title>
      <year iso-8601-date="2013">2013</year>
      <fpage>8</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-gao2017">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Gao</surname><given-names>Song</given-names></name>
        <name><surname>Janowicz</surname><given-names>Krzysztof</given-names></name>
        <name><surname>Couclelis</surname><given-names>Helen</given-names></name>
      </person-group>
      <article-title>Extracting urban functional regions from points of interest and human activities on location-based social networks</article-title>
      <source>Transactions in GIS</source>
      <year iso-8601-date="2017">2017</year>
      <date-in-citation content-type="access-date"><year iso-8601-date="2022-04-27">2022</year><month>04</month><day>27</day></date-in-citation>
      <volume>21</volume>
      <issue>3</issue>
      <issn>1467-9671</issn>
      <pub-id pub-id-type="doi">10.1111/tgis.12289</pub-id>
      <fpage>446</fpage>
      <lpage>467</lpage>
    </element-citation>
  </ref>
  <ref id="ref-gardner2018">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Gardner</surname><given-names>Matt</given-names></name>
        <name><surname>Grus</surname><given-names>Joel</given-names></name>
        <name><surname>Neumann</surname><given-names>Mark</given-names></name>
        <name><surname>Tafjord</surname><given-names>Oyvind</given-names></name>
        <name><surname>Dasigi</surname><given-names>Pradeep</given-names></name>
        <name><surname>Liu</surname><given-names>Nelson F.</given-names></name>
        <name><surname>Peters</surname><given-names>Matthew</given-names></name>
        <name><surname>Schmitz</surname><given-names>Michael</given-names></name>
        <name><surname>Zettlemoyer</surname><given-names>Luke</given-names></name>
      </person-group>
      <article-title>AllenNLP: A Deep Semantic Natural Language Processing Platform</article-title>
      <source>Proceedings of Workshop for NLP Open Source Software (NLP-OSS)</source>
      <publisher-name>Association for Computational Linguistics</publisher-name>
      <publisher-loc>Melbourne, Australia</publisher-loc>
      <year iso-8601-date="2018">2018</year>
      <date-in-citation content-type="access-date"><year iso-8601-date="2020-08-26">2020</year><month>08</month><day>26</day></date-in-citation>
      <pub-id pub-id-type="doi">10.18653/v1/w18-2501</pub-id>
      <fpage>1</fpage>
      <lpage>6</lpage>
    </element-citation>
  </ref>
  <ref id="ref-goodchild2007">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Goodchild</surname><given-names>Michael F.</given-names></name>
      </person-group>
      <article-title>Citizens as sensors: The world of volunteered geography</article-title>
      <source>GeoJournal</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2007">2007</year>
      <date-in-citation content-type="access-date"><year iso-8601-date="2020-08-28">2020</year><month>08</month><day>28</day></date-in-citation>
      <volume>69</volume>
      <issue>4</issue>
      <issn>0343-2521</issn>
      <uri>https://www.jstor.org/stable/41148191</uri>
      <pub-id pub-id-type="doi">10.1007/s10708-007-9111-y</pub-id>
      <fpage>211</fpage>
      <lpage>221</lpage>
    </element-citation>
  </ref>
  <ref id="ref-goodchild2011">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Goodchild</surname><given-names>Michael F.</given-names></name>
        <name><surname>Li</surname><given-names>Linna</given-names></name>
      </person-group>
      <article-title>Formalizing space and place</article-title>
      <year iso-8601-date="2011">2011</year>
    </element-citation>
  </ref>
  <ref id="ref-gritta2017a">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Gritta</surname><given-names>Milan</given-names></name>
        <name><surname>Pilehvar</surname><given-names>Mohammad Taher</given-names></name>
        <name><surname>Limsopatham</surname><given-names>Nut</given-names></name>
        <name><surname>Collier</surname><given-names>Nigel</given-names></name>
      </person-group>
      <article-title>What‚Äôs missing in geographical parsing?</article-title>
      <source>Language Resources and Evaluation</source>
      <year iso-8601-date="2017-03">2017</year><month>03</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2020-01-27">2020</year><month>01</month><day>27</day></date-in-citation>
      <volume>52</volume>
      <issue>2</issue>
      <issn>1574-020X, 1574-0218</issn>
      <pub-id pub-id-type="doi">10/ggwjt9</pub-id>
      <fpage>603</fpage>
      <lpage>623</lpage>
    </element-citation>
  </ref>
  <ref id="ref-gritta2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Gritta</surname><given-names>Milan</given-names></name>
        <name><surname>Pilehvar</surname><given-names>Mohammad Taher</given-names></name>
        <name><surname>Collier</surname><given-names>Nigel</given-names></name>
      </person-group>
      <article-title>A pragmatic guide to geoparsing evaluation: Toponyms, Named Entity Recognition and pragmatics</article-title>
      <source>Language Resources and Evaluation</source>
      <year iso-8601-date="2020-09">2020</year><month>09</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2022-07-20">2022</year><month>07</month><day>20</day></date-in-citation>
      <volume>54</volume>
      <issue>3</issue>
      <issn>1574-020X, 1574-0218</issn>
      <pub-id pub-id-type="doi">10.1007/s10579-019-09475-3</pub-id>
      <fpage>683</fpage>
      <lpage>712</lpage>
    </element-citation>
  </ref>
  <ref id="ref-halterman2017">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Halterman</surname><given-names>Andrew</given-names></name>
      </person-group>
      <article-title>Mordecai: Full text geoparsing and event geocoding</article-title>
      <source>The Journal of Open Source Software</source>
      <year iso-8601-date="2017">2017</year>
      <volume>2</volume>
      <issue>9</issue>
      <pub-id pub-id-type="doi">10.21105/joss.00091</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-hollenstein2010">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Hollenstein</surname><given-names>Livia</given-names></name>
        <name><surname>Purves</surname><given-names>Ross</given-names></name>
      </person-group>
      <article-title>Exploring place through user-generated content: Using Flickr tags to describe city cores</article-title>
      <source>Journal of Spatial Information Science</source>
      <year iso-8601-date="2010-12">2010</year><month>12</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2022-04-25">2022</year><month>04</month><day>25</day></date-in-citation>
      <issue>1</issue>
      <issn>1948-660X</issn>
      <fpage>21</fpage>
      <lpage>48</lpage>
    </element-citation>
  </ref>
  <ref id="ref-honnibal2015">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Honnibal</surname><given-names>Matthew</given-names></name>
        <name><surname>Johnson</surname><given-names>Mark</given-names></name>
      </person-group>
      <article-title>An Improved Non-monotonic Transition System for Dependency Parsing</article-title>
      <source>Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</source>
      <publisher-name>Association for Computational Linguistics</publisher-name>
      <publisher-loc>Lisbon, Portugal</publisher-loc>
      <year iso-8601-date="2015">2015</year>
      <date-in-citation content-type="access-date"><year iso-8601-date="2021-03-17">2021</year><month>03</month><day>17</day></date-in-citation>
      <pub-id pub-id-type="doi">10.18653/v1/d15-1162</pub-id>
      <fpage>1373</fpage>
      <lpage>1378</lpage>
    </element-citation>
  </ref>
  <ref id="ref-honnibal2017">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Honnibal</surname><given-names>Matthew</given-names></name>
        <name><surname>Montani</surname><given-names>Ines</given-names></name>
      </person-group>
      <article-title>Spacy 2: Natural language understanding with bloom embeddings, convolutional neural networks and incremental parsing</article-title>
      <source>To appear</source>
      <year iso-8601-date="2017">2017</year>
      <volume>7</volume>
      <issue>1</issue>
    </element-citation>
  </ref>
  <ref id="ref-hu2019">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Hu</surname><given-names>Yingjie</given-names></name>
        <name><surname>Mao</surname><given-names>Huina</given-names></name>
        <name><surname>McKenzie</surname><given-names>Grant</given-names></name>
      </person-group>
      <article-title>A natural language processing and geospatial clustering framework for harvesting local place names from geotagged housing advertisements</article-title>
      <source>International Journal of Geographical Information Science</source>
      <year iso-8601-date="2019-04">2019</year><month>04</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2022-01-19">2022</year><month>01</month><day>19</day></date-in-citation>
      <volume>33</volume>
      <issue>4</issue>
      <issn>1365-8816, 1362-3087</issn>
      <pub-id pub-id-type="doi">10.1080/13658816.2018.1458986</pub-id>
      <fpage>714</fpage>
      <lpage>738</lpage>
    </element-citation>
  </ref>
  <ref id="ref-karimzadeh2019">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Karimzadeh</surname><given-names>Morteza</given-names></name>
        <name><surname>Pezanowski</surname><given-names>Scott</given-names></name>
        <name><surname>MacEachren</surname><given-names>Alan M.</given-names></name>
        <name><surname>Wallgr√ºn</surname><given-names>Jan O.</given-names></name>
      </person-group>
      <article-title>GeoTxt: A scalable geoparsing system for unstructured text geolocation</article-title>
      <source>Transactions in GIS</source>
      <year iso-8601-date="2019-02">2019</year><month>02</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2020-01-28">2020</year><month>01</month><day>28</day></date-in-citation>
      <volume>23</volume>
      <issue>1</issue>
      <issn>1361-1682, 1467-9671</issn>
      <pub-id pub-id-type="doi">10.1111/tgis.12510</pub-id>
      <fpage>118</fpage>
      <lpage>136</lpage>
    </element-citation>
  </ref>
  <ref id="ref-kingma2017">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kingma</surname><given-names>Diederik P.</given-names></name>
        <name><surname>Ba</surname><given-names>Jimmy</given-names></name>
      </person-group>
      <article-title>Adam: A Method for Stochastic Optimization</article-title>
      <source>arXiv:1412.6980 [cs]</source>
      <year iso-8601-date="2017-01">2017</year><month>01</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2021-03-19">2021</year><month>03</month><day>19</day></date-in-citation>
      <uri>https://arxiv.org/abs/1412.6980</uri>
    </element-citation>
  </ref>
  <ref id="ref-kumar2019">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kumar</surname><given-names>Abhinav</given-names></name>
        <name><surname>Singh</surname><given-names>Jyoti Prakash</given-names></name>
      </person-group>
      <article-title>Location reference identification from tweets during emergencies: A deep learning approach</article-title>
      <source>International Journal of Disaster Risk Reduction</source>
      <year iso-8601-date="2019-02">2019</year><month>02</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2020-02-10">2020</year><month>02</month><day>10</day></date-in-citation>
      <volume>33</volume>
      <issn>22124209</issn>
      <pub-id pub-id-type="doi">10.1016/j.ijdrr.2018.10.021</pub-id>
      <fpage>365</fpage>
      <lpage>375</lpage>
    </element-citation>
  </ref>
  <ref id="ref-lample2016">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Lample</surname><given-names>Guillaume</given-names></name>
        <name><surname>Ballesteros</surname><given-names>Miguel</given-names></name>
        <name><surname>Subramanian</surname><given-names>Sandeep</given-names></name>
        <name><surname>Kawakami</surname><given-names>Kazuya</given-names></name>
        <name><surname>Dyer</surname><given-names>Chris</given-names></name>
      </person-group>
      <article-title>Neural Architectures for Named Entity Recognition</article-title>
      <source>arXiv:1603.01360 [cs]</source>
      <year iso-8601-date="2016-04">2016</year><month>04</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2020-01-15">2020</year><month>01</month><day>15</day></date-in-citation>
      <uri>https://arxiv.org/abs/1603.01360</uri>
    </element-citation>
  </ref>
  <ref id="ref-leidner2008">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Leidner</surname><given-names>Jochen Lothar</given-names></name>
      </person-group>
      <article-title>Toponym Resolution in Text</article-title>
      <year iso-8601-date="2008">2008</year>
      <pub-id pub-id-type="doi">10.1145/1328964.1328989</pub-id>
      <fpage>287</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-liu2019">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Liu</surname><given-names>Yinhan</given-names></name>
        <name><surname>Ott</surname><given-names>Myle</given-names></name>
        <name><surname>Goyal</surname><given-names>Naman</given-names></name>
        <name><surname>Du</surname><given-names>Jingfei</given-names></name>
        <name><surname>Joshi</surname><given-names>Mandar</given-names></name>
        <name><surname>Chen</surname><given-names>Danqi</given-names></name>
        <name><surname>Levy</surname><given-names>Omer</given-names></name>
        <name><surname>Lewis</surname><given-names>Mike</given-names></name>
        <name><surname>Zettlemoyer</surname><given-names>Luke</given-names></name>
        <name><surname>Stoyanov</surname><given-names>Veselin</given-names></name>
      </person-group>
      <article-title>RoBERTa: A Robustly Optimized BERT Pretraining Approach</article-title>
      <source>arXiv:1907.11692 [cs]</source>
      <year iso-8601-date="2019-07">2019</year><month>07</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2020-08-26">2020</year><month>08</month><day>26</day></date-in-citation>
      <uri>https://arxiv.org/abs/1907.11692</uri>
    </element-citation>
  </ref>
  <ref id="ref-loshchilov2019">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Loshchilov</surname><given-names>Ilya</given-names></name>
        <name><surname>Hutter</surname><given-names>Frank</given-names></name>
      </person-group>
      <article-title>Decoupled Weight Decay Regularization</article-title>
      <source>arXiv:1711.05101 [cs, math]</source>
      <year iso-8601-date="2019-01">2019</year><month>01</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2020-08-28">2020</year><month>08</month><day>28</day></date-in-citation>
      <uri>https://arxiv.org/abs/1711.05101</uri>
    </element-citation>
  </ref>
  <ref id="ref-mani2010">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Mani</surname><given-names>Inderjeet</given-names></name>
        <name><surname>Doran</surname><given-names>Christy</given-names></name>
        <name><surname>Harris</surname><given-names>Dave</given-names></name>
        <name><surname>Hitzeman</surname><given-names>Janet</given-names></name>
        <name><surname>Quimby</surname><given-names>Rob</given-names></name>
        <name><surname>Richer</surname><given-names>Justin</given-names></name>
        <name><surname>Wellner</surname><given-names>Ben</given-names></name>
        <name><surname>Mardis</surname><given-names>Scott</given-names></name>
        <name><surname>Clancy</surname><given-names>Seamus</given-names></name>
      </person-group>
      <article-title>SpatialML: Annotation scheme, resources, and evaluation</article-title>
      <source>Language Resources and Evaluation</source>
      <year iso-8601-date="2010-09">2010</year><month>09</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2020-03-20">2020</year><month>03</month><day>20</day></date-in-citation>
      <volume>44</volume>
      <issue>3</issue>
      <issn>1574-020X, 1574-0218</issn>
      <pub-id pub-id-type="doi">10.1007/s10579-010-9121-0</pub-id>
      <fpage>263</fpage>
      <lpage>280</lpage>
    </element-citation>
  </ref>
  <ref id="ref-mikolov2013">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Mikolov</surname><given-names>Tomas</given-names></name>
        <name><surname>Chen</surname><given-names>Kai</given-names></name>
        <name><surname>Corrado</surname><given-names>Greg</given-names></name>
        <name><surname>Dean</surname><given-names>Jeffrey</given-names></name>
      </person-group>
      <article-title>Efficient estimation of word representations in vector space</article-title>
      <source>arXiv preprint arXiv:1301.3781</source>
      <year iso-8601-date="2013">2013</year>
      <uri>https://arxiv.org/abs/1301.3781</uri>
    </element-citation>
  </ref>
  <ref id="ref-moncla2014">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Moncla</surname><given-names>Ludovic</given-names></name>
        <name><surname>Renteria-Agualimpia</surname><given-names>Walter</given-names></name>
        <name><surname>Nogueras-Iso</surname><given-names>Javier</given-names></name>
        <name><surname>Gaio</surname><given-names>Mauro</given-names></name>
      </person-group>
      <article-title>Geocoding for texts with fine-grain toponyms: An experiment on a geoparsed hiking descriptions corpus</article-title>
      <source>Proceedings of the 22nd ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems</source>
      <publisher-name>ACM</publisher-name>
      <publisher-loc>Dallas Texas</publisher-loc>
      <year iso-8601-date="2014-11">2014</year><month>11</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2022-07-20">2022</year><month>07</month><day>20</day></date-in-citation>
      <isbn>978-1-4503-3131-9</isbn>
      <pub-id pub-id-type="doi">10.1145/2666310.2666386</pub-id>
      <fpage>183</fpage>
      <lpage>192</lpage>
    </element-citation>
  </ref>
  <ref id="ref-nakayama2018">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Nakayama</surname><given-names>Hiroki</given-names></name>
        <name><surname>Kubo</surname><given-names>Takahiro</given-names></name>
        <name><surname>Kamura</surname><given-names>Junya</given-names></name>
        <name><surname>Taniguchi</surname><given-names>Yasufumi</given-names></name>
        <name><surname>Liang</surname><given-names>Xu</given-names></name>
      </person-group>
      <article-title>Doccano: Text annotation for humans</article-title>
      <year iso-8601-date="2018">2018</year>
    </element-citation>
  </ref>
  <ref id="ref-nguyen2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Nguyen</surname><given-names>Dat Quoc</given-names></name>
        <name><surname>Vu</surname><given-names>Thanh</given-names></name>
        <name><surname>Nguyen</surname><given-names>Anh Tuan</given-names></name>
      </person-group>
      <article-title>BERTweet: A pre-trained language model for English Tweets</article-title>
      <source>arXiv:2005.10200 [cs]</source>
      <year iso-8601-date="2020-10">2020</year><month>10</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2022-03-08">2022</year><month>03</month><day>08</day></date-in-citation>
      <uri>https://arxiv.org/abs/2005.10200</uri>
    </element-citation>
  </ref>
  <ref id="ref-paszke2019">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Paszke</surname><given-names>Adam</given-names></name>
        <name><surname>Gross</surname><given-names>Sam</given-names></name>
        <name><surname>Massa</surname><given-names>Francisco</given-names></name>
        <name><surname>Lerer</surname><given-names>Adam</given-names></name>
        <name><surname>Bradbury</surname><given-names>James</given-names></name>
        <name><surname>Chanan</surname><given-names>Gregory</given-names></name>
        <name><surname>Killeen</surname><given-names>Trevor</given-names></name>
        <name><surname>Lin</surname><given-names>Zeming</given-names></name>
        <name><surname>Gimelshein</surname><given-names>Natalia</given-names></name>
        <name><surname>Antiga</surname><given-names>Luca</given-names></name>
        <name><surname>Desmaison</surname><given-names>Alban</given-names></name>
        <name><surname>K√∂pf</surname><given-names>Andreas</given-names></name>
        <name><surname>Yang</surname><given-names>Edward</given-names></name>
        <name><surname>DeVito</surname><given-names>Zach</given-names></name>
        <name><surname>Raison</surname><given-names>Martin</given-names></name>
        <name><surname>Tejani</surname><given-names>Alykhan</given-names></name>
        <name><surname>Chilamkurthy</surname><given-names>Sasank</given-names></name>
        <name><surname>Steiner</surname><given-names>Benoit</given-names></name>
        <name><surname>Fang</surname><given-names>Lu</given-names></name>
        <name><surname>Bai</surname><given-names>Junjie</given-names></name>
        <name><surname>Chintala</surname><given-names>Soumith</given-names></name>
      </person-group>
      <article-title>PyTorch: An Imperative Style, High-Performance Deep Learning Library</article-title>
      <publisher-name>arXiv</publisher-name>
      <year iso-8601-date="2019-12">2019</year><month>12</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2023-02-06">2023</year><month>02</month><day>06</day></date-in-citation>
      <uri>https://arxiv.org/abs/1912.01703</uri>
    </element-citation>
  </ref>
  <ref id="ref-pennington2014">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Pennington</surname><given-names>Jeffrey</given-names></name>
        <name><surname>Socher</surname><given-names>Richard</given-names></name>
        <name><surname>Manning</surname><given-names>Christopher</given-names></name>
      </person-group>
      <article-title>GloVe: Global Vectors for Word Representation</article-title>
      <source>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</source>
      <publisher-name>Association for Computational Linguistics</publisher-name>
      <publisher-loc>Doha, Qatar</publisher-loc>
      <year iso-8601-date="2014-10">2014</year><month>10</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2020-07-16">2020</year><month>07</month><day>16</day></date-in-citation>
      <pub-id pub-id-type="doi">10.3115/v1/d14-1162</pub-id>
      <fpage>1532</fpage>
      <lpage>1543</lpage>
    </element-citation>
  </ref>
  <ref id="ref-peters2018">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Peters</surname><given-names>Matthew E.</given-names></name>
        <name><surname>Neumann</surname><given-names>Mark</given-names></name>
        <name><surname>Iyyer</surname><given-names>Mohit</given-names></name>
        <name><surname>Gardner</surname><given-names>Matt</given-names></name>
        <name><surname>Clark</surname><given-names>Christopher</given-names></name>
        <name><surname>Lee</surname><given-names>Kenton</given-names></name>
        <name><surname>Zettlemoyer</surname><given-names>Luke</given-names></name>
      </person-group>
      <article-title>Deep contextualized word representations</article-title>
      <source>arXiv:1802.05365 [cs]</source>
      <year iso-8601-date="2018-03">2018</year><month>03</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2020-02-11">2020</year><month>02</month><day>11</day></date-in-citation>
      <uri>https://arxiv.org/abs/1802.05365</uri>
    </element-citation>
  </ref>
  <ref id="ref-purves2018">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Purves</surname><given-names>Ross S.</given-names></name>
        <name><surname>Clough</surname><given-names>Paul</given-names></name>
        <name><surname>Jones</surname><given-names>Christopher B.</given-names></name>
        <name><surname>Hall</surname><given-names>Mark H.</given-names></name>
        <name><surname>Murdock</surname><given-names>Vanessa</given-names></name>
      </person-group>
      <source>Geographic Information Retrieval: Progress and Challenges in Spatial Search of Text</source>
      <publisher-name>now</publisher-name>
      <year iso-8601-date="2018">2018</year>
      <isbn>978-1-68083-413-0</isbn>
      <pub-id pub-id-type="doi">10.1561/1500000034</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-pustejovsky2015">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Pustejovsky</surname><given-names>James</given-names></name>
        <name><surname>Kordjamshidi</surname><given-names>Parisa</given-names></name>
        <name><surname>Moens</surname><given-names>Marie-Francine</given-names></name>
        <name><surname>Levine</surname><given-names>Aaron</given-names></name>
        <name><surname>Dworman</surname><given-names>Seth</given-names></name>
        <name><surname>Yocum</surname><given-names>Zachary</given-names></name>
      </person-group>
      <article-title>SemEval-2015 Task 8: SpaceEval</article-title>
      <source>Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015)</source>
      <publisher-name>Association for Computational Linguistics</publisher-name>
      <publisher-loc>Denver, Colorado</publisher-loc>
      <year iso-8601-date="2015">2015</year>
      <date-in-citation content-type="access-date"><year iso-8601-date="2020-08-06">2020</year><month>08</month><day>06</day></date-in-citation>
      <pub-id pub-id-type="doi">10.18653/v1/s15-2149</pub-id>
      <fpage>884</fpage>
      <lpage>894</lpage>
    </element-citation>
  </ref>
  <ref id="ref-pustejovsky2017">
    <element-citation publication-type="chapter">
      <person-group person-group-type="author">
        <name><surname>Pustejovsky</surname><given-names>James</given-names></name>
      </person-group>
      <article-title>ISO-Space: Annotating Static and Dynamic Spatial Information</article-title>
      <source>Handbook of Linguistic Annotation</source>
      <person-group person-group-type="editor">
        <name><surname>Ide</surname><given-names>Nancy</given-names></name>
        <name><surname>Pustejovsky</surname><given-names>James</given-names></name>
      </person-group>
      <publisher-name>Springer Netherlands</publisher-name>
      <publisher-loc>Dordrecht</publisher-loc>
      <year iso-8601-date="2017">2017</year>
      <date-in-citation content-type="access-date"><year iso-8601-date="2020-07-14">2020</year><month>07</month><day>14</day></date-in-citation>
      <isbn>978-94-024-0879-9 978-94-024-0881-2</isbn>
      <pub-id pub-id-type="doi">10.1007/978-94-024-0881-2_37</pub-id>
      <fpage>989</fpage>
      <lpage>1024</lpage>
    </element-citation>
  </ref>
  <ref id="ref-qi2018">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Qi</surname><given-names>Peng</given-names></name>
        <name><surname>Dozat</surname><given-names>Timothy</given-names></name>
        <name><surname>Zhang</surname><given-names>Yuhao</given-names></name>
        <name><surname>Manning</surname><given-names>Christopher D.</given-names></name>
      </person-group>
      <article-title>Universal Dependency Parsing from Scratch</article-title>
      <source>Proceedings of the</source>
      <publisher-name>Association for Computational Linguistics</publisher-name>
      <publisher-loc>Brussels, Belgium</publisher-loc>
      <year iso-8601-date="2018">2018</year>
      <date-in-citation content-type="access-date"><year iso-8601-date="2021-03-15">2021</year><month>03</month><day>15</day></date-in-citation>
      <pub-id pub-id-type="doi">10.18653/v1/k18-2016</pub-id>
      <fpage>160</fpage>
      <lpage>170</lpage>
    </element-citation>
  </ref>
  <ref id="ref-ratinov2009">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Ratinov</surname><given-names>Lev</given-names></name>
        <name><surname>Roth</surname><given-names>Dan</given-names></name>
      </person-group>
      <article-title>Design Challenges and Misconceptions in Named Entity Recognition</article-title>
      <source>Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL-2009)</source>
      <publisher-name>Association for Computational Linguistics</publisher-name>
      <publisher-loc>Boulder, Colorado</publisher-loc>
      <year iso-8601-date="2009-06">2009</year><month>06</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2020-08-26">2020</year><month>08</month><day>26</day></date-in-citation>
      <fpage>147</fpage>
      <lpage>155</lpage>
    </element-citation>
  </ref>
  <ref id="ref-sanh2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Sanh</surname><given-names>Victor</given-names></name>
        <name><surname>Debut</surname><given-names>Lysandre</given-names></name>
        <name><surname>Chaumond</surname><given-names>Julien</given-names></name>
        <name><surname>Wolf</surname><given-names>Thomas</given-names></name>
      </person-group>
      <article-title>DistilBERT, a distilled version of BERT: Smaller, faster, cheaper and lighter</article-title>
      <source>arXiv:1910.01108 [cs]</source>
      <year iso-8601-date="2020-02">2020</year><month>02</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2021-03-18">2021</year><month>03</month><day>18</day></date-in-citation>
      <uri>https://arxiv.org/abs/1910.01108</uri>
    </element-citation>
  </ref>
  <ref id="ref-see2016">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>See</surname><given-names>Linda</given-names></name>
        <name><surname>Mooney</surname><given-names>Peter</given-names></name>
        <name><surname>Foody</surname><given-names>Giles</given-names></name>
        <name><surname>Bastin</surname><given-names>Lucy</given-names></name>
        <name><surname>Comber</surname><given-names>Alexis</given-names></name>
        <name><surname>Estima</surname><given-names>Jacinto</given-names></name>
        <name><surname>Fritz</surname><given-names>Steffen</given-names></name>
        <name><surname>Kerle</surname><given-names>Norman</given-names></name>
        <name><surname>Jiang</surname><given-names>Bin</given-names></name>
        <name><surname>Laakso</surname><given-names>Mari</given-names></name>
        <name><surname>Liu</surname><given-names>Hai-Ying</given-names></name>
        <name><surname>Milƒçinski</surname><given-names>Grega</given-names></name>
        <name><surname>Nik≈°iƒç</surname><given-names>Matej</given-names></name>
        <name><surname>Painho</surname><given-names>Marco</given-names></name>
        <name><surname>P≈ëd√∂r</surname><given-names>Andrea</given-names></name>
        <name><surname>Olteanu-Raimond</surname><given-names>Ana-Maria</given-names></name>
        <name><surname>Rutzinger</surname><given-names>Martin</given-names></name>
      </person-group>
      <article-title>Crowdsourcing, Citizen Science or Volunteered Geographic Information? The Current State of Crowdsourced Geographic Information</article-title>
      <source>ISPRS International Journal of Geo-Information</source>
      <publisher-name>Multidisciplinary Digital Publishing Institute</publisher-name>
      <year iso-8601-date="2016-05">2016</year><month>05</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2021-04-06">2021</year><month>04</month><day>06</day></date-in-citation>
      <volume>5</volume>
      <issue>5</issue>
      <pub-id pub-id-type="doi">10.3390/ijgi5050055</pub-id>
      <fpage>55</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-speriosu2013">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Speriosu</surname><given-names>Michael</given-names></name>
        <name><surname>Baldridge</surname><given-names>Jason</given-names></name>
      </person-group>
      <article-title>Text-Driven Toponym Resolution using Indirect Supervision</article-title>
      <year iso-8601-date="2013">2013</year>
      <fpage>10</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-tjongkimsang2003">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Tjong Kim Sang</surname><given-names>Erik F.</given-names></name>
        <name><surname>De Meulder</surname><given-names>Fien</given-names></name>
      </person-group>
      <article-title>Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition</article-title>
      <source>Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</source>
      <year iso-8601-date="2003">2003</year>
      <date-in-citation content-type="access-date"><year iso-8601-date="2020-08-06">2020</year><month>08</month><day>06</day></date-in-citation>
      <pub-id pub-id-type="doi">10.3115/1119176.1119195</pub-id>
      <fpage>142</fpage>
      <lpage>147</lpage>
    </element-citation>
  </ref>
  <ref id="ref-twaroch2019">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Twaroch</surname><given-names>Florian A.</given-names></name>
        <name><surname>Brindley</surname><given-names>Paul</given-names></name>
        <name><surname>Clough</surname><given-names>Paul D.</given-names></name>
        <name><surname>Jones</surname><given-names>Christopher B.</given-names></name>
        <name><surname>Pasley</surname><given-names>Robert C.</given-names></name>
        <name><surname>Mansbridge</surname><given-names>Sue</given-names></name>
      </person-group>
      <article-title>Investigating behavioural and computational approaches for defining imprecise regions</article-title>
      <source>Spatial Cognition &amp; Computation</source>
      <year iso-8601-date="2019-04">2019</year><month>04</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2020-06-16">2020</year><month>06</month><day>16</day></date-in-citation>
      <volume>19</volume>
      <issue>2</issue>
      <issn>1387-5868, 1542-7633</issn>
      <pub-id pub-id-type="doi">10.1080/13875868.2018.1531871</pub-id>
      <fpage>146</fpage>
      <lpage>171</lpage>
    </element-citation>
  </ref>
  <ref id="ref-vaswani2017">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Vaswani</surname><given-names>Ashish</given-names></name>
        <name><surname>Shazeer</surname><given-names>Noam</given-names></name>
        <name><surname>Parmar</surname><given-names>Niki</given-names></name>
        <name><surname>Uszkoreit</surname><given-names>Jakob</given-names></name>
        <name><surname>Jones</surname><given-names>Llion</given-names></name>
        <name><surname>Gomez</surname><given-names>Aidan N.</given-names></name>
        <name><surname>Kaiser</surname><given-names>Lukasz</given-names></name>
        <name><surname>Polosukhin</surname><given-names>Illia</given-names></name>
      </person-group>
      <article-title>Attention Is All You Need</article-title>
      <source>arXiv:1706.03762 [cs]</source>
      <year iso-8601-date="2017-12">2017</year><month>12</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2020-10-09">2020</year><month>10</month><day>09</day></date-in-citation>
      <uri>https://arxiv.org/abs/1706.03762</uri>
    </element-citation>
  </ref>
  <ref id="ref-wallgrun2018">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Wallgr√ºn</surname><given-names>Jan Oliver</given-names></name>
        <name><surname>Karimzadeh</surname><given-names>Morteza</given-names></name>
        <name><surname>MacEachren</surname><given-names>Alan M.</given-names></name>
        <name><surname>Pezanowski</surname><given-names>Scott</given-names></name>
      </person-group>
      <article-title>GeoCorpora: Building a corpus to test and train microblog geoparsers</article-title>
      <source>International Journal of Geographical Information Science</source>
      <year iso-8601-date="2018-01">2018</year><month>01</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2022-03-08">2022</year><month>03</month><day>08</day></date-in-citation>
      <volume>32</volume>
      <issue>1</issue>
      <issn>1365-8816, 1362-3087</issn>
      <pub-id pub-id-type="doi">10.1080/13658816.2017.1368523</pub-id>
      <fpage>1</fpage>
      <lpage>29</lpage>
    </element-citation>
  </ref>
  <ref id="ref-weischedelralph2013">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Weischedel</surname><given-names>Ralph</given-names></name>
        <name><surname>Palmer</surname><given-names>Martha</given-names></name>
        <name><surname>Marcus</surname><given-names>Mitchell</given-names></name>
        <name><surname>Hovy</surname><given-names>Eduard</given-names></name>
        <name><surname>Pradhan</surname><given-names>Sameer</given-names></name>
        <name><surname>Ramshaw</surname><given-names>Lance</given-names></name>
        <name><surname>Xue</surname><given-names>Nianwen</given-names></name>
        <name><surname>Taylor</surname><given-names>Ann</given-names></name>
        <name><surname>Kaufman</surname><given-names>Jeff</given-names></name>
        <name><surname>Franchini</surname><given-names>Michelle</given-names></name>
        <string-name>El-Bachouti, Mohammed</string-name>
        <name><surname>Belvin</surname><given-names>Robert</given-names></name>
        <name><surname>Houston</surname><given-names>Ann</given-names></name>
      </person-group>
      <article-title>OntoNotes Release 5.0</article-title>
      <publisher-name>Linguistic Data Consortium</publisher-name>
      <year iso-8601-date="2013-10">2013</year><month>10</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2022-09-07">2022</year><month>09</month><day>07</day></date-in-citation>
      <pub-id pub-id-type="doi">10.35111/XMHB-2B84</pub-id>
      <fpage>2806280 KB</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-weissenbacher2019">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Weissenbacher</surname><given-names>Davy</given-names></name>
        <name><surname>Magge</surname><given-names>Arjun</given-names></name>
        <name><surname>O‚ÄôConnor</surname><given-names>Karen</given-names></name>
        <name><surname>Scotch</surname><given-names>Matthew</given-names></name>
        <name><surname>Gonzalez-Hernandez</surname><given-names>Graciela</given-names></name>
      </person-group>
      <article-title>SemEval-2019 Task 12: Toponym Resolution in Scientific Papers</article-title>
      <source>Proceedings of the 13th International Workshop on Semantic Evaluation</source>
      <publisher-name>Association for Computational Linguistics</publisher-name>
      <publisher-loc>Minneapolis, Minnesota, USA</publisher-loc>
      <year iso-8601-date="2019">2019</year>
      <date-in-citation content-type="access-date"><year iso-8601-date="2020-02-11">2020</year><month>02</month><day>11</day></date-in-citation>
      <pub-id pub-id-type="doi">10.18653/v1/s19-2155</pub-id>
      <fpage>907</fpage>
      <lpage>916</lpage>
    </element-citation>
  </ref>
  <ref id="ref-wolf2020">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Wolf</surname><given-names>Thomas</given-names></name>
        <name><surname>Debut</surname><given-names>Lysandre</given-names></name>
        <name><surname>Sanh</surname><given-names>Victor</given-names></name>
        <name><surname>Chaumond</surname><given-names>Julien</given-names></name>
        <name><surname>Delangue</surname><given-names>Clement</given-names></name>
        <name><surname>Moi</surname><given-names>Anthony</given-names></name>
        <name><surname>Cistac</surname><given-names>Pierric</given-names></name>
        <name><surname>Rault</surname><given-names>Tim</given-names></name>
        <name><surname>Louf</surname><given-names>R√©mi</given-names></name>
        <name><surname>Funtowicz</surname><given-names>Morgan</given-names></name>
        <name><surname>Davison</surname><given-names>Joe</given-names></name>
        <name><surname>Shleifer</surname><given-names>Sam</given-names></name>
        <name><surname>von Platen</surname><given-names>Patrick</given-names></name>
        <name><surname>Ma</surname><given-names>Clara</given-names></name>
        <name><surname>Jernite</surname><given-names>Yacine</given-names></name>
        <name><surname>Plu</surname><given-names>Julien</given-names></name>
        <name><surname>Xu</surname><given-names>Canwen</given-names></name>
        <name><surname>Scao</surname><given-names>Teven Le</given-names></name>
        <name><surname>Gugger</surname><given-names>Sylvain</given-names></name>
        <name><surname>Drame</surname><given-names>Mariama</given-names></name>
        <name><surname>Lhoest</surname><given-names>Quentin</given-names></name>
        <name><surname>Rush</surname><given-names>Alexander M.</given-names></name>
      </person-group>
      <article-title>Transformers: State-of-the-art natural language processing</article-title>
      <source>Proceedings of the 2020 conference on empirical methods in natural language processing: System demonstrations</source>
      <publisher-name>Association for Computational Linguistics</publisher-name>
      <publisher-loc>Online</publisher-loc>
      <year iso-8601-date="2020-10">2020</year><month>10</month>
      <fpage>38</fpage>
      <lpage>45</lpage>
    </element-citation>
  </ref>
</ref-list>
<fn-group>
  <fn id="fn1">
    <label>1</label><p>CoNLL03:
    <ext-link ext-link-type="uri" xlink:href="https://www.clips.uantwerpen.be/conll2003/ner/">https://www.clips.uantwerpen.be/conll2003/ner/</ext-link>,
    OntoNotes 5:
    <ext-link ext-link-type="uri" xlink:href="https://catalog.ldc.upenn.edu/LDC2013T19">https://catalog.ldc.upenn.edu/LDC2013T19</ext-link></p>
  </fn>
  <fn id="fn2">
    <label>2</label><p><ext-link ext-link-type="uri" xlink:href="https://github.com/openeventdata/mordecai">https://github.com/openeventdata/mordecai</ext-link></p>
  </fn>
  <fn id="fn3">
    <label>3</label><p>https://www.geonames.org/8714914/m1-motorway.html</p>
  </fn>
  <fn id="fn4">
    <label>4</label><p>Compared with the Reuters corpus used for CoNLL03
    for example</p>
  </fn>
</fn-group>
</back>


</article>